WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:05.040
到目前为止 已经有了一个时间差分控制算法

00:00:05.040 --> 00:00:07.095
注意 对于 Sarsa 算法

00:00:07.094 --> 00:00:09.869
我们先将所有动作值初始化为 0

00:00:09.869 --> 00:00:14.625
并构建相应的 Epsilon 贪婪策略

00:00:14.625 --> 00:00:19.940
然后 智能体开始与环境互动并接收第一个状态

00:00:19.940 --> 00:00:23.359
接着 它使用该策略选择动作

00:00:23.359 --> 00:00:27.554
紧接着 它收到一个奖励和下个状态

00:00:27.554 --> 00:00:32.289
然后 智能体再次使用相同的策略选择下个动作

00:00:32.289 --> 00:00:34.244
选择该动作后

00:00:34.244 --> 00:00:38.954
它会更新对应于上个状态动作对的动作值

00:00:38.954 --> 00:00:41.850
并根据最新的动作估值

00:00:41.850 --> 00:00:45.435
将该策略更新为 Epsilon 贪婪策略

00:00:45.435 --> 00:00:47.370
在本视频的后续阶段

00:00:47.369 --> 00:00:49.784
我们将根据该算法

00:00:49.784 --> 00:00:53.789
设计另一个稍微有所不同的控制算法

00:00:53.789 --> 00:00:56.100
该算法叫做 Sarsamax

00:00:56.100 --> 00:00:58.630
亦称之为 Q 学习

00:00:58.630 --> 00:01:03.660
我们依然采用相同的初始动作值和策略

00:01:03.659 --> 00:01:06.420
智能体接收初始状态

00:01:06.420 --> 00:01:09.540
依然根据初始策略选择第一个动作

00:01:09.540 --> 00:01:13.175
但是在接收奖励和下个状态后

00:01:13.174 --> 00:01:15.974
我们将执行不同的操作

00:01:15.974 --> 00:01:20.464
即我们将在选择下个动作之前更新策略

00:01:20.465 --> 00:01:24.140
你能猜到这里适合采用什么样的动作吗？

00:01:24.140 --> 00:01:26.060
对于 Sarsa 算法

00:01:26.060 --> 00:01:29.180
我们的更新步骤是晚一个步骤

00:01:29.180 --> 00:01:33.050
并代入使用 Epsilon 贪婪策略选择的动作

00:01:33.049 --> 00:01:35.325
对于该算法的每一步

00:01:35.325 --> 00:01:40.245
我们更新动作值使用的所有动作

00:01:40.245 --> 00:01:44.160
都完全与智能体体验的动作一样

00:01:44.159 --> 00:01:47.670
但是通常并非必须这样

00:01:47.670 --> 00:01:52.049
尤其是 考虑使用贪婪策略的动作

00:01:52.049 --> 00:01:54.894
而不是使用 Epsilon 贪婪策略的动作

00:01:54.894 --> 00:01:58.664
这就是 Sarsamax 或 Q 学习的流程

00:01:58.665 --> 00:02:01.740
在这种情况下 你可以重写该方程

00:02:01.739 --> 00:02:04.709
相当于某个状态

00:02:04.709 --> 00:02:07.439
对应的贪婪动作

00:02:07.439 --> 00:02:11.414
正好是最大化该状态的动作值的动作

00:02:11.414 --> 00:02:13.769
发生的情况是当我们使用贪婪动作

00:02:13.770 --> 00:02:17.520
更新时间步 0 的动作值后

00:02:17.520 --> 00:02:19.620
我们使用刚刚更新的动作值

00:02:19.620 --> 00:02:24.944
对应的 Epsilon 贪婪策略选择 A1

00:02:24.944 --> 00:02:29.829
当我们收到奖励和下个状态后继续这一流程

00:02:29.830 --> 00:02:33.760
然后像之前使用贪婪动作

00:02:33.759 --> 00:02:38.341
更新 S1 和 A1 对应的动作一样

00:02:38.342 --> 00:02:43.307
使用相应的 Epsilon 贪婪策略选择 A2

00:02:43.306 --> 00:02:46.424
为了清晰地理解这个更新步骤的作用

00:02:46.425 --> 00:02:50.155
我们将其与 Sarsa 算法中的相应步骤进行比较

00:02:50.155 --> 00:02:54.699
在 Sarsa 中 更新步骤使动作值更接近于

00:02:54.699 --> 00:02:59.964
智能体当前遵守 Epsilon 贪婪策略获得的动作值

00:02:59.965 --> 00:03:03.474
可以显示 Sarsamax

00:03:03.474 --> 00:03:08.814
直接在每个时间步估算最优值函数

00:03:08.814 --> 00:03:11.620
很快你将有机会自己实现此步骤

00:03:11.620 --> 00:03:15.000
并直接研究这两个算法之间的区别

