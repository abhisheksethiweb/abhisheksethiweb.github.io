WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.265
我们已经解决了预测问题

00:00:02.265 --> 00:00:04.320
可以讨论控制问题了

00:00:04.320 --> 00:00:07.695
智能体如何确定最优策略？

00:00:07.695 --> 00:00:12.115
我们将采用估算动作值函数的算法

00:00:12.115 --> 00:00:14.655
在此算法中 选择每个动作后

00:00:14.654 --> 00:00:16.934
智能体都更新估值

00:00:16.934 --> 00:00:19.140
需要注意的是

00:00:19.140 --> 00:00:22.820
智能体在每个时间步都使用相同的策略来选择动作

00:00:22.820 --> 00:00:25.995
但是现在 为了调整该算法以便生成控制算法

00:00:25.995 --> 00:00:27.990
我们将逐渐更改该策略

00:00:27.989 --> 00:00:31.199
使其在每个时间步都越来越完善

00:00:31.199 --> 00:00:33.810
我们将使用的方法之一

00:00:33.810 --> 00:00:36.480
与蒙特卡洛方法非常相似

00:00:36.479 --> 00:00:41.779
即在每个时间步使用一个针对当前动作估值的

00:00:41.780 --> 00:00:44.850
Epsilon 贪婪策略选择一个动作

00:00:44.850 --> 00:00:46.259
在初始时间步

00:00:46.259 --> 00:00:48.795
我们先将 ε 设为 1

00:00:48.795 --> 00:00:54.734
然后根据对等概率随机策略选择 A0 和 A1

00:00:54.734 --> 00:00:58.420
在选择某个动作之后的未来所有时间步

00:00:58.420 --> 00:01:00.600
我们都更新动作值函数

00:01:00.600 --> 00:01:03.675
并构建相应的 Epsilon 贪婪策略

00:01:03.674 --> 00:01:07.349
只要我们为 ε 指定合适的值

00:01:07.349 --> 00:01:11.614
该算法就肯定会收敛于最优策略

00:01:11.614 --> 00:01:14.104
该算法的名称叫做 Sarsa 0

00:01:14.105 --> 00:01:16.424
简称为 Sarsa

00:01:16.424 --> 00:01:23.564
得名原因是每个动作值更新都使用状态动作奖励

00:01:23.564 --> 00:01:27.000
后续状态 后续动作 互动元组

