WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.424
我们将继续先解决

00:00:02.424 --> 00:00:05.494
强化学习中的预测问题

00:00:05.495 --> 00:00:07.310
给定一个策略

00:00:07.309 --> 00:00:09.765
如何估算其值函数？

00:00:09.765 --> 00:00:13.169
我们来回顾下在上节课是如何完成的

00:00:13.169 --> 00:00:14.550
在蒙特卡洛方法中

00:00:14.550 --> 00:00:17.760
智能体以阶段形式与环境互动

00:00:17.760 --> 00:00:19.395
一个阶段结束后

00:00:19.394 --> 00:00:23.309
我们按顺序查看每个状态动作对

00:00:23.309 --> 00:00:24.974
如果是首次经历

00:00:24.975 --> 00:00:30.205
则计算相应的回报并使用它来更新动作值

00:00:30.204 --> 00:00:34.129
我们经历了很多很多个阶段

00:00:34.130 --> 00:00:37.160
需要注意的是 只要我们不在阶段之间更改策略

00:00:37.159 --> 00:00:42.458
该算法就可以解决预测问题

00:00:42.459 --> 00:00:45.425
只要我们运行该算法足够长的时间

00:00:45.424 --> 00:00:49.984
就肯定能够获得一个很完美的动作值函数估计结果

00:00:49.984 --> 00:00:53.500
现在将重点转移到这个更新步骤

00:00:53.500 --> 00:00:58.945
如果我们想要跟踪状态值 可以使用一个类似的方程式

00:00:58.945 --> 00:01:01.030
在本视频的剩余时间里

00:01:01.030 --> 00:01:05.995
我们将修改该更新步骤并获得一个新的算法

00:01:05.995 --> 00:01:10.480
注意 这行的主要原理是任何状态的值

00:01:10.480 --> 00:01:12.910
定义为智能体遵守策略后

00:01:12.909 --> 00:01:15.909
在该状态之后很可能会出现的预期回报

00:01:15.909 --> 00:01:20.144
对取样回报取平均值生成了很好的估值

00:01:20.144 --> 00:01:25.634
此刻 我要提醒下关于状态值的贝尔曼预期方程

00:01:25.635 --> 00:01:28.125
它可以使用潜在地跟在后面的状态的值

00:01:28.125 --> 00:01:32.890
表示任何状态的值

00:01:32.890 --> 00:01:35.879
如果我们使用像它上方的方程

00:01:35.879 --> 00:01:39.159
修改为一个稍微不同的更新规则呢？

00:01:39.159 --> 00:01:42.390
现在我们不再对取样回报取平均值

00:01:42.390 --> 00:01:45.719
而是对即时奖励的求和取样值

00:01:45.719 --> 00:01:50.000
加上下个状态的折扣值取平均值

00:01:50.000 --> 00:01:53.084
你将发现 现在的更新步骤

00:01:53.084 --> 00:01:58.199
根据后续状态的值计算某个状态的值

00:01:58.200 --> 00:02:00.240
为何要这么做呢？

00:02:00.239 --> 00:02:03.314
首先要注意的是

00:02:03.314 --> 00:02:06.929
我们删掉了阶段结束时的回报

00:02:06.930 --> 00:02:10.140
实际上 这个新的更新步骤

00:02:10.139 --> 00:02:13.514
使我们能够在每个时间步之后更新状态值

00:02:13.514 --> 00:02:19.289
为此 我们来看看在随机时间步 t 会发生什么

00:02:19.289 --> 00:02:21.625
和往常一样 我们将使用 St 表示状态

00:02:21.625 --> 00:02:25.550
假设智能体使用该策略选择动作

00:02:25.550 --> 00:02:30.180
然后从环境那获得奖励和下个状态

00:02:30.180 --> 00:02:33.960
然后 预测算法使用这个非常小的

00:02:33.960 --> 00:02:38.534
时间信息窗口更新值函数

00:02:38.534 --> 00:02:44.224
具体来说 我们将更新时间 t 的状态

00:02:44.224 --> 00:02:49.104
为此 首先我们查看时间 t 和 t+1 处状态的值

00:02:49.104 --> 00:02:51.560
通过代入奖励

00:02:51.560 --> 00:02:54.564
我们能够计算整个右侧部分

00:02:54.564 --> 00:02:59.050
这就是在时间 t 时的状态的新估算值

00:02:59.050 --> 00:03:01.005
现在要注意的是 我们不用等待阶段结束

00:03:01.004 --> 00:03:04.079
就能更新值

00:03:04.080 --> 00:03:06.040
这是处理连续性任务时

00:03:06.039 --> 00:03:09.334
你可以使用的第一个预测问题算法

00:03:09.335 --> 00:03:11.784
在详细讲解该算法之前

00:03:11.784 --> 00:03:15.245
我们来讨论下这个更新步骤的作用

00:03:15.245 --> 00:03:17.640
在随机时间步 t

00:03:17.639 --> 00:03:19.768
在智能体采取动作之前

00:03:19.769 --> 00:03:21.640
当前状态的最佳值估算

00:03:21.639 --> 00:03:25.004
包含在值函数中

00:03:25.004 --> 00:03:29.355
然后智能体采取动作并获得奖励和下个状态

00:03:29.355 --> 00:03:30.849
这是新的信息

00:03:30.849 --> 00:03:35.819
我们可以使用该信息表达同一状态的值替代估值

00:03:35.819 --> 00:03:38.924
不过采用的是后续状态的值

00:03:38.925 --> 00:03:42.745
将这个新的估值称为 TD 目标

00:03:42.745 --> 00:03:45.550
这整个更新方程的作用

00:03:45.550 --> 00:03:48.985
找到两个估值之间的中间值

00:03:48.985 --> 00:03:53.335
你将根据更加信任的估值设置 α 的值

00:03:53.335 --> 00:03:54.905
为了更清晰地体现这一点

00:03:54.905 --> 00:03:57.210
我们将重写更新方程

00:03:57.210 --> 00:04:01.270
注意 α 必须设为 0 和 1 之间的某个数字

00:04:01.270 --> 00:04:03.320
当 α 设为 1 时

00:04:03.319 --> 00:04:06.021
新的估值是 TD 目标

00:04:06.021 --> 00:04:09.289
我们完全忽略并替换之前的估值

00:04:09.289 --> 00:04:11.724
如果将 α 设为 0

00:04:11.724 --> 00:04:16.500
则完全忽略目标并保留旧的估值

00:04:16.500 --> 00:04:20.814
我们肯定不希望出现这种结果 因为智能体将无法学到规律

00:04:20.814 --> 00:04:24.910
但是将 α 设为一个接近 0 的小值很有帮助

00:04:24.910 --> 00:04:27.700
通常 α 越小

00:04:27.699 --> 00:04:30.769
我们在进行更新时对目标的信任就越低

00:04:30.769 --> 00:04:34.704
并且更加依赖于状态值的现有估值

00:04:34.704 --> 00:04:39.488
很快你将有机会自己设置该值

00:04:39.488 --> 00:04:43.810
现在将该更新步骤放入完整的算法中

00:04:43.810 --> 00:04:48.120
称之为一步时间差分 简称 TD

00:04:48.120 --> 00:04:51.189
一步是指在每个时间步之后

00:04:51.189 --> 00:04:54.785
都更新值函数

00:04:54.785 --> 00:04:57.760
它还称之为 TD(0)

00:04:57.759 --> 00:04:59.680
该算法旨在确定

00:04:59.680 --> 00:05:02.530
对应于某个策略的状态值函数

00:05:02.529 --> 00:05:04.504
我们将该策略表示为 π

00:05:04.504 --> 00:05:08.629
我们先将每个状态的值初始化为 0

00:05:08.629 --> 00:05:10.149
然后在每个时间步

00:05:10.149 --> 00:05:12.139
智能体都与环境互动

00:05:12.139 --> 00:05:15.310
选择由策略决定的动作

00:05:15.310 --> 00:05:20.584
从环境中获得奖励和下个状态后

00:05:20.584 --> 00:05:23.959
它会立即更新上个状态的值函数

00:05:23.959 --> 00:05:26.884
这就是连续性任务的版本

00:05:26.884 --> 00:05:30.724
只要智能体与环境互动足够长的时间

00:05:30.725 --> 00:05:34.715
该算法就应该会返回一个很好的值函数逼近结果

00:05:34.714 --> 00:05:37.868
那么阶段性任务呢？

00:05:37.869 --> 00:05:39.605
如果是阶段性任务

00:05:39.605 --> 00:05:44.773
我们只需检查在每个时间步 最近的状态是否为最终状态

00:05:44.773 --> 00:05:50.570
如果是 我们最后一次运行更新步骤以便更新上一个状态

00:05:50.569 --> 00:05:53.189
然后开始一个新的阶段

00:05:53.189 --> 00:05:56.560
原理基本是一样的

