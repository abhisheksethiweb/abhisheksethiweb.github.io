WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.424
We'll continue with the trend of addressing

00:00:02.424 --> 00:00:05.494
the prediction problem and reinforcement learning first.

00:00:05.495 --> 00:00:07.310
So, given a policy,

00:00:07.309 --> 00:00:09.765
how might we estimate its value function?

00:00:09.765 --> 00:00:13.169
Let's build up how we did this in the previous lesson.

00:00:13.169 --> 00:00:14.550
In our Monte Carlo approach,

00:00:14.550 --> 00:00:17.760
the agent interacted with the environment in episodes.

00:00:17.760 --> 00:00:19.395
After an episode finished,

00:00:19.394 --> 00:00:23.309
we looked at every state-action pair in the sequence.

00:00:23.309 --> 00:00:24.974
If it was a first visit,

00:00:24.975 --> 00:00:30.205
we calculated the corresponding return and used it to update the action value.

00:00:30.204 --> 00:00:34.129
And we did this for many, many episodes.

00:00:34.130 --> 00:00:37.160
It's important to note that this algorithm is a solution for

00:00:37.159 --> 00:00:42.458
the prediction problem as long as we never change the policy between episodes.

00:00:42.459 --> 00:00:45.425
And as long as we run the algorithm for long enough,

00:00:45.424 --> 00:00:49.984
we're guaranteed to end with a nice estimate for the action-value function.

00:00:49.984 --> 00:00:53.500
But let's move our focus to this update step.

00:00:53.500 --> 00:00:58.945
And there is an analogous equation if we instead want to keep track of the state values.

00:00:58.945 --> 00:01:01.030
Now for the rest of this video,

00:01:01.030 --> 00:01:05.995
what we'll do is adapt this update step to come up with a new algorithm.

00:01:05.995 --> 00:01:10.480
Remember that the main idea behind this line is that the value of any state is

00:01:10.480 --> 00:01:12.910
defined as the expected return that's likely to

00:01:12.909 --> 00:01:15.909
follow that state if the agent follows the policy.

00:01:15.909 --> 00:01:20.144
So averaging sampled returns yields a good estimate.

00:01:20.144 --> 00:01:25.634
At this point, I'll remind you of the Bellman expectation equation for the state values.

00:01:25.635 --> 00:01:28.125
It gives us a way to express the value of

00:01:28.125 --> 00:01:32.890
any state in terms of the values of the states that could potentially follow.

00:01:32.890 --> 00:01:35.879
And so what if we used the equation just like the one above

00:01:35.879 --> 00:01:39.159
it to motivate a slightly different update rule?

00:01:39.159 --> 00:01:42.390
So now instead of averaging sampled returns,

00:01:42.390 --> 00:01:45.719
we average the sampled value of the sum of

00:01:45.719 --> 00:01:50.000
the immediate reward plus the discounted value of the next state.

00:01:50.000 --> 00:01:53.084
And you'll notice that we now have an update step that

00:01:53.084 --> 00:01:58.199
understands the value of a state in terms of the values of its successor states.

00:01:58.200 --> 00:02:00.240
And why would we want to do that anyway?

00:02:00.239 --> 00:02:03.314
Well, the first thing to notice is that we've removed

00:02:03.314 --> 00:02:06.929
any mention of the return that comes at the end of the episode.

00:02:06.930 --> 00:02:10.140
And in fact, this new update step gives us

00:02:10.139 --> 00:02:13.514
a way to update the state values after every time step.

00:02:13.514 --> 00:02:19.289
To see this, let's consider what would happen at an arbitrary time step t. As always,

00:02:19.289 --> 00:02:21.625
we'll use S_sub_t to denote the state.

00:02:21.625 --> 00:02:25.550
Say the agent uses the policy to pick its action A_sub_t,

00:02:25.550 --> 00:02:30.180
then it receives a reward and next state from the environment.

00:02:30.180 --> 00:02:33.960
So then what the prediction algorithm could do is use

00:02:33.960 --> 00:02:38.534
this very small time window of information to update value function.

00:02:38.534 --> 00:02:44.224
Specifically, we'll update the value of this state at time t. In order to do that,

00:02:44.224 --> 00:02:49.104
we begin by looking up the values of the states from time t and time t plus one.

00:02:49.104 --> 00:02:51.560
By also plugging in the reward,

00:02:51.560 --> 00:02:54.564
we can calculate this entire right hand side,

00:02:54.564 --> 00:02:59.050
and that's our new estimate for the value of the state at time t. Now it's

00:02:59.050 --> 00:03:01.005
important to realize that we won't have to wait

00:03:01.004 --> 00:03:04.079
anymore until the end of the episode to update the values.

00:03:04.080 --> 00:03:06.040
And this is the first algorithm you can use for

00:03:06.039 --> 00:03:09.334
the prediction problem when working with continuous tasks.

00:03:09.335 --> 00:03:11.784
But before detailing the algorithm in full,

00:03:11.784 --> 00:03:15.245
let's talk a bit more about what this update step accomplishes.

00:03:15.245 --> 00:03:17.640
So at an arbitrary time step t,

00:03:17.639 --> 00:03:19.768
before the agent takes an action,

00:03:19.769 --> 00:03:21.640
it's best estimate for the value of

00:03:21.639 --> 00:03:25.004
the current state is just what's contained in the value function.

00:03:25.004 --> 00:03:29.355
But then once it takes an action and receives the reward and next state,

00:03:29.355 --> 00:03:30.849
well that's new information.

00:03:30.849 --> 00:03:35.819
And we can use it to express an alternative estimate for the value of the same state,

00:03:35.819 --> 00:03:38.924
but in terms of the value of the state that followed.

00:03:38.925 --> 00:03:42.745
And we refer to this new estimate as the TD target.

00:03:42.745 --> 00:03:45.550
So then what this entire update equation does is

00:03:45.550 --> 00:03:48.985
find some middle ground between the two estimates.

00:03:48.985 --> 00:03:53.335
You will set the value of alpha according to which estimate you trust more.

00:03:53.335 --> 00:03:54.905
To see this more clearly,

00:03:54.905 --> 00:03:57.210
we'll rewrite the updated equation.

00:03:57.210 --> 00:04:01.270
Note that alpha must be set to a number between zero and one.

00:04:01.270 --> 00:04:03.320
When alpha is set to one,

00:04:03.319 --> 00:04:06.021
the new estimate is just the TD target,

00:04:06.021 --> 00:04:09.289
where we completely ignore and replace the previous estimate.

00:04:09.289 --> 00:04:11.724
And if we were to set alpha to zero,

00:04:11.724 --> 00:04:16.500
we would completely ignore the target and keep the old estimate unchanged.

00:04:16.500 --> 00:04:20.814
This is not something that we'd ever want to do because then our agent would never learn.

00:04:20.814 --> 00:04:24.910
But it will prove useful to set alpha to a small number that's close to zero.

00:04:24.910 --> 00:04:27.700
And in general, the smaller alpha is,

00:04:27.699 --> 00:04:30.769
the less we trust the target when performing an update,

00:04:30.769 --> 00:04:34.704
and the more we rely on our existing estimate of the state value.

00:04:34.704 --> 00:04:39.488
You'll soon get a chance to experiment with setting the value of alpha yourself.

00:04:39.488 --> 00:04:43.810
We'll now put this update step back into the context of the full algorithm,

00:04:43.810 --> 00:04:48.120
which we'll call One-Step temporal difference or TD for short.

00:04:48.120 --> 00:04:51.189
Of course the One-Step just refers to the fact that we

00:04:51.189 --> 00:04:54.785
update the value function after any individual step.

00:04:54.785 --> 00:04:57.760
You'll also see it referred to as TD zero.

00:04:57.759 --> 00:04:59.680
The algorithm is designed to determine

00:04:59.680 --> 00:05:02.530
the state value function corresponding to a policy,

00:05:02.529 --> 00:05:04.504
which we denote by pi.

00:05:04.504 --> 00:05:08.629
We begin by initializing the value of each state to zero.

00:05:08.629 --> 00:05:10.149
Then, at every time step,

00:05:10.149 --> 00:05:12.139
the agent interacts with the environment,

00:05:12.139 --> 00:05:15.310
choosing actions that are dictated by the policy.

00:05:15.310 --> 00:05:20.584
And immediately after receiving the reward and next state from the environment,

00:05:20.584 --> 00:05:23.959
it updates the value function for the previous state.

00:05:23.959 --> 00:05:26.884
So this is the version for continuous tasks.

00:05:26.884 --> 00:05:30.724
And as long as the agent interacts with the environment for long enough,

00:05:30.725 --> 00:05:34.715
the algorithm should return a nice estimate for the value function.

00:05:34.714 --> 00:05:37.868
Okay. And what about episodic tasks?

00:05:37.869 --> 00:05:39.605
Well in that case,

00:05:39.605 --> 00:05:44.773
we need only check at every time step if the most recent state is a terminal state.

00:05:44.773 --> 00:05:50.570
And if so, we run the update step one last time to update the preceding state.

00:05:50.569 --> 00:05:53.189
Then, we start a new episode,

00:05:53.189 --> 00:05:56.560
but as you can see the idea is basically the same.

