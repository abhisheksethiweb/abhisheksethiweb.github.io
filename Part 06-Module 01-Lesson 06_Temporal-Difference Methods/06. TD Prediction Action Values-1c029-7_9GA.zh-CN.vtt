WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.409
在这节课的先前阶段

00:00:01.409 --> 00:00:07.044
我们详细讲解了计算某个策略对应的状态值函数的算法

00:00:07.044 --> 00:00:09.000
现在 我们将调整该算法

00:00:09.000 --> 00:00:12.589
并返回动作值函数的估值

00:00:12.589 --> 00:00:17.589
我们来回忆下一步时间差分的原理

00:00:17.589 --> 00:00:20.170
智能体与环境互动

00:00:20.170 --> 00:00:21.560
在时间步 0

00:00:21.559 --> 00:00:24.179
收到状态 S0

00:00:24.179 --> 00:00:28.045
然后根据策略选择一个动作

00:00:28.045 --> 00:00:33.414
紧接着智能体收到奖励和下个状态

00:00:33.414 --> 00:00:36.659
此刻 智能体根据经验

00:00:36.659 --> 00:00:40.429
更新时间步 0 时状态的估算值

00:00:40.429 --> 00:00:42.359
在下个时间点

00:00:42.359 --> 00:00:44.714
智能体通过查看策略

00:00:44.715 --> 00:00:46.275
再次选择一个动作

00:00:46.274 --> 00:00:49.229
并收到奖励和下个状态

00:00:49.229 --> 00:00:54.504
然后利用该信息更新时间步 1 时的状态的值

00:00:54.505 --> 00:00:56.984
该流程继续下去

00:00:56.984 --> 00:01:00.339
智能体始终根据相同的策略选择一个动作

00:01:00.340 --> 00:01:02.985
收到奖励和下个步骤

00:01:02.984 --> 00:01:05.534
然后更新值函数

00:01:05.534 --> 00:01:07.254
问题是

00:01:07.254 --> 00:01:12.879
我们如何调整该流程 返回动作值的估值？

00:01:12.879 --> 00:01:19.194
我们不再使用与后续状态的值相关的更新方程

00:01:19.194 --> 00:01:22.809
而是需要获得一个与后续状态动作对的值

00:01:22.810 --> 00:01:27.400
相关的更新方程

00:01:27.400 --> 00:01:32.658
智能体将在每次选择动作后都更新值

00:01:32.658 --> 00:01:37.215
而不是在接收每个状态后更新值

00:01:37.215 --> 00:01:39.079
这是唯一的区别

00:01:39.079 --> 00:01:42.819
如果智能体与环境互动足够长的时间

00:01:42.819 --> 00:01:46.269
它将能够很准确地估算动作值函数

00:01:46.269 --> 00:01:48.034
在接下来的几个视频中

00:01:48.034 --> 00:01:53.000
你将详细了解如何使用该算法搜索最优策略

