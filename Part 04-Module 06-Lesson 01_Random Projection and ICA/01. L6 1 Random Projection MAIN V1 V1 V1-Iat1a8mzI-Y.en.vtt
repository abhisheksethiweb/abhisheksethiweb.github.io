WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.555
Hello, I'm J. In this lesson,

00:00:02.555 --> 00:00:05.175
we'll be talking about dimensionality reduction.

00:00:05.174 --> 00:00:08.910
The first method we'll look at in this lesson is random projection,

00:00:08.910 --> 00:00:11.460
which is a powerful dimensionality reduction method

00:00:11.460 --> 00:00:14.255
that is computationally more efficient than PCA.

00:00:14.255 --> 00:00:17.010
It is commonly used in cases where a dataset has

00:00:17.010 --> 00:00:20.405
too many dimensions for PCA to be directly computed.

00:00:20.405 --> 00:00:25.250
Let's say your application is running on a system with limited computational resources,

00:00:25.250 --> 00:00:29.329
or you just find that PCA is too taxing for a specific situation that you're in.

00:00:29.329 --> 00:00:31.289
Just like PCA, it takes a dataset.

00:00:31.289 --> 00:00:34.685
Let's say this is our dataset with d dimensions,

00:00:34.685 --> 00:00:38.469
let's say 1,000 and a certain number of samples or rows,

00:00:38.469 --> 00:00:40.320
let's say n. These are columns.

00:00:40.320 --> 00:00:42.905
So, it takes our dataset and it produces

00:00:42.905 --> 00:00:47.564
a transformation of it that is in a much smaller number of columns.

00:00:47.564 --> 00:00:50.164
So, okay, let's say 50 for example,

00:00:50.164 --> 00:00:52.899
but the same number of samples and where your each column

00:00:52.899 --> 00:00:55.869
here it captures information from multiple columns there.

00:00:55.869 --> 00:00:59.119
Let's look at an oversimplified example for reducing

00:00:59.119 --> 00:01:02.989
the dimensions of a dataset from two dimensions into one dimension.

00:01:02.990 --> 00:01:05.379
PCA here will try to maximize variance.

00:01:05.379 --> 00:01:09.679
So, it finds the vector or direction that maximizes the variance so it

00:01:09.680 --> 00:01:11.570
loses the least amount of information when it

00:01:11.569 --> 00:01:14.539
projects the data from two dimensions to one.

00:01:14.540 --> 00:01:16.430
So, that line would be something like this,

00:01:16.430 --> 00:01:18.170
and these would be the projections,

00:01:18.170 --> 00:01:19.980
and so in one dimension,

00:01:19.980 --> 00:01:21.770
the dataset would look like this.

00:01:21.769 --> 00:01:23.989
Random projection, so with this calculation that PCA

00:01:23.989 --> 00:01:26.629
did especially if you're talking about a lot of dimensions,

00:01:26.629 --> 00:01:28.409
it consumes a certain amount of resources.

00:01:28.409 --> 00:01:30.619
Random projection just say, pick a line,

00:01:30.620 --> 00:01:34.689
any line, we'll do a projection on that, that's our dataset.

00:01:34.689 --> 00:01:37.640
So, while it doesn't really make a lot of sense in

00:01:37.640 --> 00:01:40.730
our over-simplified scenario like this from two dimensions to one,

00:01:40.730 --> 00:01:44.210
it actually works and it works really well in higher dimensions,

00:01:44.209 --> 00:01:47.029
and it works in a high-performance way.

00:01:47.030 --> 00:01:50.000
The basic premise of a random projection is that we can

00:01:50.000 --> 00:01:52.790
simply reduce the number of dimensions in our dataset,

00:01:52.790 --> 00:01:56.725
by multiplying it by a random matrix like this.

00:01:56.724 --> 00:02:02.629
So, d, we will have d in our dataset but k is something we either compute or it's

00:02:02.629 --> 00:02:05.509
something that we desire and there is a way to compute what is

00:02:05.510 --> 00:02:08.640
a conservative or a good estimate for k. So,

00:02:08.639 --> 00:02:10.244
that would be the resulting dataset.

00:02:10.245 --> 00:02:12.375
Just multiplying by a random matrix,

00:02:12.375 --> 00:02:14.884
that's all random projection is in a way.

00:02:14.884 --> 00:02:17.254
Let's take a concrete example here,

00:02:17.254 --> 00:02:18.840
let's say this is our dataset,

00:02:18.840 --> 00:02:21.770
and it has 12,000 dimensions,

00:02:21.770 --> 00:02:26.570
that's our d, and it has 1,500 rows or samples.

00:02:26.569 --> 00:02:29.870
If we give this to scikit-learn and say, okay, scikit-learn,

00:02:29.870 --> 00:02:34.150
can you please do a random projection for this dataset just using your default values?

00:02:34.150 --> 00:02:36.950
It will return this dataset for us which it will be in

00:02:36.949 --> 00:02:42.174
6,200 dimensions and the same number of samples obviously.

00:02:42.175 --> 00:02:46.450
So, how do we know that it works and where does the k come from?

00:02:46.449 --> 00:02:50.419
The theoretical underpinning for random projection is this idea called

00:02:50.419 --> 00:02:53.799
the Johnson-Lindenstrauss lemma which states

00:02:53.800 --> 00:02:57.635
that a dataset of N points in high-dimensional space.

00:02:57.634 --> 00:03:00.659
So, this dataset, N points,

00:03:00.659 --> 00:03:03.509
high-dimensional space, 12,000 is pretty high.

00:03:03.509 --> 00:03:04.954
It can be mapped.

00:03:04.955 --> 00:03:06.890
Multiplying by this random matrix,

00:03:06.889 --> 00:03:12.789
down to a space in much lower dimension which is this narrow dataset in a way.

00:03:12.789 --> 00:03:14.280
This is why it's really important for us.

00:03:14.280 --> 00:03:15.770
It can be done in a way that preserves

00:03:15.770 --> 00:03:18.855
the distances between the points to a large degree.

00:03:18.854 --> 00:03:21.699
So, the distances between each two points,

00:03:21.699 --> 00:03:23.629
each pair of points in these datasets,

00:03:23.629 --> 00:03:26.990
after projection that is preserved in a certain way.

00:03:26.990 --> 00:03:29.469
That's really important because in most or

00:03:29.469 --> 00:03:32.050
in a lot of supervised and unsupervised learning,

00:03:32.050 --> 00:03:34.765
the algorithms really care about the distances between the points.

00:03:34.764 --> 00:03:37.789
So, we have a set a level of guarantee that

00:03:37.789 --> 00:03:41.914
these distances will be distorted a little bit but they can be preserved.

00:03:41.914 --> 00:03:43.750
How can they be preserved?

00:03:43.750 --> 00:03:45.389
What sort of guarantees we have?

00:03:45.389 --> 00:03:48.619
Let's take a quick example and just actually calculate that out.

00:03:48.620 --> 00:03:52.140
So, let's say we take these first two rows here,

00:03:52.139 --> 00:03:53.604
so this one and this one,

00:03:53.604 --> 00:03:58.609
the first two points in our dataset and then these are their values after projection.

00:03:58.610 --> 00:04:02.210
So, the same samples but the different level of dimensionality.

00:04:02.210 --> 00:04:05.430
So, what the Johnson-Lindentrauss lemma tells us that,

00:04:05.430 --> 00:04:11.314
the distance between the two points in the projection squared is sort of squeezed.

00:04:11.314 --> 00:04:14.810
So, it's larger than the distance between the two points in

00:04:14.810 --> 00:04:19.259
the original dataset squared times one minus Epsilon.

00:04:19.259 --> 00:04:22.009
So, epsilon is the level of error that we are

00:04:22.009 --> 00:04:25.050
allowing the random projection to have in the projection.

00:04:25.050 --> 00:04:28.100
So, the distance between the two points in

00:04:28.100 --> 00:04:31.910
the projected data set will be larger than one minus Epsilon

00:04:31.910 --> 00:04:35.270
times square of distance of the two points in the original dataset

00:04:35.269 --> 00:04:39.454
and it will be smaller than one plus Epsilon off that distance squared.

00:04:39.454 --> 00:04:41.430
I've actually calculated those numbers.

00:04:41.430 --> 00:04:45.805
So, the distance between these two points is 125.6.

00:04:45.805 --> 00:04:47.694
So, we put that in here.

00:04:47.694 --> 00:04:50.719
Epsilon, the default value so we didn't change anything,

00:04:50.720 --> 00:04:53.765
the default value in scikit-learn is 0.1.

00:04:53.764 --> 00:04:56.019
It can take any value from 0-1.

00:04:56.019 --> 00:04:58.625
So, if we do it like this,

00:04:58.625 --> 00:05:01.819
the distance between these two points is 125.8.

00:05:01.819 --> 00:05:06.409
So, this distance here will be larger than 0.9

00:05:06.410 --> 00:05:11.785
times this distance and smaller than 1.1 times this distance squared.

00:05:11.785 --> 00:05:15.235
So, you can see that Epsilon is like a liver.

00:05:15.235 --> 00:05:18.650
It goes into the calculation of how many columns are produced,

00:05:18.649 --> 00:05:21.319
and it is the level of error that we are allowing

00:05:21.319 --> 00:05:25.480
the distortion to have this reduction of dimensionality.

00:05:25.480 --> 00:05:30.660
So, this guarantee preserves the distance between every pair of points in the dataset.

00:05:30.660 --> 00:05:32.840
So, it's not just one and two,

00:05:32.839 --> 00:05:34.909
it's one and every other point,

00:05:34.910 --> 00:05:36.090
and two and every other point.

00:05:36.089 --> 00:05:38.064
So, that guarantee is there.

00:05:38.064 --> 00:05:41.404
An Epsilon is just an input into the function that we can use

00:05:41.404 --> 00:05:45.269
to say preserve the distances by this degree.

