WEBVTT
Kind: captions
Language: pt-BR

00:00:00.042 --> 00:00:01.447
Bem-vindo de volta.

00:00:01.480 --> 00:00:03.521
Neste vídeo,
vamos falar um pouco

00:00:03.554 --> 00:00:06.774
sobre algoritmos de análise
de componentes independentes.

00:00:06.807 --> 00:00:08.580
Essa será uma visão
bem avançada,

00:00:08.613 --> 00:00:10.974
mas não vamos analisar
a matemática a fundo.

00:00:11.007 --> 00:00:12.952
Vamos indicar um bom lugar
para isso.

00:00:12.985 --> 00:00:15.763
Mas é bom e importante
que você compreenda

00:00:15.796 --> 00:00:17.679
a ideia geral
de como isso funciona

00:00:17.712 --> 00:00:20.930
e quais são as suposições
que queremos usar.

00:00:20.963 --> 00:00:25.670
Vamos chamar o conjunto de dados
que temos de X.

00:00:25.703 --> 00:00:29.760
O X foi gerado
pela multiplicação

00:00:29.793 --> 00:00:32.491
do que chamaríamos
de "matriz de mistura",

00:00:32.524 --> 00:00:34.794
que é A,
com as fontes de sinais,

00:00:34.827 --> 00:00:36.444
que também não temos.

00:00:36.477 --> 00:00:38.112
Não temos A, não temos S,

00:00:38.145 --> 00:00:40.978
mas S é o que queremos
calcular no final das contas.

00:00:41.011 --> 00:00:44.858
Se X = AS,
podemos dizer que S,

00:00:44.891 --> 00:00:48.501
a fonte do que queremos aqui,
é W,

00:00:48.534 --> 00:00:50.657
que é o contrário de A.

00:00:50.690 --> 00:00:52.342
Se A é a matriz de mistura,

00:00:52.375 --> 00:00:54.637
podemos chamar W
de matriz de não mistura

00:00:54.670 --> 00:00:56.375
vezes X,
vezes o conjunto de dados,

00:00:56.408 --> 00:00:58.504
as gravações originais
que temos.

00:00:58.537 --> 00:01:00.643
Esta fórmula aqui.

00:01:00.676 --> 00:01:02.331
X é um input que temos,

00:01:02.364 --> 00:01:05.086
W é o que estamos
tentando calcular,

00:01:05.119 --> 00:01:06.549
S são os resultados.

00:01:06.582 --> 00:01:09.917
O algoritmo da ICA em um processo
serve para aproximar W

00:01:09.950 --> 00:01:14.031
ou encontrar o melhor W possível,
multiplicando por X os dados

00:01:14.064 --> 00:01:18.057
para produzir
os sinais originais.

00:01:18.090 --> 00:01:21.786
O algoritmo da ICA está explicado
em um trabalho chamado

00:01:21.819 --> 00:01:23.699
"Análise de Componentes
Independentes:

00:01:23.732 --> 00:01:25.481
Algoritmos e Aplicações".

00:01:25.514 --> 00:01:28.089
Fala da derivação
de tudo isso aqui.

00:01:28.122 --> 00:01:32.067
Mostra algumas maneiras de calcular
várias partes do algoritmo,

00:01:32.100 --> 00:01:35.470
mas temos que ter uma visão
bem avançada do algoritmo,

00:01:35.503 --> 00:01:37.535
chamada FastICA.

00:01:37.568 --> 00:01:38.860
Essa é uma maneira,

00:01:38.893 --> 00:01:41.170
a única que está implementada
na scikit-learn.

00:01:41.203 --> 00:01:43.770
Primeiro, temos X,
que é o conjunto de dados.

00:01:43.803 --> 00:01:45.854
Vamos centralizá-lo,
branqueá-lo,

00:01:45.887 --> 00:01:49.123
e vamos escolher uma matriz
com pesos iniciais aleatórios.

00:01:49.156 --> 00:01:50.680
Chamamos de W.

00:01:50.713 --> 00:01:52.861
No terceiro passo,
estimamos W.

00:01:52.894 --> 00:01:55.168
E W, como matriz,
contém vetores.

00:01:55.201 --> 00:01:57.919
O número de vetores,
cada um é um vetor de peso.

00:01:57.952 --> 00:02:00.551
Depois de estimá-lo,
nós o decorrelacionamos.

00:02:00.584 --> 00:02:04.920
E a decorrelação serve
para prevenir W1 e W2

00:02:04.953 --> 00:02:07.363
de converterem
os mesmos valores.

00:02:07.396 --> 00:02:09.869
Queremos que eles convirjam
em valores diferentes.

00:02:09.902 --> 00:02:14.230
Então repetimos a partir daqui
até convergirmos.

00:02:14.263 --> 00:02:17.127
Até encontrarmos um valor de W
que nos satisfaça.

00:02:17.160 --> 00:02:19.340
A maioria deles aparece
no passo número 3.

00:02:19.373 --> 00:02:23.494
Mas como a estimativa
acontece?

00:02:23.527 --> 00:02:28.524
Esta é a fórmula para estimar
cada vetor aqui.

00:02:28.557 --> 00:02:30.586
E é o valor esperado,

00:02:30.619 --> 00:02:32.742
X é o conjunto de dados,

00:02:32.775 --> 00:02:36.014
G é apenas uma função
não quadrática.

00:02:36.047 --> 00:02:38.937
Temos a habilidade de escolher
algumas delas.

00:02:38.970 --> 00:02:40.646
O que é usado normalmente,

00:02:40.679 --> 00:02:43.364
o que a scikit usa
e o que o trabalho propõe

00:02:43.397 --> 00:02:47.348
como uma das opções, é tahn,
uma função hipertangente.

00:02:47.381 --> 00:02:51.311
E a decorrelação é
calculada assim.

00:02:51.344 --> 00:02:53.235
Vamos falar
um pouco mais sobre isso,

00:02:53.268 --> 00:02:55.295
já que pode parecer
um pouco complicado.

00:02:55.328 --> 00:02:58.261
ICA pressupõe algumas coisas.

00:02:58.294 --> 00:03:02.009
Pressupõe que os componentes
são estatisticamente independentes,

00:03:02.042 --> 00:03:06.690
e o trabalho explica isso
em linguagem estatística.

00:03:06.723 --> 00:03:08.759
Também pressupõe
que os componentes

00:03:08.792 --> 00:03:12.263
devam ter distribuições
não gaussianas.

00:03:12.296 --> 00:03:14.777
A não gaussianidade
é bem importante aqui.

00:03:14.810 --> 00:03:18.212
É a chave para estimar a ICA,
e, sem isso,

00:03:18.245 --> 00:03:19.928
não conseguiríamos calcular,

00:03:19.961 --> 00:03:22.309
não conseguiríamos recuperar,
os sinais originais

00:03:22.342 --> 00:03:23.728
se eles fossem gaussianos.

00:03:23.761 --> 00:03:25.329
A partir daqui,

00:03:25.362 --> 00:03:27.029
o teorema central do limite

00:03:27.062 --> 00:03:31.657
nos diz que distribuição das somas
de variáveis independentes

00:03:31.690 --> 00:03:34.566
tende em direção
à distribuição gaussiana.

00:03:34.599 --> 00:03:36.931
Sabendo disso, pegamos W,

00:03:36.964 --> 00:03:38.744
esta matriz de peso aqui.

00:03:38.777 --> 00:03:43.294
Fazemos com que ela maximize
a não gaussianidade de W

00:03:43.327 --> 00:03:45.191
transposta em X.

00:03:45.224 --> 00:03:48.331
A não gaussianidade
ataca novamente.

00:03:48.364 --> 00:03:51.370
Mas aqui temos que calcular
a não gaussianidade

00:03:51.403 --> 00:03:55.595
porque é o termo
que o algoritmo tenta maximizar.

00:03:55.628 --> 00:03:59.777
Qual é forma de calcular
a não gaussianidade?

00:03:59.810 --> 00:04:03.890
Este termo aqui é uma aproximação
de algo chamado "negentropia".

00:04:03.923 --> 00:04:08.082
A negentropia vem
da teoria da informação,

00:04:08.115 --> 00:04:10.503
a ideia de entropia vem disso,

00:04:10.536 --> 00:04:13.398
e é uma maneira
de aproximá-la.

00:04:13.431 --> 00:04:15.268
Você não precisa
saber tudo isso,

00:04:15.301 --> 00:04:17.187
desde de que saiba
todos os pressupostos

00:04:17.220 --> 00:04:18.800
da não gaussianidade.

00:04:18.833 --> 00:04:21.757
Os componentes independentes
têm que ser independentes,

00:04:21.790 --> 00:04:24.637
senão não conseguimos
encontrá-los.

00:04:24.670 --> 00:04:26.691
Se você conhece
essas condições

00:04:26.724 --> 00:04:30.231
e conhece a situação
da qual a ICA precisa,

00:04:30.264 --> 00:04:31.499
isso é importante.

00:04:31.532 --> 00:04:35.041
Se quiser saber
os detalhes disso na derivação,

00:04:35.074 --> 00:04:37.005
o link do trabalho
está nos comentários

00:04:37.038 --> 00:04:38.211
e na descrição abaixo.

