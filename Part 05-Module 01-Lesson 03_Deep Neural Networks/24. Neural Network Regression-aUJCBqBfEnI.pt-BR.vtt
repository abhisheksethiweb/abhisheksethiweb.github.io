WEBVTT
Kind: captions
Language: pt-BR

00:00:00.400 --> 00:00:03.618
Esta é outra forma de lidar
com dados não lineares como este.

00:00:03.651 --> 00:00:06.121
Nós usamos
uma função linear por partes,

00:00:06.154 --> 00:00:09.855
que é só uma combinação de linhas
que se encaixam bem nos dados.

00:00:09.888 --> 00:00:11.072
Mas como fazemos isso?

00:00:11.105 --> 00:00:14.270
Podemos usar nossas amigas:
as redes neurais.

00:00:14.303 --> 00:00:17.375
Este é um exemplo de uma rede neural
usada para classificação,

00:00:17.408 --> 00:00:20.121
com n inputs
e algumas unidades de viés,

00:00:20.154 --> 00:00:22.424
além de várias
funções de ativação ReLU.

00:00:23.273 --> 00:00:25.235
No fim, podemos ver
uma função sigmoide.

00:00:25.268 --> 00:00:28.041
Isso é porque estamos usando a rede
para classificação,

00:00:28.074 --> 00:00:32.427
então queremos que a resposta final
seja um número entre 0 e 1.

00:00:32.460 --> 00:00:34.983
E se não precisarmos da rede
para classificação,

00:00:35.016 --> 00:00:36.371
mas para regressão?

00:00:36.404 --> 00:00:38.641
Não queremos retornar
um número de 0 a 1,

00:00:38.674 --> 00:00:41.098
mas qualquer número.

00:00:41.131 --> 00:00:43.017
Vou mostrar
um truque muito simples.

00:00:43.050 --> 00:00:46.267
Só temos que remover
esta unidade sigmoide final

00:00:46.300 --> 00:00:49.555
e retornar o valor
que tínhamos antes.

00:00:49.588 --> 00:00:53.940
Esse valor é a soma ponderada
dos outputs da camada anterior.

00:00:53.973 --> 00:00:55.263
Só isso.

00:00:55.296 --> 00:00:58.799
Para treinar esta rede, usaremos
outro erro, ou função de perda.

00:00:58.832 --> 00:01:02.652
Como já vimos nesta seção, será
a função do erro quadrático médio,

00:01:02.685 --> 00:01:06.427
ou o quadrado da diferença
entre o rótulo e a predição,

00:01:06.460 --> 00:01:09.140
isto é, (y-ý)².

00:01:09.173 --> 00:01:12.635
É claro que vamos calcular a média
de todos os pontos,

00:01:12.668 --> 00:01:15.613
mas, se usarmos essa função
aliada à retropropagação,

00:01:15.646 --> 00:01:17.497
poderemos treinar
a rede neural,

00:01:17.530 --> 00:01:19.813
assim como fizemos
para a classificação.

00:01:19.846 --> 00:01:22.524
É assim que eu visualizo
a rede neural de regressão.

00:01:22.557 --> 00:01:25.368
Na camada oculta, há
várias funções lineares e os inputs,

00:01:25.401 --> 00:01:28.122
que são x
e a unidade de viés, 1.

00:01:28.155 --> 00:01:30.926
As funções lineares
são apenas linhas.

00:01:30.959 --> 00:01:33.764
Depois as mesclamos
com uma função ReLU,

00:01:33.797 --> 00:01:37.775
convertendo a parte da linha que é
negativa, ou está abaixo do eixo X,

00:01:37.808 --> 00:01:39.162
no valor 0.

00:01:39.195 --> 00:01:41.433
A camada seguinte pega
combinações lineares

00:01:41.466 --> 00:01:44.185
dessas funções lineares
mescladas com as funções ReLU.

00:01:44.218 --> 00:01:48.059
As combinações lineares são assim:
parecem funções lineares por partes.

00:01:48.092 --> 00:01:50.048
Podemos usar
outras funções de ativação,

00:01:50.081 --> 00:01:51.497
como a sigmoide ou tanh.

00:01:51.530 --> 00:01:53.777
Treinaríamos a rede
com retropropagação,

00:01:53.810 --> 00:01:55.725
para minimizar
o erro quadrático médio.

00:01:55.758 --> 00:01:57.334
Essas redes seriam assim.

00:01:57.367 --> 00:01:59.512
Mesclaríamos funções lineares
com sigmoides

00:01:59.545 --> 00:02:02.375
e obteríamos
combinações lineares.

00:02:02.408 --> 00:02:04.421
Em geral, esta é
uma ótima notícia.

00:02:04.454 --> 00:02:07.594
Significa que podemos usar
as redes neurais para classificação

00:02:07.627 --> 00:02:08.917
e também para regressão.

00:02:08.950 --> 00:02:11.594
Basta removermos
a função de ativação final.

