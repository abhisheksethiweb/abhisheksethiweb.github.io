WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.770
到目前为止

00:00:01.770 --> 00:00:04.529
你已经使用了策略评估来计算策略的值函数

00:00:04.530 --> 00:00:08.250
判断该策略的效果如何

00:00:08.250 --> 00:00:12.539
你还使用了策略完善

00:00:12.539 --> 00:00:18.059
即使用策略的值函数构建好于或等同于当前策略的新策略

00:00:18.059 --> 00:00:22.589
我提到将这两种算法相结合

00:00:22.589 --> 00:00:28.019
可以生成一个能够不断得出越来越好的策略的算法

00:00:28.019 --> 00:00:31.125
将这两步相结合的算法叫做策略迭代

00:00:31.125 --> 00:00:35.130
它是我们现在的讲解重点

00:00:35.130 --> 00:00:39.790
该算法先对最优策略进行初始猜测

00:00:39.789 --> 00:00:41.429
从对等概率随机策略开始

00:00:41.429 --> 00:00:44.310
比较合适

00:00:44.310 --> 00:00:48.475
对于每个状态 选择每个动作的概率都是一样的

00:00:48.475 --> 00:00:54.160
然后我们将通过策略评估获得相应的值函数

00:00:54.159 --> 00:01:00.027
接着 我们将通过策略完善获得更好或对等的策略

00:01:00.027 --> 00:01:02.820
然后通过策略评估和策略完善

00:01:02.820 --> 00:01:05.805
重复这一循环过程

00:01:05.805 --> 00:01:07.830
直到最终的完善步骤

00:01:07.829 --> 00:01:12.829
使策略没有任何变化

00:01:12.829 --> 00:01:16.125
比较好的方面是 对于有限的 MDP

00:01:16.125 --> 00:01:19.230
我们遇到了最优策略收敛情况

00:01:19.230 --> 00:01:21.719
在下个部分

00:01:21.719 --> 00:01:24.000
你将有机会将你所写的所有代码相结合

00:01:24.000 --> 00:01:29.000
最终帮助你的智能体使用策略迭代获得最优策略

