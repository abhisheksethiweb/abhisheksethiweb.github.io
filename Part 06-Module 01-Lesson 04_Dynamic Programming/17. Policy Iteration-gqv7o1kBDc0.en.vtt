WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.770
At this point in the lesson,

00:00:01.770 --> 00:00:04.529
you've used policy evaluation to determine how

00:00:04.530 --> 00:00:08.250
good a policy is by calculating its value function.

00:00:08.250 --> 00:00:12.539
You've also used policy improvement which uses the value function for

00:00:12.539 --> 00:00:18.059
a policy to construct a new policy that's better than or equal to the current one.

00:00:18.059 --> 00:00:22.589
I mentioned that it will make sense to combine these two algorithms to

00:00:22.589 --> 00:00:28.019
produce an algorithm that successively proposes better and better policies.

00:00:28.019 --> 00:00:31.125
The name for the algorithm that combines these two steps

00:00:31.125 --> 00:00:35.130
is policy iteration and it's our current focus.

00:00:35.130 --> 00:00:39.790
The algorithm begins with an initial guess for the optimal policy.

00:00:39.789 --> 00:00:41.429
It makes sense to begin with

00:00:41.429 --> 00:00:44.310
the equal probable random policy where for

00:00:44.310 --> 00:00:48.475
each state each action is equally likely to be chosen.

00:00:48.475 --> 00:00:54.160
Then we'll use policy evaluation to obtain the corresponding value function.

00:00:54.159 --> 00:01:00.027
Next, we'll use policy improvement to obtain a better or equivalent policy.

00:01:00.027 --> 00:01:02.820
Then we just repeat this loop over and over with

00:01:02.820 --> 00:01:05.805
policy evaluation and then policy improvement

00:01:05.805 --> 00:01:07.830
until finally we encounter

00:01:07.829 --> 00:01:12.829
a policy improvement step that doesn't result in any change to the policy.

00:01:12.829 --> 00:01:16.125
And, what's great is that in the case of a finite MDP,

00:01:16.125 --> 00:01:19.230
we have guaranteed convergence to the optimal policy.

00:01:19.230 --> 00:01:21.719
In the next concept, you'll have the chance to

00:01:21.719 --> 00:01:24.000
combine all the code you've already written to

00:01:24.000 --> 00:01:29.000
finally help your agent use policy iteration to obtain an optimal policy.

