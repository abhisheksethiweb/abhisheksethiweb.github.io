WEBVTT
Kind: captions
Language: pt-BR

00:00:00.001 --> 00:00:03.065
Nesta lição, você foi apresentado
a um método iterativo

00:00:03.098 --> 00:00:07.153
para determinar a função valor
correspondente a uma política.

00:00:07.186 --> 00:00:10.400
Neste vídeo,
discutiremos melhor o algoritmo.

00:00:10.433 --> 00:00:13.473
O algoritmo se chama
Avaliação de Política Iterativa

00:00:13.506 --> 00:00:17.133
e supõe que o agente
já tem total conhecimento

00:00:17.166 --> 00:00:20.148
sobre o MDP
que caracteriza o ambiente.

00:00:20.720 --> 00:00:22.904
Lembre-se que este algoritmo
foi motivado

00:00:22.937 --> 00:00:27.090
pela equação de expectativa
de Bellman da função valor-estado.

00:00:27.123 --> 00:00:29.732
Na verdade, essa equação é
um sistema de equações,

00:00:29.765 --> 00:00:33.384
já que temos uma equação
para cada estado do ambiente.

00:00:33.417 --> 00:00:35.779
Cada equação relaciona
o valor de um estado

00:00:35.812 --> 00:00:38.241
com os valores
dos seus estados seguintes.

00:00:38.956 --> 00:00:41.435
Teoricamente, como temos
um sistema de equações

00:00:41.468 --> 00:00:44.089
com uma equação
para cada quantidade desconhecida,

00:00:44.122 --> 00:00:46.240
nós poderíamos resolver
o sistema.

00:00:46.273 --> 00:00:49.803
Mas é mais fácil, em vez de resolver
o sistema exatamente,

00:00:49.836 --> 00:00:51.902
construir
um algoritmo iterativo

00:00:51.935 --> 00:00:54.430
em que cada passo
nos aproxima cada vez mais

00:00:54.463 --> 00:00:57.212
de resolver
o sistema de equações.

00:00:57.937 --> 00:01:00.237
Especificamente,
nosso algoritmo iterativo

00:01:00.270 --> 00:01:02.149
vai adaptar
a equação de Bellman

00:01:02.182 --> 00:01:04.834
para que ela seja usada
como uma regra de atualização.

00:01:04.867 --> 00:01:09.146
Aqui o V representa o palpite atual
para a função valor da política.

00:01:09.179 --> 00:01:12.115
Veja que eu só mudei duas coisas
na equação de Bellman,

00:01:12.148 --> 00:01:14.683
sublinhadas aqui em amarelo.

00:01:15.066 --> 00:01:18.011
Então o algoritmo começa
com um palpite inicial

00:01:18.044 --> 00:01:20.304
para a função valor
da política.

00:01:21.033 --> 00:01:24.949
A praxe é definir o palpite inicial
para cada estado como 0.

00:01:25.509 --> 00:01:28.972
Bom, certamente esta não é
a função valor verdadeira,

00:01:29.005 --> 00:01:31.147
mas precisamos começar
de algum lugar.

00:01:31.785 --> 00:01:34.817
Então observamos os estados
e usamos a regra de atualização

00:01:34.850 --> 00:01:38.050
para melhorar nosso palpite
para a função valor-estado.

00:01:38.083 --> 00:01:42.034
O bom é que, se alguns
requisitos técnicos forem cumpridos,

00:01:42.067 --> 00:01:46.855
esse algoritmo certamente convergirá
para a função valor da política.

00:01:47.345 --> 00:01:49.359
Mas nós só alcançaremos
essa convergência

00:01:49.392 --> 00:01:52.807
se executarmos esse algoritmo
um número infinito de vezes,

00:01:52.840 --> 00:01:55.095
o que, na prática,
é inviável.

00:01:55.128 --> 00:01:58.560
Então vamos parar um pouco antes
de obter a convergência verdadeira.

00:01:59.145 --> 00:02:02.621
A pergunta é: como vamos saber
se já estamos perto o suficiente?

00:02:03.578 --> 00:02:06.024
Na prática, quando você
implementar o algoritmo,

00:02:06.057 --> 00:02:09.079
verá que, nas primeiras vezes
que a atualização for aplicada,

00:02:09.112 --> 00:02:12.290
a função valor
vai mudar muito.

00:02:12.323 --> 00:02:14.692
Mas, por fim,
em um momento posterior,

00:02:14.725 --> 00:02:18.138
você verá que as atualizações
serão quase imperceptíveis.

00:02:18.171 --> 00:02:21.129
E, quando você perceber
que aplicar a regra de atualização

00:02:21.162 --> 00:02:24.192
não muda muito
a estimativa da função valor,

00:02:24.225 --> 00:02:27.748
será um forte indício
de que o algoritmo está bem próximo

00:02:27.781 --> 00:02:30.427
de convergir
para a função valor verdadeira.

00:02:30.460 --> 00:02:34.094
Inspirados por esse fato,
podemos criar um critério de parada,

00:02:34.127 --> 00:02:38.557
que encerra o algoritmo
ao passarmos por todos os estados

00:02:38.590 --> 00:02:41.740
e nenhum dos valores
mudar muito.

00:02:41.773 --> 00:02:43.957
Para fazer isso,
emendaremos o pseudocódigo,

00:02:43.990 --> 00:02:48.998
e o seu 1º passo será definir
um número positivo bem baixo, teta.

00:02:49.902 --> 00:02:53.836
Em seguida, você define
que o 1º palpite para a função valor

00:02:53.869 --> 00:02:55.749
será sempre 0.

00:02:55.782 --> 00:02:57.723
Depois você define
o loop iterativo,

00:02:57.756 --> 00:03:00.248
aplicando a atualização de Bellman
a cada passo

00:03:00.281 --> 00:03:04.655
e verificando o quanto
os valores de cada estado mudaram.

00:03:04.688 --> 00:03:06.920
Se a mudança máxima
de todos os estados

00:03:06.953 --> 00:03:10.150
for menor que o número baixo teta
que você definiu,

00:03:10.183 --> 00:03:11.617
você para o algoritmo

00:03:11.650 --> 00:03:15.406
e indica que sua estimativa
para a função valor é satisfatória.

00:03:15.439 --> 00:03:16.547
Só isso.

00:03:16.580 --> 00:03:19.508
Mas, se você achar este passo
de atualização meio suspeito,

00:03:19.541 --> 00:03:23.274
eu entendo, e a sua intuição
de querer uma prova está certa.

00:03:23.307 --> 00:03:27.571
Não é intuitivo que esse algoritmo
funcione do jeito que funciona.

00:03:27.604 --> 00:03:31.102
Mas veja a ideia geral por trás
do que o passo de atualização faz.

00:03:31.707 --> 00:03:33.570
Lembre-se
que observamos os estados

00:03:33.603 --> 00:03:37.355
e aplicamos a atualização
a um estado de cada vez.

00:03:37.388 --> 00:03:39.028
E, a partir de cada estado,

00:03:39.061 --> 00:03:43.265
o agente pode escolher qualquer uma
das várias ações possíveis

00:03:43.298 --> 00:03:47.484
que o conduzam a qualquer um
dos estados seguintes possíveis.

00:03:47.517 --> 00:03:51.580
E a equação de Bellman nos ajuda
a relacionar o valor do estado pai

00:03:51.613 --> 00:03:55.347
com os valores de todos
os seus estados seguintes possíveis.

00:03:55.380 --> 00:03:59.823
Agora vemos a função valor estimada
do estado pai,

00:03:59.856 --> 00:04:03.310
assim como os valores
dos estados seguintes.

00:04:03.343 --> 00:04:05.906
E, se inserirmos esses valores
na equação de Bellman

00:04:05.939 --> 00:04:07.963
e não satisfazermos
a equação,

00:04:07.996 --> 00:04:11.120
mudaremos
o valor do estado pai

00:04:11.153 --> 00:04:14.332
para satisfazer
a equação de Bellman.

00:04:14.365 --> 00:04:17.700
Depois fazemos a mesma coisa
para o 2º estado e assim por diante.

00:04:17.733 --> 00:04:19.069
É importante notar

00:04:19.102 --> 00:04:22.252
que, se em algum momento
aplicarmos a regra de atualização

00:04:22.285 --> 00:04:26.603
e não notarmos nenhuma mudança
nos valores de nenhum estado,

00:04:26.636 --> 00:04:31.362
é porque temos uma função valor
que satisfaz à equação de Bellman,

00:04:31.395 --> 00:04:33.445
e a única função valor
que faz isso

00:04:33.478 --> 00:04:36.036
é a função valor verdadeira
da política.

00:04:36.618 --> 00:04:38.826
Nós não nos aprofundaremos nisso
neste vídeo,

00:04:38.859 --> 00:04:41.331
mas, como sempre,
se quiser falar mais sobre isso,

00:04:41.364 --> 00:04:43.933
você pode tirar dúvidas
durante o expediente

00:04:43.966 --> 00:04:46.226
ou postá-las
nos canais adequados.

00:04:46.259 --> 00:04:50.142
Mas agora é hora de você
implementar sozinho esse algoritmo.

