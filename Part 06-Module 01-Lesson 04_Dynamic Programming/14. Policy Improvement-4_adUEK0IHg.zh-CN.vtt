WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.254
希望你在迷你项目的第一部分

00:00:02.254 --> 00:00:06.459
成功地实现了迭代策略评估

00:00:06.459 --> 00:00:12.160
你可以使用你的算法评估任何有限 MDP 的策略

00:00:12.160 --> 00:00:15.775
不一定非要使用冰冻湖泊环境

00:00:15.775 --> 00:00:17.984
只需记住

00:00:17.984 --> 00:00:21.695
策略评估要求智能体完全了解 MDP

00:00:21.695 --> 00:00:24.060
然后 你可以通过迷你项目的第二部分

00:00:24.059 --> 00:00:27.714
获得相应的动作值函数

00:00:27.714 --> 00:00:31.484
现在 我们继续搜索最优策略

00:00:31.484 --> 00:00:34.935
策略评估已经使我们进一步接近最优策略

00:00:34.935 --> 00:00:38.100
毕竟 为了得出最佳策略

00:00:38.100 --> 00:00:41.590
能够评估候选策略是有帮助的

00:00:41.590 --> 00:00:45.145
在本视频中 你将学习策略完善

00:00:45.145 --> 00:00:49.320
该算法使用策略的值函数

00:00:49.320 --> 00:00:53.984
提出一个至少和当前策略一样好的新策略

00:00:53.984 --> 00:00:56.549
为何要这么做呢？

00:00:56.549 --> 00:00:59.309
很快我们就会发现

00:00:59.310 --> 00:01:04.644
策略评估和策略完善可以完美结合

00:01:04.644 --> 00:01:09.625
策略评估获得一个策略并生成值函数

00:01:09.625 --> 00:01:12.239
然后我们使用该值函数和策略完善方法

00:01:12.239 --> 00:01:17.174
获得一个潜在完善的新策略

00:01:17.174 --> 00:01:20.614
然后代入该新策略

00:01:20.614 --> 00:01:23.549
再次进行策略评估

00:01:23.549 --> 00:01:28.959
然后进行策略完善 不断重复下去 直到我们收敛于最优策略

00:01:28.959 --> 00:01:31.140
我们通过这节课先前阶段用到的网格示例

00:01:31.140 --> 00:01:34.810
进一步了解这一过程

00:01:34.810 --> 00:01:37.500
注意 这是阶段性任务

00:01:37.500 --> 00:01:41.734
唯一的最终状态是右下角的状态

00:01:41.734 --> 00:01:45.629
我们假设智能体已经知道关于该环境的所有信息

00:01:45.629 --> 00:01:49.670
它知道变动是如何发生的以及奖励是如何确定的

00:01:49.670 --> 00:01:52.200
尤其是 它不需要与环境互动

00:01:52.200 --> 00:01:54.856
来获得这些信息

00:01:54.856 --> 00:01:58.000
但是它依然需要确定最优策略

00:01:58.000 --> 00:02:02.745
假设智能体先对最优策略进行初始猜测

00:02:02.745 --> 00:02:05.280
在先前的视频中 我们发现

00:02:05.280 --> 00:02:08.159
从对等概率随机策略开始比较合适

00:02:08.159 --> 00:02:13.579
在每个状态下 智能体都从一组潜在动作中随机选择一个动作

00:02:13.580 --> 00:02:15.360
根据这一理论

00:02:15.360 --> 00:02:18.150
智能体通过迭代策略评估

00:02:18.150 --> 00:02:20.655
计算相应的值函数

00:02:20.655 --> 00:02:23.094
我们在先前的视频中完成了这一计算过程

00:02:23.094 --> 00:02:26.391
我们发现这是值函数

00:02:26.391 --> 00:02:28.560
现在的问题变成

00:02:28.560 --> 00:02:29.835
如何设计这个策略完善步骤

00:02:29.835 --> 00:02:35.594
以便找到至少和当前策略一样好的策略？

00:02:35.594 --> 00:02:39.979
我们将策略完善分成两个步骤

00:02:39.979 --> 00:02:45.109
第一步是将状态值函数转换成动作值函数

00:02:45.110 --> 00:02:47.655
你已经知道如何实现这一步

00:02:47.655 --> 00:02:51.585
当我们在这个小网格环境中遵循相同的流程时

00:02:51.585 --> 00:02:54.645
获得了这个动作值函数

00:02:54.645 --> 00:02:58.500
现在我们只需了解

00:02:58.500 --> 00:03:01.050
如何使用这个动作值函数

00:03:01.050 --> 00:03:05.760
获得一个好于对等概率随机策略的策略

00:03:05.759 --> 00:03:08.635
原理是 对于每个状态

00:03:08.635 --> 00:03:13.240
我们将选择最大化动作值函数的动作

00:03:13.240 --> 00:03:16.905
从左上角的状态开始

00:03:16.905 --> 00:03:19.680
1 大于 -1

00:03:19.680 --> 00:03:23.520
因此该策略是向右移动 而不是向下

00:03:23.520 --> 00:03:26.372
接着 5 大于 -1

00:03:26.372 --> 00:03:28.575
因此该策略向下移动

00:03:28.574 --> 00:03:32.155
同样 5 大于 -1

00:03:32.155 --> 00:03:34.455
因此该策略向右移动

00:03:34.455 --> 00:03:38.880
现在你可能会疑问 如果某个状态有多个动作都能

00:03:38.879 --> 00:03:44.039
最大化动作值函数 该怎么办

00:03:44.039 --> 00:03:46.268
在这种情况下 你可以进行选择

00:03:46.268 --> 00:03:49.590
你可以随机选择一个动作

00:03:49.590 --> 00:03:55.240
或者构建一个随机性策略 为任何/所有这些动作分配非零概率

00:03:55.240 --> 00:03:57.719
稍后我们将详细讲解

00:03:57.719 --> 00:04:02.400
现在深入了解为何这么做可行

00:04:02.400 --> 00:04:06.885
注意 值函数对应于当智能体在所有时间步都遵循该策略时

00:04:06.884 --> 00:04:12.564
存储预期回报的策略

00:04:12.564 --> 00:04:15.914
当我们计算动作值函数时

00:04:15.914 --> 00:04:19.653
我们查看的是当智能体在时间步 t 选择动作 a

00:04:19.653 --> 00:04:25.545
然后遵循该策略时会发生的情况

00:04:25.545 --> 00:04:31.050
构建下个策略 π′的方法是

00:04:31.050 --> 00:04:34.275
查看这个动作值函数 对于每个状态

00:04:34.274 --> 00:04:39.539
判断最适合最大化回报的第一个动作 a

00:04:39.540 --> 00:04:43.350
这样的话 紧接着就是

00:04:43.350 --> 00:04:47.800
与在所有时间步遵循旧策略 π 时相关的预期回报

00:04:47.800 --> 00:04:51.660
如果智能体一开始遵循策略 π′ 然后遵循策略 π

00:04:51.660 --> 00:04:57.080
预期回报将更高

00:04:57.079 --> 00:05:00.990
但是 有可能证明不仅在第一个时间步

00:05:00.990 --> 00:05:05.367
遵循策略 π′ 更好

00:05:05.367 --> 00:05:09.835
而且在所有时间步遵循策略 π′都更好

00:05:09.834 --> 00:05:12.810
换句话说 有可能证明

00:05:12.810 --> 00:05:17.644
策略 π′ 好于或等同于策略 π

00:05:17.644 --> 00:05:20.079
注意 为了证明这一点

00:05:20.079 --> 00:05:24.329
我们需要表示在所有状态下 策略 π′ 的值函数

00:05:24.329 --> 00:05:29.719
都大于或等于策略 π 的值函数

00:05:29.720 --> 00:05:32.235
如果你想详细了解如何完成这一证明

00:05:32.235 --> 00:05:35.850
建议你参阅教科书中的课外资料

00:05:35.850 --> 00:05:41.670
现在我们将这一步代入策略完善算法中

00:05:41.670 --> 00:05:43.830
可以看出 原理是

00:05:43.829 --> 00:05:47.459
首先根据状态值函数计算动作值函数

00:05:47.459 --> 00:05:50.969
然后为了为每个状态构建更好的策略

00:05:50.970 --> 00:05:55.855
我们只需选择一个最大化动作值函数的动作

00:05:55.855 --> 00:06:00.000
很快你将有机会自己实现该算法

