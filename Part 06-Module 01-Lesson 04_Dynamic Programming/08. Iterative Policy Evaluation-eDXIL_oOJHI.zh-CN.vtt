WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.215
在这节课的前面阶段

00:00:01.215 --> 00:00:03.450
我们介绍了计算策略

00:00:03.450 --> 00:00:07.425
对应的值函数的迭代方法

00:00:07.424 --> 00:00:10.394
在本视频中 我们将详细讲解该算法

00:00:10.394 --> 00:00:13.605
该算法称为迭代策略评估

00:00:13.605 --> 00:00:15.960
假设智能体已经完全了解

00:00:15.960 --> 00:00:20.940
关于环境的 MDP

00:00:20.940 --> 00:00:23.220
注意 该算法的依据是

00:00:23.219 --> 00:00:26.914
状态值函数的贝尔曼预期方程

00:00:26.914 --> 00:00:29.875
该方程本质上是个方程组

00:00:29.875 --> 00:00:33.164
因为每个环境状态都有一个方程

00:00:33.164 --> 00:00:38.994
每个方程将状态值与后续状态的值关联起来

00:00:38.994 --> 00:00:41.625
理论上来说 如果有一个方程组

00:00:41.625 --> 00:00:44.234
每个未知元素都有一个方程

00:00:44.234 --> 00:00:46.354
那么可以求解该方程组

00:00:46.354 --> 00:00:49.959
但是还有一种更简单的方法 我们不用完全求解该方程组

00:00:49.960 --> 00:00:53.340
而是构建一个迭代算法

00:00:53.340 --> 00:00:57.435
每一步都使我们越来越接近于求解该方程组

00:00:57.435 --> 00:01:02.355
具体而言 我们的迭代算法将调整该贝尔曼方程

00:01:02.354 --> 00:01:04.579
因此可以当做更新规则

00:01:04.579 --> 00:01:09.420
大写的 V 表示对策略的值函数的当前猜测

00:01:09.420 --> 00:01:11.460
注意 我只更改了贝尔曼方程的两个部分

00:01:11.459 --> 00:01:15.149
我用黄色标出来了

00:01:15.150 --> 00:01:21.109
该算法先对策略的值函数进行初始猜测

00:01:21.109 --> 00:01:25.150
通常是将每个状态的初始猜测设为 0

00:01:25.150 --> 00:01:27.990
因此不可能这就是真正的值函数

00:01:27.989 --> 00:01:31.884
但是我们只是进行一个初始猜测

00:01:31.885 --> 00:01:34.200
然后我们将循环访问每个状态

00:01:34.200 --> 00:01:37.844
并使用更新规则改善对状态值函数的猜测

00:01:37.844 --> 00:01:42.164
真正妙的是只要满足几个技术性条件

00:01:42.165 --> 00:01:47.600
这个算法就能保证会向该策略的值函数收敛

00:01:47.599 --> 00:01:50.159
要获得真正的收敛效果

00:01:50.159 --> 00:01:52.909
就必须运行该算法无限次

00:01:52.909 --> 00:01:55.200
这在现实中是不可行的

00:01:55.200 --> 00:01:58.719
因此我们将在刚要真正收敛之前停止

00:01:58.719 --> 00:02:03.655
问题是 如何判断已经很接近了？

00:02:03.655 --> 00:02:06.150
在现实中 当你实现该算法时

00:02:06.150 --> 00:02:09.289
你将发现前几次应用更新规则时

00:02:09.289 --> 00:02:12.044
值函数的变化非常大

00:02:12.044 --> 00:02:14.864
但是在后续阶段

00:02:14.865 --> 00:02:17.905
你将发现几乎没有变化

00:02:17.905 --> 00:02:20.400
当我们应用更新规则后

00:02:20.400 --> 00:02:24.200
值函数的估值变化不大

00:02:24.199 --> 00:02:26.939
这种情况就强烈表示

00:02:26.939 --> 00:02:30.104
我们的算法已经非常收敛于真正的值函数

00:02:30.104 --> 00:02:32.234
因此

00:02:32.235 --> 00:02:34.980
我们可以设计一个停止条件

00:02:34.979 --> 00:02:38.759
当我们完整地遍历了所有状态

00:02:38.759 --> 00:02:41.459
所有值都没有什么变化时 则停止该算法

00:02:41.460 --> 00:02:45.390
为此 我们将修改伪代码

00:02:45.389 --> 00:02:49.944
第一步是需要你来设置一个非常小的正 θ

00:02:49.944 --> 00:02:55.989
然后和之前一样 将值函数的初始猜测全设为 0

00:02:55.990 --> 00:02:59.460
然后进行迭代循环

00:02:59.460 --> 00:03:01.560
在每一步应用贝尔曼更新规则

00:03:01.560 --> 00:03:04.949
检查每个状态的值变化幅度

00:03:04.949 --> 00:03:07.259
如果所有状态的最大变化

00:03:07.259 --> 00:03:10.169
小于所设置的小数字 θ

00:03:10.169 --> 00:03:12.239
则停止该算法

00:03:12.240 --> 00:03:15.254
并表示值函数的估值已经很不错

00:03:15.254 --> 00:03:20.805
就这样 如果你不太熟悉这个更新步骤 也不用担心

00:03:20.805 --> 00:03:23.469
你的直觉是想证明这一步 这很正常

00:03:23.469 --> 00:03:27.740
这个算法并不是那么的直观

00:03:27.740 --> 00:03:31.950
下面说说更新步骤的原理

00:03:31.949 --> 00:03:37.079
我们循环访问状态 并且每次向一个状态应用更新规则

00:03:37.080 --> 00:03:39.195
然后对于每个状态

00:03:39.194 --> 00:03:41.849
智能体都可以选择任何一个潜在的动作

00:03:41.849 --> 00:03:47.284
并进入任何一个潜在后续状态

00:03:47.284 --> 00:03:50.460
贝尔曼方程可以帮助我们将此父状态的值

00:03:50.460 --> 00:03:55.615
与所有潜在后续状态的值相关联

00:03:55.615 --> 00:03:59.835
现在我们查看父状态的估算值函数

00:03:59.835 --> 00:04:03.305
以及后续状态的值

00:04:03.305 --> 00:04:06.050
如果将这些值代入贝尔曼方程

00:04:06.050 --> 00:04:07.840
会发现方程不满足

00:04:07.840 --> 00:04:10.590
我们将更改父状态的值

00:04:10.590 --> 00:04:14.550
以便满足贝尔曼方程

00:04:14.550 --> 00:04:17.685
然后对第二个状态执行相同的流程 以此类推

00:04:17.685 --> 00:04:20.610
需要注意的是

00:04:20.610 --> 00:04:23.790
如果在某个时刻应用该更新规则时

00:04:23.790 --> 00:04:27.500
任何状态的任何值都没有变化

00:04:27.500 --> 00:04:31.350
则表明 我们获得了完全满足贝尔曼方程的值函数

00:04:31.350 --> 00:04:36.715
唯一满足这一条件的值函数是真正的策略值函数

00:04:36.714 --> 00:04:39.994
在本视频中我们将不再赘述

00:04:39.995 --> 00:04:41.490
但是和往常一样 如果你想深入了解

00:04:41.490 --> 00:04:46.220
建议你在问答时间内提出问题或将问题发布到相关 slack 频道上

00:04:46.220 --> 00:04:50.000
现在该你自己来实现该算法了

