WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.919
恭喜！

00:00:01.919 --> 00:00:05.915
你已经实现了可以解决 MDP 的首个算法

00:00:05.915 --> 00:00:07.740
在这节课的剩余时间内

00:00:07.740 --> 00:00:10.679
我们将讨论该算法的一些变体形式

00:00:10.679 --> 00:00:15.750
你将有机会实现所有这些算法并比较它们的效果

00:00:15.750 --> 00:00:19.495
首先 我们将查看这个策略评估步骤

00:00:19.495 --> 00:00:21.750
注意 策略评估是一种迭代算法

00:00:21.750 --> 00:00:26.370
并依赖于贝尔曼更新规则

00:00:26.370 --> 00:00:29.760
完成一轮策略评估所需的迭代次数

00:00:29.760 --> 00:00:35.789
由你所设置的小数字 θ 决定

00:00:35.789 --> 00:00:38.689
要使你的估算状态值函数

00:00:38.689 --> 00:00:41.244
与真正的值函数越接近

00:00:41.244 --> 00:00:45.134
就要使这个超参数 θ 越小

00:00:45.134 --> 00:00:46.754
现在的问题是

00:00:46.755 --> 00:00:49.480
这里能够牺牲一定的准确性吗？

00:00:49.479 --> 00:00:54.329
毕竟谁可以决定这个策略评估算法应该持续多久呢

00:00:54.329 --> 00:00:56.948
θ 越小 时长越久

00:00:56.948 --> 00:01:00.899
但是具体多久取决于你的 MDP

00:01:00.899 --> 00:01:05.129
或许我们不再使用这个停止条件终止算法

00:01:05.129 --> 00:01:09.524
而是设定我们愿意计算的绝对迭代次数

00:01:09.525 --> 00:01:13.260
我们可以修改这个策略评估步骤

00:01:13.260 --> 00:01:16.920
在更新所有状态的值 1 次 2 次

00:01:16.920 --> 00:01:21.750
或次数达到某个正整数后 终止该算法

00:01:21.750 --> 00:01:25.814
原理是如果我们的目标是获得最优策略

00:01:25.814 --> 00:01:30.929
那么我们可以使用一个不太接近的值函数

00:01:30.930 --> 00:01:34.320
因此我们不需要完全等待准确地估算出值函数

00:01:34.319 --> 00:01:38.439
才去计算优化的策略

00:01:38.439 --> 00:01:44.650
例如 假设这是虚拟 MDP 的最优动作值函数

00:01:44.650 --> 00:01:50.189
我们可以通过单独查看每个状态获得一个最优策略

00:01:50.189 --> 00:01:53.670
然后选择最大化该函数的动作

00:01:53.670 --> 00:01:57.210
假设我们没有这个最优动作值函数

00:01:57.209 --> 00:02:01.250
我们使用的是估算很不正确的函数

00:02:01.250 --> 00:02:03.673
所有值都偏差很大

00:02:03.673 --> 00:02:08.310
但是很多状态动作对之间的相对值正确

00:02:08.310 --> 00:02:14.175
然后 如果我们使用这个严格意义上来说不正确的值函数来获得一个策略

00:02:14.175 --> 00:02:18.719
该策略实际上是相同的最优策略

00:02:18.719 --> 00:02:21.164
这就是主要原理

00:02:21.164 --> 00:02:22.259
我们不需要一个完美或几乎完美的

00:02:22.259 --> 00:02:28.000
值函数逼近结果才能获得最优策略

