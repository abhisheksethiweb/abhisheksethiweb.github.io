<!-- udacimak v1.2.0 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Summary</title>
  <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../assets/css/plyr.css">
  <link rel="stylesheet" href="../assets/css/katex.min.css">
  <link rel="stylesheet" href="../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../assets/img/udacimak.png" />
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>Dynamic Programming</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="01. Introduction.html">01. Introduction</a>
    </li>
    <li class="">
      <a href="02. OpenAI Gym FrozenLakeEnv.html">02. OpenAI Gym: FrozenLakeEnv</a>
    </li>
    <li class="">
      <a href="03. Your Workspace.html">03. Your Workspace</a>
    </li>
    <li class="">
      <a href="04. Another Gridworld Example.html">04. Another Gridworld Example</a>
    </li>
    <li class="">
      <a href="05. An Iterative Method, Part 1.html">05. An Iterative Method, Part 1</a>
    </li>
    <li class="">
      <a href="06. An Iterative Method, Part 2.html">06. An Iterative Method, Part 2</a>
    </li>
    <li class="">
      <a href="07. Quiz An Iterative Method.html">07. Quiz: An Iterative Method</a>
    </li>
    <li class="">
      <a href="08. Iterative Policy Evaluation.html">08. Iterative Policy Evaluation</a>
    </li>
    <li class="">
      <a href="09. Implementation.html">09. Implementation</a>
    </li>
    <li class="">
      <a href="10. Mini Project DP (Parts 0 and 1).html">10. Mini Project: DP (Parts 0 and 1)</a>
    </li>
    <li class="">
      <a href="11. Action Values.html">11. Action Values</a>
    </li>
    <li class="">
      <a href="12. Implementation.html">12. Implementation</a>
    </li>
    <li class="">
      <a href="13. Mini Project DP (Part 2).html">13. Mini Project: DP (Part 2)</a>
    </li>
    <li class="">
      <a href="14. Policy Improvement.html">14. Policy Improvement</a>
    </li>
    <li class="">
      <a href="15. Implementation.html">15. Implementation</a>
    </li>
    <li class="">
      <a href="16. Mini Project DP (Part 3).html">16. Mini Project: DP (Part 3)</a>
    </li>
    <li class="">
      <a href="17. Policy Iteration.html">17. Policy Iteration</a>
    </li>
    <li class="">
      <a href="18. Implementation.html">18. Implementation</a>
    </li>
    <li class="">
      <a href="19. Mini Project DP (Part 4).html">19. Mini Project: DP (Part 4)</a>
    </li>
    <li class="">
      <a href="20. Truncated Policy Iteration.html">20. Truncated Policy Iteration</a>
    </li>
    <li class="">
      <a href="21. Implementation.html">21. Implementation</a>
    </li>
    <li class="">
      <a href="22. Mini Project DP (Part 5).html">22. Mini Project: DP (Part 5)</a>
    </li>
    <li class="">
      <a href="23. Value Iteration.html">23. Value Iteration</a>
    </li>
    <li class="">
      <a href="24. Implementation.html">24. Implementation</a>
    </li>
    <li class="">
      <a href="25. Mini Project DP (Part 6).html">25. Mini Project: DP (Part 6)</a>
    </li>
    <li class="">
      <a href="26. Check Your Understanding.html">26. Check Your Understanding</a>
    </li>
    <li class="">
      <a href="27. Summary.html">27. Summary</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">27. Summary</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <h1 id="summary">Summary</h1>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2017-10-02-at-10.41.44-am.png" alt="First step of policy iteration in gridworld example (Sutton and Barto, 2017)" class="img img-fluid">
    <figcaption class="figure-caption">
      <p>First step of policy iteration in gridworld example (Sutton and Barto, 2017)</p>
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-introduction">## Introduction</h2>
<ul>
<li>In the <strong>dynamic programming</strong> setting, the agent has full knowledge of the MDP.  (This is much easier than the <strong>reinforcement learning</strong> setting, where the agent initially knows nothing about how the environment decides state and reward and must learn entirely from interaction how to select actions.)</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-an-iterative-method">### An Iterative Method</h2>
<ul>
<li>In order to obtain the state-value function <span class="mathquill ud-math">v_\pi</span> corresponding to a policy <span class="mathquill ud-math">\pi</span>, we need only solve the system of equations corresponding to the Bellman expectation equation for <span class="mathquill ud-math">v_\pi</span>.</li>
<li>While it is possible to analytically solve the system, we will focus on an iterative solution approach.</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-iterative-policy-evaluation">### Iterative Policy Evaluation</h2>
<ul>
<li><strong>Iterative policy evaluation</strong> is an algorithm used in the dynamic programming setting to estimate the state-value function <span class="mathquill ud-math">v_\pi</span>  corresponding to a policy <span class="mathquill ud-math">\pi</span>.  In this approach, a Bellman update is applied to the value function estimate until the changes to the estimate are nearly imperceptible.</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2017-09-26-at-11.03.16-pm.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-estimation-of-action-values">### Estimation of Action Values</h2>
<ul>
<li>In the dynamic programming setting, it is possible to quickly obtain the action-value function <span class="mathquill ud-math">q_\pi</span> from the state-value function <span class="mathquill ud-math">v_\pi</span> with the equation: <span class="mathquill ud-math">q_\pi(s,a) = \sum_{s'\in\mathcal{S}, r\in\mathcal{R}}p(s',r|s,a)(r+\gamma v_\pi(s'))</span>.</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/est-action.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-policy-improvement">### Policy Improvement</h2>
<ul>
<li><strong>Policy improvement</strong> takes an estimate <span class="mathquill ud-math">V</span> of the action-value function <span class="mathquill ud-math">v_\pi</span> corresponding to a policy <span class="mathquill ud-math">\pi</span>, and returns an improved (or equivalent) policy <span class="mathquill ud-math">\pi'</span>, where <span class="mathquill ud-math">\pi'\geq\pi</span>.  The algorithm first constructs the action-value function estimate <span class="mathquill ud-math">Q</span>.  Then, for each state <span class="mathquill ud-math">s\in\mathcal{S}</span>, you need only select the action <span class="mathquill ud-math">a</span> that maximizes <span class="mathquill ud-math">Q(s,a)</span>.  In other words, <span class="mathquill ud-math">\pi'(s) = \arg\max_{a\in\mathcal{A}(s)}Q(s,a)</span> for all <span class="mathquill ud-math">s\in\mathcal{S}</span>.</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/improve.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-policy-iteration">### Policy Iteration</h2>
<ul>
<li><strong>Policy iteration</strong> is an algorithm that can solve an MDP in the dynamic programming setting.  It proceeds as a sequence of policy evaluation and improvement steps, and is guaranteed to converge to the optimal policy (for an arbitrary <em>finite</em> MDP).</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/iteration.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-truncated-policy-iteration">### Truncated Policy Iteration</h2>
<ul>
<li><strong>Truncated policy iteration</strong> is an algorithm used in the dynamic programming setting to estimate the state-value function <span class="mathquill ud-math">v_\pi</span>  corresponding to a policy <span class="mathquill ud-math">\pi</span>.  In this approach, the evaluation step is stopped after a fixed number of sweeps through the state space.  We refer to the algorithm in the evaluation step as <strong>truncated policy evaluation</strong>.</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/truncated-eval.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/truncated-iter.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-value-iteration">### Value Iteration</h2>
<ul>
<li><strong>Value iteration</strong> is an algorithm used in the dynamic programming setting to estimate the state-value function <span class="mathquill ud-math">v_\pi</span> corresponding to a policy <span class="mathquill ud-math">\pi</span>.  In this approach, each sweep over the state space simultaneously performs policy evaluation and policy improvement.</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/value-iteration.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div>
          </div>
        </div>
      </main>

      <footer class="footer">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p class="text-center">
                <a href="https://github.com/udacimak/udacimak#readme" target="_blank">udacimak v1.2.0</a>
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </div>


  <script src="../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../assets/js/bootstrap.min.js"></script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });
    });
  </script>
</body>

</html>
