WEBVTT
Kind: captions
Language: zh-CN

00:00:03.960 --> 00:00:07.195
欢迎学习深度强化学习课程

00:00:07.195 --> 00:00:11.050
随着 AlphaGo 等成功故事和 OpenAI 等平台的出现

00:00:11.050 --> 00:00:15.089
强化学习领域忽然间吸引了无数人的关注

00:00:15.089 --> 00:00:19.344
对该领域的研究自上世纪 80 年代起呈稳步发展状态

00:00:19.344 --> 00:00:23.199
但是随着近期深度学习的发展 强化学习也飞速得到发展

00:00:23.199 --> 00:00:25.509
在我们学习这一单元的过程中

00:00:25.510 --> 00:00:30.640
你将设计能够执行复杂控制任务的智能体

00:00:30.640 --> 00:00:34.420
包括物理问题等简单的领域和棋类游戏

00:00:34.420 --> 00:00:39.880
以及智能体会处理原始像素数据的视频游戏 甚至包括机器人

00:00:39.880 --> 00:00:42.820
对于强化学习 我最喜欢的一个方面是

00:00:42.820 --> 00:00:46.384
观察智能体不断成长并越来越擅长于某个任务

00:00:46.384 --> 00:00:48.789
这一点并不始终容易实现

00:00:48.789 --> 00:00:51.984
但是一旦让智能体学会任务的各个细节部分

00:00:51.984 --> 00:00:54.744
它就可以完美地执行该任务

00:00:54.744 --> 00:00:58.579
这是世界上最有成就感的体验

00:00:58.579 --> 00:01:02.309
在学习深度强化学习课程之前

00:01:02.310 --> 00:01:05.629
我们来快速复习下一些基本概念

00:01:05.629 --> 00:01:08.890
强化学习问题通常都会转化为

00:01:08.890 --> 00:01:12.265
马尔可夫决策流程 简称 MDP

00:01:12.265 --> 00:01:19.704
一个 MDP 由一组状态 S 动作 A 概率 P

00:01:19.704 --> 00:01:22.885
奖励 R 和折扣因子 γ 组成

00:01:22.885 --> 00:01:27.189
P 表示不同转换和奖励的发生频率

00:01:27.189 --> 00:01:31.239
通常建模为单个联合概率

00:01:31.239 --> 00:01:35.599
任何时间步 t+1 的状态和奖励仅依赖于

00:01:35.599 --> 00:01:39.324
在上个时间步 t 的状态和采取的动作

00:01:39.325 --> 00:01:45.195
特定环境的这一特性称之为马尔可夫性

00:01:45.194 --> 00:01:48.364
我们通常对两个量感兴趣

00:01:48.364 --> 00:01:50.839
我们尝试估算或预测的

00:01:50.840 --> 00:01:53.265
状态值 V(S)

00:01:53.265 --> 00:01:56.935
以及在特定状态下采取的动作的值 Q(S,A)

00:01:56.935 --> 00:02:01.594
后者可以帮助我们判断应采取什么样的动作

00:02:01.594 --> 00:02:06.719
这两个映射或方法关系非常紧密

00:02:06.719 --> 00:02:13.344
可以帮助我们找到问题的最优策略 π* 从而最大化接收的总奖励

00:02:13.344 --> 00:02:16.740
注意 因为 MDP 本质上具有概率性

00:02:16.740 --> 00:02:18.990
因此我们无法完全确定地预测

00:02:18.990 --> 00:02:22.200
未来将获得什么样的奖励以及持续多久

00:02:22.199 --> 00:02:26.250
因此我们通常计算的是总预期奖励

00:02:26.250 --> 00:02:29.854
这时候就要提到折扣因子 γ 了

00:02:29.854 --> 00:02:32.039
在计算状态和动作值时

00:02:32.039 --> 00:02:36.200
我们用它来为未来的奖励分配更低的权重

00:02:36.200 --> 00:02:40.560
强化学习算法通常分为两大类别

00:02:40.560 --> 00:02:43.680
基于模型的方法 例如策略迭代

00:02:43.680 --> 00:02:48.330
以及需要已知转换和奖励模型的值迭代

00:02:48.330 --> 00:02:51.735
它们本质上通过动态规划并使用该模型

00:02:51.735 --> 00:02:56.810
以迭代方式计算期望的值函数和最优策略

00:02:56.810 --> 00:02:59.729
另一方面 蒙特卡洛方法和时间差分学习等

00:02:59.729 --> 00:03:05.129
不基于模型的方法不需要明确的模型

00:03:05.129 --> 00:03:08.909
它们通过执行探索性动作对环境抽样

00:03:08.909 --> 00:03:13.490
并使用获得的经验直接估计值函数

00:03:13.491 --> 00:03:18.850
这就是强化学习的简单介绍 当然还有更多内容

00:03:18.849 --> 00:03:21.019
深度强化学习是一个相对较新的术语

00:03:21.020 --> 00:03:24.965
是指使用深度学习（主要是多层神经网络）

00:03:24.965 --> 00:03:29.854
解决强化学习问题的方法

00:03:29.854 --> 00:03:34.039
强化学习通常包含有限的 MDP

00:03:34.039 --> 00:03:37.219
即状态和动作数量是有限的

00:03:37.219 --> 00:03:41.120
但是有太多的问题具有非常大的状态和动作空间

00:03:41.120 --> 00:03:45.015
甚至由连续的实数组成

00:03:45.014 --> 00:03:47.989
传统算法使用表格或字典

00:03:47.990 --> 00:03:52.625
或其他有限结构来记录状态和动作值

00:03:52.625 --> 00:03:54.995
但是不再适合此类问题

00:03:54.995 --> 00:03:58.250
因此 你在这一单元内首先将学习的是

00:03:58.250 --> 00:04:02.629
如何泛化这些算法以便适合大型连续空间

00:04:02.629 --> 00:04:07.939
这就为开发深度强化学习算法奠定了基础

00:04:07.939 --> 00:04:10.460
包括深度 Q 学习等基于值的技巧

00:04:10.460 --> 00:04:13.550
以及直接尝试优化策略的方法

00:04:13.550 --> 00:04:15.835
例如策略梯度

00:04:15.835 --> 00:04:18.965
最后 你将了解尝试结合这两类方法的更高级方法

00:04:18.964 --> 00:04:22.774
即行动者-评论者方法

00:04:22.774 --> 00:04:25.704
这些算法可能比较难懂

00:04:25.704 --> 00:04:29.154
如果你一开始觉得有挑战性 也别担心

00:04:29.154 --> 00:04:32.000
确保练习实现这些算法的核心部分

00:04:32.000 --> 00:04:35.180
并将它们应用到各种环境中

00:04:35.180 --> 00:04:36.879
观察它们的性能

00:04:36.879 --> 00:04:40.000
这是掌握深度强化学习的唯一方式

