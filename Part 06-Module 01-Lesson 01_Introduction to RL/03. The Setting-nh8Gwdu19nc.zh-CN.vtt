WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.700
在整个课程中 我们将了解

00:00:02.700 --> 00:00:05.400
互动学习概念

00:00:05.400 --> 00:00:07.980
对于强化学习

00:00:07.980 --> 00:00:12.060
我们将学习者或决策制定者称作智能体

00:00:12.060 --> 00:00:14.865
但是我非常喜欢狗狗

00:00:14.865 --> 00:00:17.789
因此我们将智能体看做一只

00:00:17.789 --> 00:00:21.899
刚刚出生的小狗 对世界一点也不了解

00:00:21.899 --> 00:00:23.324
当它第一次睁开眼睛时

00:00:23.324 --> 00:00:26.414
首先看到的是它的主人

00:00:26.414 --> 00:00:31.799
假设主人向小狗发出行为指令

00:00:31.800 --> 00:00:34.950
小狗看着主人的手势并根据观察的结果

00:00:34.950 --> 00:00:38.130
做出如何响应的决定

00:00:38.130 --> 00:00:42.075
当然 它还没有兴趣做出正确的响应

00:00:42.075 --> 00:00:46.645
主要是不惹麻烦 甚至获得奖赏

00:00:46.645 --> 00:00:50.850
刚出生的小狗非常复杂

00:00:50.850 --> 00:00:55.649
根本无法计算它在任何时间可以采取的动作数量

00:00:55.649 --> 00:00:58.409
此外 它尚不知道任何动作

00:00:58.409 --> 00:01:01.904
或者这些动作带来的影响

00:01:01.905 --> 00:01:05.579
我们需要尝试这些指令并看看发生的结果

00:01:05.579 --> 00:01:08.564
小狗会如何对主人的指令做出响应？

00:01:08.564 --> 00:01:12.969
此刻 它不可能对一种动作更偏爱

00:01:12.969 --> 00:01:15.659
假设它只是随机选择一种动作

00:01:15.659 --> 00:01:20.444
当然 它根本不知道自己的动作有什么含义

00:01:20.444 --> 00:01:23.069
在这种情况下 它选择坐下

00:01:23.069 --> 00:01:24.689
完成这一动作后

00:01:24.689 --> 00:01:26.500
它等待主人的响应

00:01:26.500 --> 00:01:29.364
可以想象等待非常长

00:01:29.364 --> 00:01:33.409
因为完全不清楚下一步会发生什么

00:01:33.409 --> 00:01:34.994
完成动作后

00:01:34.995 --> 00:01:37.255
它从主人那获得了反馈

00:01:37.254 --> 00:01:40.799
在此示例中 它获得了一次奖赏

00:01:40.799 --> 00:01:45.129
这很令他兴奋 它表现得不错

00:01:45.129 --> 00:01:46.814
我们假设

00:01:46.814 --> 00:01:49.829
它的唯一目标是最大化奖励

00:01:49.829 --> 00:01:52.974
它希望一直表现得很正面

00:01:52.974 --> 00:01:57.125
暂时是这种假设 以后也一直是这种假设

00:01:57.125 --> 00:01:59.069
现在主人发出了新的指令

00:01:59.069 --> 00:02:04.079
它需要选择下一个响应

00:02:04.079 --> 00:02:06.844
你认为呢？坐下似乎很合适

00:02:06.844 --> 00:02:08.984
但是现在指令不一样了

00:02:08.985 --> 00:02:12.660
因此不同的动作可能更合适

00:02:12.659 --> 00:02:15.944
它决定再次随机选择一个动作

00:02:15.944 --> 00:02:20.764
在此示例中 它决定奔跑并等待主人的响应

00:02:20.764 --> 00:02:25.554
糟糕 主人给出了负面反馈

00:02:25.555 --> 00:02:29.085
现在不清楚的是 奔跑是一般的负面动作

00:02:29.085 --> 00:02:33.719
还是不适合这个特定指令

00:02:33.719 --> 00:02:36.870
它只能通过系统地尝试和测试假设

00:02:36.870 --> 00:02:42.300
与主人进行更多的互动来发现结果

00:02:42.300 --> 00:02:45.689
这一流程继续下去

00:02:45.689 --> 00:02:48.449
每一刻 小狗都采取一种动作

00:02:48.449 --> 00:02:52.909
同时获得反馈并获得主人的响应

00:02:52.909 --> 00:02:54.210
在每一刻

00:02:54.210 --> 00:03:00.355
它都尽量采取合适的动作 使主人开心

00:03:00.354 --> 00:03:04.694
咋一看 当我们以这种方式看待互动时

00:03:04.694 --> 00:03:07.009
一切似乎都很简单

00:03:07.009 --> 00:03:11.269
毕竟小狗与主人的互动非常具有系统性

00:03:11.270 --> 00:03:15.284
虽然它可能需要一段时间才明白所发生的情况

00:03:15.284 --> 00:03:18.775
但是它应该最终会明白的

00:03:18.775 --> 00:03:20.985
在你学习这门课程的过程中

00:03:20.985 --> 00:03:25.460
你会发现这种小狗情形非常复杂

00:03:25.460 --> 00:03:27.900
例如 它需要在以下两种情形之间找到平衡点

00:03:27.900 --> 00:03:32.159
一方面 探索如何选择动作的潜在假设

00:03:32.159 --> 00:03:33.750
另一方面

00:03:33.750 --> 00:03:38.835
探索已有的可行有限知识

00:03:38.835 --> 00:03:43.590
也就是说 它应该满足于一个奖赏 还是希望获得更多的奖赏

00:03:43.590 --> 00:03:47.490
这是强化学习中的一个非常重要的概念

00:03:47.490 --> 00:03:49.740
虽然对于如何平衡这两项要求

00:03:49.740 --> 00:03:52.215
有一些广泛采用的技巧

00:03:52.215 --> 00:03:56.655
但是这个问题推动了一个完整的强化学习分支领域的出现

00:03:56.655 --> 00:04:00.419
并且成为一个热点研究领域

00:04:00.419 --> 00:04:02.549
另一个值得注意的要点是

00:04:02.550 --> 00:04:05.700
如果小狗真的是强化学习智能体

00:04:05.699 --> 00:04:09.714
那么它不仅关心现在可以获得的奖励

00:04:09.715 --> 00:04:12.210
而且要最大化在一生中

00:04:12.210 --> 00:04:16.650
可以获得的奖励数量

00:04:16.649 --> 00:04:18.929
因此发现在短期内效果不错的策略

00:04:18.930 --> 00:04:22.035
可能相对来说比较简单

00:04:22.035 --> 00:04:24.630
但是如果要得出长期效果不错的策略

00:04:24.629 --> 00:04:28.300
小狗就必须更加聪明

00:04:28.300 --> 00:04:34.439
通常 这种通过经验学习规律的概念从学术上来说比较有趣

00:04:34.439 --> 00:04:36.569
例如 如果你训练过小狗

00:04:36.569 --> 00:04:39.774
可能就会发现过程并不简单

00:04:39.774 --> 00:04:43.349
它的第一反应可能是每当我坐下来就能获得奖励

00:04:43.350 --> 00:04:48.635
而不是每当主人说坐下来 我坐下来

00:04:48.634 --> 00:04:50.687
就能获得奖励

00:04:50.687 --> 00:04:54.432
虽然你的智能体通常会犯错

00:04:54.432 --> 00:04:58.000
但是向你保证 它会学会更多知识的

