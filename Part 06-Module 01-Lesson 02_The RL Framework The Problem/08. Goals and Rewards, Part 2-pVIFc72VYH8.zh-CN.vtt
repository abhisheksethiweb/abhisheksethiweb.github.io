WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.145
到目前为止 我们一直都尝试运用强化学习

00:00:03.145 --> 00:00:06.640
解决机器人学习如何行走这一问题

00:00:06.639 --> 00:00:08.429
我们讲解了状态和动作

00:00:08.429 --> 00:00:11.099
但是依然需要指定奖励

00:00:11.099 --> 00:00:15.699
DeepMind 团队发表的论文中给出的奖励公式非常直观

00:00:15.699 --> 00:00:19.230
这行公式摘自 DeepMind 团队论文的附录

00:00:19.230 --> 00:00:22.835
描述了每个时间步的奖励计算方法

00:00:22.835 --> 00:00:27.785
每项都向智能体表示我们希望它能实现的某个部分目标

00:00:27.785 --> 00:00:30.365
我们来看看每一项

00:00:30.364 --> 00:00:32.969
首先 在每个时间步

00:00:32.969 --> 00:00:36.759
智能体收到一个与其前进速度成正比的奖励

00:00:36.759 --> 00:00:38.344
如果走得很快

00:00:38.344 --> 00:00:40.699
就会获得更多的奖励 但是有个上限

00:00:40.700 --> 00:00:43.880
表示为 Vmax

00:00:43.880 --> 00:00:47.740
但是它会受到惩罚 惩罚的量与应用到每个接合点的力成正比

00:00:47.740 --> 00:00:50.429
如果智能体向接合点应用了更多的力

00:00:50.429 --> 00:00:53.225
就会拿走更多的奖励以进行惩罚

00:00:53.225 --> 00:00:57.632
研究人员还希望机器人向前行走

00:00:57.631 --> 00:01:02.640
如果智能体向左 向右或垂直移动 也会受到惩罚

00:01:02.640 --> 00:01:07.875
如果智能体的身体离开道路中心 也会受到惩罚

00:01:07.875 --> 00:01:12.549
智能体需要使机器人尽量接近中心

00:01:12.549 --> 00:01:14.039
在每个时间步

00:01:14.040 --> 00:01:19.885
如果机器人还没有摔倒 智能体还会受到一些正面奖励

00:01:19.885 --> 00:01:23.984
他们将该问题视作阶段性任务 如果机器人摔倒了

00:01:23.984 --> 00:01:26.025
该阶段结束

00:01:26.025 --> 00:01:29.190
此刻 智能体在该时间点能够获得的累积奖励

00:01:29.189 --> 00:01:32.564
将是它能够获得的所有奖励

00:01:32.564 --> 00:01:35.084
这样就设计了奖励信号

00:01:35.084 --> 00:01:39.149
如果机器人完全侧重于最大化这一奖励

00:01:39.150 --> 00:01:42.510
它还会碰巧学会行走

00:01:42.510 --> 00:01:47.400
为此 首先注意 如果机器人摔倒了 这一阶段结束

00:01:47.400 --> 00:01:51.835
这样就失去了收集这种积极奖励的机会

00:01:51.834 --> 00:01:55.494
通常 如果机器人行走了 10 个时间步

00:01:55.495 --> 00:01:57.915
则只有 10 个获得奖励的机会

00:01:57.915 --> 00:02:00.450
如果一直行走了 100 步

00:02:00.450 --> 00:02:03.375
则有更多的时间收集更多的奖励

00:02:03.375 --> 00:02:05.834
如果以这种方式获得奖励

00:02:05.834 --> 00:02:09.715
智能体将尝试尽量长时间不摔倒

00:02:09.715 --> 00:02:14.474
接下来 因为奖励与前进速度成正比

00:02:14.474 --> 00:02:19.109
这样就确保机器人感受到压力

00:02:19.110 --> 00:02:21.375
朝着道路的方向尽量快速行走

00:02:21.375 --> 00:02:26.735
同时 当智能体向接合点应用太多的力时 有必要做出惩罚

00:02:26.735 --> 00:02:28.740
因为否则的话

00:02:28.740 --> 00:02:32.805
机器人可能会行走得非常古怪

00:02:32.805 --> 00:02:34.939
通过惩罚很大的力

00:02:34.939 --> 00:02:38.155
可以尝试让动作更加平缓优雅

00:02:38.155 --> 00:02:42.685
并且 我们希望智能体沿着道路行走并向前移动

00:02:42.685 --> 00:02:46.500
否则 谁知道它会走到哪里

00:02:46.500 --> 00:02:51.094
当然 机器人不能只侧重于快速行走

00:02:51.094 --> 00:02:53.509
或只向前移动

00:02:53.509 --> 00:02:56.284
或只平缓地行走

00:02:56.284 --> 00:03:00.310
或只尽量走很长的时间

00:03:00.310 --> 00:03:04.625
智能体在朝着最大化期望累积奖励目标前进的所有时间步中

00:03:04.625 --> 00:03:09.865
必须平衡所有这四个相互影响的要求

00:03:09.865 --> 00:03:14.655
Google DeepMind 团队的研究结果表明 根据这个非常简单的奖励函数

00:03:14.655 --> 00:03:19.340
智能体能够学会以非常像人类的方式行走

00:03:19.340 --> 00:03:22.009
实际上 这个奖励函数如此的简单

00:03:22.009 --> 00:03:24.979
似乎让我们觉得判断奖励很简单

00:03:24.979 --> 00:03:28.155
但通常并非如此

00:03:28.155 --> 00:03:31.844
当然 有一些反例

00:03:31.844 --> 00:03:35.115
例如 如果你指导智能体玩视频游戏

00:03:35.115 --> 00:03:37.715
奖励就只是屏幕上的得分

00:03:37.715 --> 00:03:41.200
如果指导智能体玩双陆棋

00:03:41.199 --> 00:03:43.854
奖励将仅在游戏结束时体现出来

00:03:43.854 --> 00:03:46.884
可以表示为如果智能体赢了 则是正面奖励

00:03:46.884 --> 00:03:48.834
如果输了 则是负面奖励

00:03:48.835 --> 00:03:51.400
实际上 正是奖励如此简单

00:03:51.400 --> 00:03:55.000
使得 DeepMind 的研究很有趣

