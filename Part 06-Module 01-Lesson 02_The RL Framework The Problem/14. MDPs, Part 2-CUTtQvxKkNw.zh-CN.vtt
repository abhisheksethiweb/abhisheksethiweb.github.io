WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.100
我们将回收机器人作为讲解示例

00:00:02.100 --> 00:00:06.004
并详细介绍了状态和动作

00:00:06.004 --> 00:00:08.640
在此示例中 状态对应的是

00:00:08.640 --> 00:00:11.865
机器人的电池电量

00:00:11.865 --> 00:00:15.900
有两个潜在状态 很高和很低的电量

00:00:15.900 --> 00:00:20.329
首先 考虑电池电量很高的状态

00:00:20.329 --> 00:00:24.464
机器人可以选择搜索 等待或重新充电

00:00:24.464 --> 00:00:30.554
但如果电量很高 则没必要充电

00:00:30.554 --> 00:00:34.302
因此唯一的选择是搜索或等待

00:00:34.302 --> 00:00:38.159
如果智能体选择搜索

00:00:38.159 --> 00:00:39.919
在下个时间步

00:00:39.920 --> 00:00:42.725
状态可能是很高或很低的电量

00:00:42.725 --> 00:00:47.250
假设有 70% 的可能性保持高电量

00:00:47.250 --> 00:00:50.850
因此有 30% 的可能性保持很低的电量

00:00:50.850 --> 00:00:54.480
在两种情形下 假设决定搜索

00:00:54.479 --> 00:00:58.019
会导致机器人刚好收集 4 个易拉罐

00:00:58.020 --> 00:00:59.565
因此

00:00:59.564 --> 00:01:04.140
环境向智能体提供 4 个奖励

00:01:04.141 --> 00:01:06.765
另一个选项是等待

00:01:06.765 --> 00:01:11.040
如果机器人的电量很高并决定等待

00:01:11.040 --> 00:01:14.955
等待不消耗任何电量

00:01:14.954 --> 00:01:19.719
可以保证在下个时间步电量还是很高

00:01:19.719 --> 00:01:24.864
在这种情形下 我们假设因为机器人没有去积极地搜索易拉罐

00:01:24.864 --> 00:01:30.629
它能够收集的易拉罐数量更少 假设只获得了一个易拉罐

00:01:30.629 --> 00:01:33.149
因此

00:01:33.150 --> 00:01:37.109
环境向智能体提供 1 个奖励

00:01:37.109 --> 00:01:40.454
对于电量很低的状态

00:01:40.454 --> 00:01:44.224
机器人同样有三个选择

00:01:44.224 --> 00:01:48.495
如果电量很低 它选择等待有人扔进易拉罐

00:01:48.495 --> 00:01:54.180
不消耗任何电量 下个时间步的状态将为低电量

00:01:54.180 --> 00:01:58.095
和电量很高时机器人决定等待一样

00:01:58.094 --> 00:02:01.620
智能体将获得一个奖励

00:02:01.620 --> 00:02:04.011
如果机器人重新充电

00:02:04.010 --> 00:02:06.509
则回到充电基站

00:02:06.510 --> 00:02:10.289
下个时间步的状态肯定是电量很高

00:02:10.289 --> 00:02:15.525
假设回去的路上没有收集到易拉罐 获得的奖励是 0

00:02:15.525 --> 00:02:19.200
如果搜索易拉罐 则风险很高

00:02:19.199 --> 00:02:23.289
有可能这次没有风险 在下个时间步

00:02:23.289 --> 00:02:27.724
电量依然很低 但是没有完全耗尽

00:02:27.724 --> 00:02:31.829
只是更有可能会耗尽电量

00:02:31.830 --> 00:02:37.195
必须需要有人将它放到充电基站上充电

00:02:37.194 --> 00:02:41.219
因此下个时间步的电量很高

00:02:41.219 --> 00:02:45.120
假设机器人耗尽电量的概率是 80%

00:02:45.120 --> 00:02:50.034
否则有 20% 的概率躲过这次风险

00:02:50.034 --> 00:02:51.314
对于奖励

00:02:51.314 --> 00:02:53.444
如果机器人需要我们营救它

00:02:53.444 --> 00:02:57.060
则确保对这种情形进行惩罚

00:02:57.060 --> 00:03:00.659
假设我们不管它能够收集的易拉罐总数是多少

00:03:00.659 --> 00:03:05.865
直接将奖励设为 -3

00:03:05.865 --> 00:03:08.125
但是如果机器人没有耗尽电量

00:03:08.125 --> 00:03:12.539
它收集了 4 个易拉罐并获得 4 个奖励

00:03:12.539 --> 00:03:17.425
这个图表完全描述了

00:03:17.425 --> 00:03:22.679
环境决定在任何时间点的下个状态可以采用的方法

00:03:22.680 --> 00:03:26.849
为此 我们来看一个具体示例

00:03:26.849 --> 00:03:31.844
例如 上个状态是电量很高 智能体决定搜索

00:03:31.844 --> 00:03:34.469
然后环境将抛掷一个虚拟硬币

00:03:34.469 --> 00:03:39.240
正面朝上的概率是 70%

00:03:39.240 --> 00:03:40.770
如果硬币正面朝上

00:03:40.770 --> 00:03:43.020
环境会判断下个状态是很高的电量

00:03:43.020 --> 00:03:46.575
智能体获得 4 个奖励

00:03:46.574 --> 00:03:49.289
否则 如果背面朝上

00:03:49.289 --> 00:03:53.759
下一个状态将是很低的电量 奖励将为 4

00:03:53.759 --> 00:04:00.780
再举个例子 如果上个状态是电量很低 智能体决定去搜索易拉罐

00:04:00.780 --> 00:04:03.569
环境再次抛掷一个虚拟硬币

00:04:03.569 --> 00:04:08.159
正面朝上的概率是 80%

00:04:08.159 --> 00:04:09.704
如果正面朝上

00:04:09.705 --> 00:04:12.120
环境判断下个状态是电量很高

00:04:12.120 --> 00:04:16.560
智能体获得的奖励是 -3

00:04:16.560 --> 00:04:18.480
否则 如果背面朝上

00:04:18.480 --> 00:04:23.075
下个状态是电量很低 奖励是 4

00:04:23.074 --> 00:04:25.469
但是要强调的是

00:04:25.470 --> 00:04:29.780
环境做出决策所需的信息很少

00:04:29.779 --> 00:04:32.924
它不关心在 10 个或 100 个甚至 2 个时间步之前

00:04:32.925 --> 00:04:36.845
提供给智能体的情形

00:04:36.845 --> 00:04:41.610
并且不会查看在上个时间步之前智能体采取的动作

00:04:41.610 --> 00:04:45.629
智能体的表现或收集的奖励数量

00:04:45.629 --> 00:04:51.115
不影响环境如何选择对智能体做出响应

00:04:51.115 --> 00:04:54.300
当然 可以设计与智能体的互动流程

00:04:54.300 --> 00:04:58.129
更加复杂的环境

00:04:58.129 --> 00:05:00.634
但是强化学习就是这么实现的

00:05:00.634 --> 00:05:03.409
当你自己去实现强化学习问题时

00:05:03.410 --> 00:05:08.000
就会发现这个框架非常强大

