WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:05.370
我将介绍一些我认为非常有趣的研究项目

00:00:05.370 --> 00:00:08.144
这些项目可以很好地解释

00:00:08.144 --> 00:00:11.309
我在上个视频中介绍的奖励假设

00:00:11.310 --> 00:00:16.740
Google DeepMind 最近解决了如何指导机器人行走这个问题

00:00:16.739 --> 00:00:18.629
除了其他问题领域之外

00:00:18.629 --> 00:00:21.929
他们利用一个类人机器人模拟

00:00:21.929 --> 00:00:26.594
并通过运用一些强大的强化学习技术获得了很棒的结果

00:00:26.594 --> 00:00:28.904
正如你在上个视频中所了解的

00:00:28.905 --> 00:00:31.830
为了将此问题构建成强化学习问题

00:00:31.829 --> 00:00:35.549
我们需要指定状态 动作和奖励

00:00:35.549 --> 00:00:41.789
我们会将此示例分成两个视频 并详细介绍动作

00:00:41.789 --> 00:00:46.619
动作是为了使机器人能行走必须做出的决策

00:00:46.619 --> 00:00:48.929
该类人机器人具有多个接合点

00:00:48.929 --> 00:00:51.269
动作是机器人运用到关节

00:00:51.270 --> 00:00:54.540
以便移动的力

00:00:54.539 --> 00:00:57.450
因为机器人能够通过智能方法

00:00:57.450 --> 00:01:00.249
判断在每个时间点运用什么样的力

00:01:00.249 --> 00:01:03.215
从而使其能够行走

00:01:03.215 --> 00:01:05.115
状态呢？

00:01:05.114 --> 00:01:10.694
状态是提供给智能体的环境信息 使其能够选择合理的动作

00:01:10.694 --> 00:01:14.069
在此示例中 任何时间点的状态

00:01:14.069 --> 00:01:18.314
包含所有接合点的当前位置和速度

00:01:18.314 --> 00:01:23.849
以及关于机器人所站地面的测量结果

00:01:23.849 --> 00:01:27.397
这些测量结果表示地面的平坦程度或坡度

00:01:27.397 --> 00:01:31.144
以及道路上是否有一个大的台阶 等等

00:01:31.144 --> 00:01:34.994
Google DeepMind 研究人员还添加了接触传感器数据

00:01:34.995 --> 00:01:40.730
从而判断机器人是否依然在行走 或者绊倒了

00:01:40.730 --> 00:01:44.609
原理是智能体必须根据状态中的信息

00:01:44.609 --> 00:01:48.355
规划下一步的动作

00:01:48.355 --> 00:01:50.924
毕竟 如果道路上有台阶

00:01:50.924 --> 00:01:56.015
则与完全平坦的地面相比 将需要完全不同的动作

00:01:56.015 --> 00:01:58.920
我们将奖励设计为一种反馈机制

00:01:58.920 --> 00:02:02.850
告诉智能体它选择了正确的动作

00:02:02.849 --> 00:02:06.347
奖励是告诉智能体以下反馈的一种形式

00:02:06.347 --> 00:02:09.074
“很棒 没有撞到墙上

00:02:09.074 --> 00:02:11.589
或者糟糕 没有看到台阶并摔倒了”

00:02:11.590 --> 00:02:16.000
这只是总体概念 我们将在下个视频中深入了解

