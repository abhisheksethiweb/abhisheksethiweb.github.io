WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.640
还记得第一节课的可爱小狗吗？

00:00:02.640 --> 00:00:06.269
我们用它来比喻智能体 智能体通过试错

00:00:06.269 --> 00:00:10.644
学习如何在环境中完成各种动作并最大化奖励

00:00:10.644 --> 00:00:15.114
但是通常强化学习到底是什么意思呢？

00:00:15.115 --> 00:00:16.905
你可能会惊讶地发现

00:00:16.905 --> 00:00:20.980
当我们将小狗替换成无人驾驶汽车

00:00:20.980 --> 00:00:26.414
机器人或一般的强化学习智能体时 没有什么太大的变化

00:00:26.414 --> 00:00:29.489
尤其是 RL 框架是指

00:00:29.489 --> 00:00:33.659
智能体学习如何与环境互动

00:00:33.659 --> 00:00:37.199
我们假设时间会流逝并离散化时间步

00:00:37.200 --> 00:00:38.910
在一开始的时间步中

00:00:38.909 --> 00:00:42.359
智能体会观察环境

00:00:42.359 --> 00:00:43.950
可以将这种观察结果看做

00:00:43.950 --> 00:00:46.995
环境呈现给智能体的情形

00:00:46.994 --> 00:00:52.199
然后 它必须选择合适的响应动作

00:00:52.200 --> 00:00:55.425
在下一个时间步对智能体的动作做出响应时

00:00:55.424 --> 00:00:59.149
环境向智能体呈现新的情形

00:00:59.149 --> 00:01:03.869
同时环境向智能体提供一个奖励

00:01:03.869 --> 00:01:09.759
作为回应，智能体必须选择一个动作 表示智能体是否对环境做出了正确的响应

00:01:09.760 --> 00:01:12.480
这一流程继续下去

00:01:12.480 --> 00:01:16.290
在每个时间步 环境都向智能体发送一个观察结果和奖励

00:01:16.290 --> 00:01:21.800
作为回应 智能体必须选择一个动作

00:01:21.799 --> 00:01:24.810
通常 我们不需要假设环境向智能体显示

00:01:24.810 --> 00:01:28.875
做出合理决策所需的一切信息

00:01:28.875 --> 00:01:33.165
但如果我们如此假设 数学会大大简化

00:01:33.165 --> 00:01:34.725
因此 在这门课程中

00:01:34.724 --> 00:01:37.199
我们将假设智能体能够完全观察

00:01:37.200 --> 00:01:40.605
环境所处的任何状态

00:01:40.605 --> 00:01:45.418
此时 智能体接收的不再是观察结果

00:01:45.418 --> 00:01:49.820
而是环境状态

00:01:49.819 --> 00:01:52.829
我们通过一些注释清晰地描述这一流程

00:01:52.829 --> 00:01:59.134
从起始时间步 0 开始

00:01:59.135 --> 00:02:03.480
智能体首先接收环境状态 表示为 S0

00:02:03.480 --> 00:02:07.635
0 表示时间步 0

00:02:07.635 --> 00:02:11.745
然后 根据该观察结果 智能体选择一个动作 A0

00:02:11.745 --> 00:02:15.120
在下一个时间步

00:02:15.120 --> 00:02:17.640
即时间步 1

00:02:17.639 --> 00:02:22.289
它是智能体所选动作 A0 的直接结果

00:02:22.289 --> 00:02:24.810
环境的之前状态是 S0

00:02:24.810 --> 00:02:28.099
现在转换为新的状态 S1

00:02:28.099 --> 00:02:30.284
并向智能体给出了

00:02:30.284 --> 00:02:32.530
一些奖励 R1

00:02:32.530 --> 00:02:36.080
智能体然后选择一个动作 A1

00:02:36.080 --> 00:02:42.680
在时间步 2 这一流程继续 环境传递奖励和状态

00:02:42.680 --> 00:02:47.659
然后智能体做出动作响应 以此类推

00:02:47.659 --> 00:02:50.340
当智能体与环境互动时

00:02:50.340 --> 00:02:56.414
这种互动表现为一系列的状态 动作和奖励

00:02:56.413 --> 00:03:01.349
但是 奖励将始终是与智能体最相关的数量

00:03:01.349 --> 00:03:05.370
具体而言 任何智能体的目标都是

00:03:05.370 --> 00:03:11.270
最大化期望累积奖励或在所有时间步中获得的某些奖励

00:03:11.270 --> 00:03:14.310
换句话说 它旨在寻找一种

00:03:14.310 --> 00:03:18.490
使累积奖励很有可能最高的动作选择策略

00:03:18.490 --> 00:03:23.879
智能体只有通过与环境互动 才能实现这一目标

00:03:23.879 --> 00:03:26.025
因为在每个时间步

00:03:26.025 --> 00:03:30.414
环境都决定智能体会接收多少奖励

00:03:30.414 --> 00:03:34.139
也就是说 智能体必须遵守环境规则

00:03:34.139 --> 00:03:36.779
但是通过互动 智能体能够学习这些规则

00:03:36.780 --> 00:03:40.763
并选择合适的动作来实现它的目标

00:03:40.763 --> 00:03:44.509
这正是我们在这门课程中要学习的内容

00:03:44.509 --> 00:03:47.399
但是要强调的是

00:03:47.400 --> 00:03:50.605
所有这些只是现实问题的数学模型

00:03:50.604 --> 00:03:52.919
如果你想到了一个

00:03:52.919 --> 00:03:55.559
你认为可以用强化学习解决的问题

00:03:55.560 --> 00:03:58.155
你需要指定状态 动作和奖励

00:03:58.155 --> 00:04:03.270
并且需要制定环境规则

00:04:03.270 --> 00:04:07.000
在这门课程中 你将见到很多如何实现这种流程的示例

