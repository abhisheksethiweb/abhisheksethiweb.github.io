WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.469
我们了解到 强化学习框架使我们能够研究

00:00:04.469 --> 00:00:09.504
智能体如何通过与环境互动实现目标

00:00:09.505 --> 00:00:13.500
这种框架适合很多现实应用

00:00:13.500 --> 00:00:18.780
并将互动简化为在智能体和环境之间传递的三种信号

00:00:18.780 --> 00:00:23.960
状态信号是环境向智能体呈现情形的方式

00:00:23.960 --> 00:00:28.530
智能体然后做出动作响应并影响到环境

00:00:28.530 --> 00:00:31.679
环境做出奖励响应

00:00:31.679 --> 00:00:37.454
表示智能体是否对环境做出了正确响应

00:00:37.454 --> 00:00:43.239
该框架还包括智能体目标 即最大化累积奖励

00:00:43.240 --> 00:00:47.645
这到底是什么意思 智能体如何实现这一目标？

00:00:47.645 --> 00:00:49.960
朝着该目标前进是什么意思？

00:00:49.960 --> 00:00:54.255
智能体可以只在每个时间步最大化奖励吗？

00:00:54.255 --> 00:00:56.969
简短答案是不可以

00:00:56.969 --> 00:01:00.789
但是我认为可能需要更详细的答案

00:01:00.789 --> 00:01:04.609
我们用步行机器人作为讲解示例

00:01:04.609 --> 00:01:06.239
注意 对于此示例

00:01:06.239 --> 00:01:09.420
机器人的目标是尽量向前行走很长的时间

00:01:09.420 --> 00:01:14.820
并且速度很快 同时表现得很自然 不用卖力行走

00:01:14.819 --> 00:01:16.439
如果机器人尝试在单个时间步

00:01:16.439 --> 00:01:19.834
最大化收到的奖励

00:01:19.834 --> 00:01:23.099
则是尝试尽量快速移动 动作自然

00:01:23.099 --> 00:01:27.099
并且不会立即摔倒

00:01:27.099 --> 00:01:31.199
短期内效果很好

00:01:31.200 --> 00:01:35.844
但是如果智能体快速移动 一开始不会摔倒

00:01:35.844 --> 00:01:38.370
但是前面的动作使得机器人不够稳定

00:01:38.370 --> 00:01:41.969
导致很快就会摔倒

00:01:41.969 --> 00:01:45.674
这样的话 如果智能体侧重于单个时间步的目标

00:01:45.674 --> 00:01:49.344
它会学习最大化初始奖励的动作

00:01:49.344 --> 00:01:52.049
但是这一阶段很快就会结束

00:01:52.049 --> 00:01:54.944
因此累积奖励很小

00:01:54.944 --> 00:01:57.269
更糟糕的是

00:01:57.269 --> 00:02:00.354
智能体将无法学会行走

00:02:00.355 --> 00:02:01.545
在此示例中

00:02:01.545 --> 00:02:06.254
很明显智能体不能侧重于单个时间步

00:02:06.254 --> 00:02:08.775
而是需要考虑所有时间步

00:02:08.775 --> 00:02:13.289
一般的强化学习智能体也是这样

00:02:13.289 --> 00:02:18.030
动作有短期后果和长期后果

00:02:18.030 --> 00:02:23.289
智能体需要了解动作对环境的复杂影响

00:02:23.289 --> 00:02:26.155
对于步行机器人示例

00:02:26.155 --> 00:02:30.280
智能体始终考虑所有时间步的奖励

00:02:30.280 --> 00:02:34.430
它将学会选择实现长期稳定性的动作

00:02:34.430 --> 00:02:40.080
这样的话 机器人会稍微移动得慢些 并牺牲一点点的奖励

00:02:40.080 --> 00:02:42.990
但是这样会有回报 因为它可以更长时间地避免摔倒

00:02:42.990 --> 00:02:47.000
并收集更高的累积奖励

00:02:47.000 --> 00:02:49.185
但是 这对智能体在随机时间步

00:02:49.185 --> 00:02:51.914
选择动作来说有什么意义

00:02:51.914 --> 00:02:55.734
它是如何记录所有时间步的

00:02:55.735 --> 00:02:59.315
对于时间步 t

00:02:59.314 --> 00:03:02.099
需要注意的是 所有之前时间步的奖励已经确定了

00:03:02.099 --> 00:03:06.884
因为它们是过去的奖励

00:03:06.884 --> 00:03:11.144
只有未来的奖励受到智能体的控制

00:03:11.145 --> 00:03:15.090
我们将后续时间步的奖励之和

00:03:15.090 --> 00:03:19.604
称之为回报 表示为 G

00:03:19.604 --> 00:03:21.794
在任何随机时间步

00:03:21.794 --> 00:03:27.530
智能体将始终选择一个目标是最大化回报的动作

00:03:27.530 --> 00:03:34.349
更准确的说法是 智能体的目标是最大化预期回报

00:03:34.349 --> 00:03:37.530
因为通常智能体无法完全肯定地预测

00:03:37.530 --> 00:03:42.219
未来奖励会怎样

00:03:42.219 --> 00:03:46.085
它必须依赖于预测或估算

00:03:46.085 --> 00:03:49.375
当我们讨论折扣回报时 将稍微讲解这一概念

00:03:49.375 --> 00:03:51.000
这就是主要概念

