WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.349
我们讨论了强化学习的各种应用

00:00:04.349 --> 00:00:07.195
每个应用都有一个智能体和环境

00:00:07.195 --> 00:00:09.120
每个智能体都有一个目标

00:00:09.119 --> 00:00:14.910
从学习无人驾驶的汽车到学习玩 Atari 游戏的智能体

00:00:14.910 --> 00:00:16.740
很令人震惊的是

00:00:16.739 --> 00:00:21.954
所有这些完全不同的目标可以通过相同的理论框架来实现

00:00:21.954 --> 00:00:24.869
到目前为止 我们通过小狗与主人互动

00:00:24.870 --> 00:00:28.710
了解了奖励这一概念

00:00:28.710 --> 00:00:31.170
在此情形中 任何时间步的状态是

00:00:31.170 --> 00:00:34.380
主人向小狗发出的指令

00:00:34.380 --> 00:00:36.480
动作是小狗的响应

00:00:36.479 --> 00:00:39.309
奖励是骨头奖赏次数

00:00:39.310 --> 00:00:42.405
和良好的强化学习智能体一样

00:00:42.405 --> 00:00:45.105
小狗的目标是最大化这种奖赏

00:00:45.104 --> 00:00:48.464
在这种情形下 奖励这一概念很自然

00:00:48.465 --> 00:00:52.075
与我们训练小狗的思维很一致

00:00:52.075 --> 00:00:56.295
但实际上 强化学习框架是指让所有智能体

00:00:56.295 --> 00:01:02.125
制定最大化期望累积奖励这一目标

00:01:02.125 --> 00:01:07.305
对于学习如何行走的机器人来说 奖励是什么意思呢？

00:01:07.305 --> 00:01:10.470
我们可以将这种环境看做训练员观察机器人的动作

00:01:10.469 --> 00:01:14.444
并在行走姿势正确时奖励机器人

00:01:14.444 --> 00:01:17.549
但是这种奖励可能会非常具有主观性

00:01:17.549 --> 00:01:21.375
根本没有科学定义

00:01:21.375 --> 00:01:22.995
什么样的行走姿势是好的姿势？

00:01:22.995 --> 00:01:24.450
什么样的又是坏的姿势？

00:01:24.450 --> 00:01:26.549
如何处理此问题？

00:01:26.549 --> 00:01:29.909
通常 如何指定奖励

00:01:29.909 --> 00:01:34.109
以便描述智能体可能会具有的任何潜在目标数量？

00:01:34.109 --> 00:01:36.355
在回答此问题之前

00:01:36.355 --> 00:01:38.180
我们先来了解另一个知识

00:01:38.180 --> 00:01:41.520
要注意的是 “强化”和“强化学习”

00:01:41.519 --> 00:01:46.164
这两个术语本身来自行为科学

00:01:46.165 --> 00:01:49.470
是指在行为之后立即发生的刺激

00:01:49.469 --> 00:01:53.605
使该行为在未来更有可能发生

00:01:53.605 --> 00:01:57.040
采用这一名称并非偶然

00:01:57.040 --> 00:02:00.359
实际上 在强化学习中 有一个重要的定义假设

00:02:00.359 --> 00:02:04.560
即智能体的目标始终可以描述为

00:02:04.560 --> 00:02:09.170
最大化期望累积奖励

00:02:09.169 --> 00:02:13.109
我们将这种假设称之为奖励假设

00:02:13.110 --> 00:02:17.055
如果你依然不太理解 不用担心 并不是只有你一个人有这种想法

00:02:17.055 --> 00:02:20.000
我将在下个视频中详细讲解

