WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.720
我们讨论了智能体如何选择动作以便实现最大化预期回报这一目标

00:00:03.720 --> 00:00:08.425
但是我们还需要深入了解这方面的知识

00:00:08.425 --> 00:00:10.920
例如 对于我们的小狗智能体

00:00:10.919 --> 00:00:15.718
它可以如何预测在未来的任何时间点 它能够获得多少奖励

00:00:15.718 --> 00:00:18.245
小狗可以活十几年

00:00:18.245 --> 00:00:22.230
它现在知道可以获得的奖励数

00:00:22.230 --> 00:00:27.074
与五年之后知道可以获得的奖励真的一样吗

00:00:27.074 --> 00:00:30.689
是否它不知道未来会怎样这个理论更合理

00:00:30.690 --> 00:00:34.905
尤其是如果小狗依然在学习

00:00:34.905 --> 00:00:38.865
提出假设 测试假设并更改策略

00:00:38.865 --> 00:00:42.030
它不太可能知道在未来第 1000 个时间步

00:00:42.030 --> 00:00:45.679
它能获得的奖励潜力是多少

00:00:45.679 --> 00:00:49.049
通常 与很远的未来相比

00:00:49.049 --> 00:00:53.399
小狗更有可能知道不久将来的状况

00:00:53.399 --> 00:00:54.839
那么 当前奖励的权重

00:00:54.840 --> 00:00:58.960
应该和未来奖励的权重一样吗？

00:00:58.960 --> 00:01:03.224
或许应该将不久将来的奖励权重设置更高

00:01:03.223 --> 00:01:06.229
因为这些奖励更容易预测

00:01:06.230 --> 00:01:07.650
也就是说 如果我告诉你

00:01:07.650 --> 00:01:10.050
“我肯定现在会给你一个棉花糖

00:01:10.049 --> 00:01:13.625
或者明天可能会给一个”

00:01:13.625 --> 00:01:16.609
难道你不会立即拿走棉花糖吗？

00:01:16.609 --> 00:01:18.879
今天的棉花糖对你的价值有多大

00:01:18.879 --> 00:01:23.280
明天的棉花糖对你来说可能只有一定百分比的价值

00:01:23.280 --> 00:01:27.519
例如价值是 90% 或比今天的低 10%

00:01:27.519 --> 00:01:31.239
毕竟你可能根本拿不到这个棉花糖

00:01:31.239 --> 00:01:35.049
如果我们继续这种模式

00:01:35.049 --> 00:01:37.209
后天的棉花糖可能性更小

00:01:37.209 --> 00:01:40.929
因此价值更低

00:01:40.930 --> 00:01:46.260
这就引出了折扣和折扣回报这个概念

00:01:46.260 --> 00:01:51.140
注意 智能体的目标始终是最大化累积奖励

00:01:51.140 --> 00:01:52.900
在实现此目标时

00:01:52.900 --> 00:01:55.305
在随机时间步 t

00:01:55.305 --> 00:01:58.930
它可以选择最大化回报的动作

00:01:58.930 --> 00:02:01.930
目前 t+1 之后的每个时间步

00:02:01.930 --> 00:02:06.005
对于智能体的决策制定有相同的影响

00:02:06.004 --> 00:02:09.069
如果我们希望之前发生的时间步

00:02:09.069 --> 00:02:12.449
权重更大 该怎么办

00:02:12.449 --> 00:02:15.709
我们不再最大化这个和

00:02:15.710 --> 00:02:19.840
而是最大化另一个不同的和

00:02:19.840 --> 00:02:24.420
时间越远的奖励将乘以更小的值

00:02:24.419 --> 00:02:27.750
我们将这种和称之为折扣回报

00:02:27.750 --> 00:02:31.764
折扣是指我们将更改目标

00:02:31.764 --> 00:02:36.489
更关心近期奖励 而不是遥远未来获得的奖励

00:02:36.490 --> 00:02:39.335
但是如何选择这里的值呢？

00:02:39.335 --> 00:02:43.090
我们将定义一个折扣率

00:02:43.090 --> 00:02:46.080
用希腊字母 γ 表示

00:02:46.080 --> 00:02:49.485
它始终是 0 到 1 之间的值

00:02:49.485 --> 00:02:51.390
对于要乘以的值

00:02:51.389 --> 00:02:54.509
第一项乘以 γ

00:02:54.509 --> 00:02:57.500
第二项乘以 γ2

00:02:57.500 --> 00:03:01.175
然后乘以 γ3 以此类推

00:03:01.175 --> 00:03:03.175
这样就可以很好地表示

00:03:03.175 --> 00:03:09.175
更早发生的奖励始终乘以更大的数字

00:03:09.175 --> 00:03:13.420
要注意的是 该 γ 值并非由智能体学习获得

00:03:13.419 --> 00:03:18.009
而是由你来设置 以便优化智能体的目标

00:03:18.009 --> 00:03:21.789
那么如何设置 γ 的值？

00:03:21.789 --> 00:03:26.164
我们先看看如果将 γ 设为 1 会发生什么

00:03:26.164 --> 00:03:31.764
全部代入 1 结果变成了

00:03:31.764 --> 00:03:36.074
在上个视频中提到的完全无折扣回报

00:03:36.074 --> 00:03:39.209
如果将 γ 设为 0 呢？

00:03:39.210 --> 00:03:41.469
这样的话 该求和公式中的每一项都消失了

00:03:41.469 --> 00:03:46.275
只有最近的奖励还存在

00:03:46.275 --> 00:03:48.925
我们发现 γ 的值越大

00:03:48.925 --> 00:03:51.939
智能体对遥远未来的关注就越高

00:03:51.939 --> 00:03:54.564
当 γ 越来越小时

00:03:54.564 --> 00:03:57.513
折扣率就越高

00:03:57.513 --> 00:03:59.020
在最极端的情形下

00:03:59.020 --> 00:04:03.430
智能体值关注最近的奖励

00:04:03.430 --> 00:04:08.860
值得注意的是 折扣与连续性任务的关系最大

00:04:08.860 --> 00:04:10.990
连续性任务是指智能体与环境的互动

00:04:10.990 --> 00:04:14.185
一直持续下去 不会结束

00:04:14.185 --> 00:04:17.774
在这种情况下 如果智能体想要最大化累积奖励

00:04:17.774 --> 00:04:23.254
但是未来是无限的 这样目标实现起来就很困难

00:04:23.254 --> 00:04:29.300
因此我们使用折扣避免无限未来的奖励产生不良的影响

00:04:29.300 --> 00:04:32.860
但是注意 无论是否有折扣

00:04:32.860 --> 00:04:35.035
目标始终不变

00:04:35.035 --> 00:04:38.820
一直都是最大化累积奖励

00:04:38.819 --> 00:04:43.870
当智能体在随机时间步选择动作时 就要用到折扣

00:04:43.870 --> 00:04:48.600
它在选择动作时 会用到折扣率

00:04:48.600 --> 00:04:53.200
程序更关注于获得更早出现的奖励

00:04:53.199 --> 00:04:58.180
而不是稍后出现并且可能性更低的奖励

00:04:58.180 --> 00:05:04.110
你将在下节课详细了解智能体应该如何选择动作

00:05:04.110 --> 00:05:09.000
暂时我们将侧重于完全指定强化学习问题

