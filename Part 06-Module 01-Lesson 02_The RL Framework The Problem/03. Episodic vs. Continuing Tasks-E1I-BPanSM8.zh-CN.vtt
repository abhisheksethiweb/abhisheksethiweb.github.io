WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.160
在这门课程中

00:00:02.160 --> 00:00:06.200
我们将考虑的很多现实问题都有一个清晰的结束点

00:00:06.200 --> 00:00:09.630
例如 假设我们要教授智能体玩游戏

00:00:09.630 --> 00:00:13.485
互动以智能体获胜或失败告终

00:00:13.484 --> 00:00:17.480
或者以模型的形式指导汽车如何自行驾驶

00:00:17.480 --> 00:00:20.550
如果汽车撞毁了 互动结束

00:00:20.550 --> 00:00:23.880
当然 并非所有强化学习任务都有一个清晰的结束点

00:00:23.879 --> 00:00:28.924
具有清晰结束点的任务称之为阶段性任务

00:00:28.925 --> 00:00:32.420
这里 我们将从头到尾的一系列完整互动

00:00:32.420 --> 00:00:36.109
称之为一个阶段

00:00:36.109 --> 00:00:37.387
当某个阶段结束时

00:00:37.387 --> 00:00:42.689
智能体看看它收到的奖励总量 并判断自己的表现如何

00:00:42.689 --> 00:00:47.149
然后能够从头开始 就好像完全第一次接触同一环境

00:00:47.149 --> 00:00:52.445
但是掌握了前生了解的一些知识

00:00:52.445 --> 00:00:56.509
这样 随着它重生多次之后

00:00:56.509 --> 00:00:59.089
智能体能够做出越来越好的决策

00:00:59.090 --> 00:01:02.730
你将在你的编程实现中自己亲身体会到这一点

00:01:02.729 --> 00:01:06.935
当你的智能体花费足够的时间来了解环境时

00:01:06.935 --> 00:01:12.055
它们应该能够选择一个使累积奖励非常高的策略

00:01:12.055 --> 00:01:15.755
换句话说 对于游戏玩家智能体

00:01:15.754 --> 00:01:19.109
它应该能够获得更高的得分

00:01:19.109 --> 00:01:23.325
因此阶段性任务具有清晰的结束点

00:01:23.325 --> 00:01:27.109
我们还将了解一些一直持续下去的任务

00:01:27.109 --> 00:01:29.659
这些任务称之为连续性任务

00:01:29.659 --> 00:01:33.259
例如 根据金融市场买入和卖出股票的算法

00:01:33.260 --> 00:01:38.850
最适合建模为连续性任务智能体

00:01:38.849 --> 00:01:41.734
在这种情况下 智能体会一直存活下去

00:01:41.734 --> 00:01:44.689
它需要学习选择动作的最佳方式

00:01:44.689 --> 00:01:48.629
同时与环境不断互动

00:01:48.629 --> 00:01:50.959
这种情形的算法更加复杂

00:01:50.959 --> 00:01:54.739
我们将在这门课程的后续阶段介绍

00:01:54.739 --> 00:01:59.000
暂时我们先深入了解奖励这一概念

