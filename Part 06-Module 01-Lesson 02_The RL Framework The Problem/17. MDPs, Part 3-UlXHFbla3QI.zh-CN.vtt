WEBVTT
Kind: captions
Language: zh-CN

00:00:01.270 --> 00:00:03.964
我们已经通过一个示例

00:00:03.964 --> 00:00:07.070
讲解了强化学习框架的正式定义

00:00:07.070 --> 00:00:10.699
现在你应该对该定义有了直观的理解

00:00:10.699 --> 00:00:17.000
马尔可夫决策流程 (MDP) 由一组状态

00:00:17.000 --> 00:00:18.283
一组动作

00:00:18.283 --> 00:00:20.720
和一组奖励

00:00:20.719 --> 00:00:24.704
以及一步环境动态特性和折扣率决定

00:00:24.704 --> 00:00:27.769
我们详细讲解了状态 动作 奖励

00:00:27.769 --> 00:00:29.960
和一步环境动态特性

00:00:29.960 --> 00:00:33.942
但是还需要讲解折扣率

00:00:33.942 --> 00:00:35.299
到目前为止

00:00:35.299 --> 00:00:39.489
需要注意的是 我们详细讲解了连续性任务

00:00:39.490 --> 00:00:45.140
因此有必要将折扣率设为小于 1

00:00:45.140 --> 00:00:49.460
否则 智能体需要查看无限的未来

00:00:49.460 --> 00:00:55.310
通常我们都将折扣率设为 0.9 此处也很适合

00:00:55.310 --> 00:00:58.850
在整个课程中 你将有机会通过实现

00:00:58.850 --> 00:01:03.545
直观地了解如何设置折扣率

00:01:03.545 --> 00:01:06.935
但是现在要记住的是

00:01:06.935 --> 00:01:11.325
折扣率始终设为非常接近 1 而不是 0 的某个值

00:01:11.325 --> 00:01:15.975
否则 智能体会出现看不清故障的情况

00:01:15.974 --> 00:01:20.399
现在你完全指定了你的第一个 MDP

00:01:20.400 --> 00:01:23.375
通常 如果你有一个现实问题需要解决

00:01:23.375 --> 00:01:26.959
则需要指定 MDP

00:01:26.959 --> 00:01:32.149
这样就会完全正式地定义你希望智能体解决的问题

00:01:32.150 --> 00:01:35.705
此框架适合连续性和阶段性任务

00:01:35.704 --> 00:01:40.025
当你遇到希望通过强化学习解决的问题时

00:01:40.025 --> 00:01:43.290
无论是无人驾驶汽车 步行机器人

00:01:43.290 --> 00:01:44.600
还是股票交易智能体

00:01:44.599 --> 00:01:47.390
你都将用到该框架

00:01:47.390 --> 00:01:52.515
智能体将知道状态 动作和折扣因子

00:01:52.515 --> 00:01:55.754
奖励和一步动态特性指定了环境的工作原理

00:01:55.754 --> 00:02:00.339
智能体对其并不了解

00:02:00.340 --> 00:02:02.840
虽然不了解这些信息

00:02:02.840 --> 00:02:08.280
但是智能体依然需要通过互动学习如何实现目标

