WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:05.400
到目前为止 我们已经讨论了如何为蒙特卡洛控制设计算法

00:00:05.400 --> 00:00:08.130
我在上个视频中提到

00:00:08.130 --> 00:00:11.385
我们依然需要处理评估步骤

00:00:11.384 --> 00:00:14.820
为此 我们讨论了一种不断估算

00:00:14.820 --> 00:00:18.675
一系列数字的平均值的算法

00:00:18.675 --> 00:00:23.160
注意 该算法侧重于一个状态动作对

00:00:23.160 --> 00:00:27.079
并经历了多次

00:00:27.079 --> 00:00:33.195
该算法按顺序查看每次经历并成功地更新均值

00:00:33.195 --> 00:00:35.737
这个算法目前只能更新

00:00:35.737 --> 00:00:41.549
对应于单个状态动作对的值函数逼近结果

00:00:41.549 --> 00:00:43.439
下一步是稍微修改该算法

00:00:43.439 --> 00:00:48.384
以便不断估算很多状态动作对的值

00:00:48.384 --> 00:00:51.539
然后将该算法当做新的已完善评估步骤

00:00:51.539 --> 00:00:56.435
代入公式中 流程如下

00:00:56.435 --> 00:01:01.005
智能体先从环境中获得一个阶段样例

00:01:01.005 --> 00:01:06.915
然后对于每个时间步 我们都查看相应的状态动作对

00:01:06.915 --> 00:01:08.190
如果是首次经历

00:01:08.189 --> 00:01:10.724
则计算相应的回报

00:01:10.724 --> 00:01:15.619
然后 根据不断取平均值的算法

00:01:15.620 --> 00:01:19.785
更新动作值的相应估值

00:01:19.784 --> 00:01:25.090
当我们将这部分代入蒙特卡洛控制算法中时

00:01:25.090 --> 00:01:28.320
我们还要初始化每个对的经历次数

00:01:28.319 --> 00:01:32.339
越来越接近最终结果了

00:01:32.340 --> 00:01:34.829
二十一点智能体很快将获得

00:01:34.829 --> 00:01:38.069
在每次玩游戏后完善策略的方法

