WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.120
我们已经拥有了完善的策略评估方法

00:00:03.120 --> 00:00:06.095
现在将重点了解如何执行策略完善步骤

00:00:06.094 --> 00:00:08.355
给定一个动作值函数

00:00:08.355 --> 00:00:11.015
如何使用它来提出策略？

00:00:11.015 --> 00:00:14.935
可以直接复制动态规划设置中的算法吗？

00:00:14.935 --> 00:00:17.190
或许可以用动作值函数

00:00:17.190 --> 00:00:19.825
构建相应的贪婪策略

00:00:19.824 --> 00:00:24.054
换句话说 在每个状态 我们直接选择值最高的动作

00:00:24.054 --> 00:00:26.939
你认为现在该方法有效吗？

00:00:26.940 --> 00:00:28.855
答案是也算有效

00:00:28.855 --> 00:00:30.780
如果你打算将其与

00:00:30.780 --> 00:00:34.498
刚刚学会的策略评估算法结合使用

00:00:34.497 --> 00:00:36.794
则需要稍加修改

00:00:36.795 --> 00:00:39.450
为此 我们来看一个示例

00:00:39.450 --> 00:00:42.815
假设你是一个智能体 你面前有两扇门

00:00:42.814 --> 00:00:45.314
你需要判断哪扇门后面的值更高

00:00:45.314 --> 00:00:49.924
我们来看看如何使用蒙特卡洛方法解决这一问题

00:00:49.924 --> 00:00:53.969
一开始 你没有任何理由偏爱某扇门

00:00:53.969 --> 00:00:58.359
因此假设将每扇门的估算值初始化为 0

00:00:58.359 --> 00:01:00.884
为了判断应该打开哪扇门

00:01:00.884 --> 00:01:02.114
假设你抛掷了一枚硬币

00:01:02.115 --> 00:01:04.500
背面朝上 因此你打开门 B

00:01:04.500 --> 00:01:07.459
这么做时 收到奖励 0

00:01:07.459 --> 00:01:09.455
为简单起见

00:01:09.456 --> 00:01:12.435
当打开一扇门时 一个阶段结束

00:01:12.435 --> 00:01:13.875
换句话说

00:01:13.875 --> 00:01:15.420
打开门 B 后

00:01:15.420 --> 00:01:17.745
你收到回报 0

00:01:17.745 --> 00:01:20.160
这不会更改值函数的估值

00:01:20.159 --> 00:01:24.429
因此可以再次随机选择一扇门

00:01:24.430 --> 00:01:25.750
你抛掷了一枚硬币

00:01:25.750 --> 00:01:27.105
这次正面朝上

00:01:27.105 --> 00:01:28.576
打开门 A

00:01:28.576 --> 00:01:29.810
这么做时

00:01:29.810 --> 00:01:31.799
获得奖励 1

00:01:31.799 --> 00:01:36.250
这样就会将门 A 的估值更新为 1

00:01:36.250 --> 00:01:39.915
现在如果对值函数进行贪婪的操作

00:01:39.915 --> 00:01:41.925
再次打开门 A

00:01:41.924 --> 00:01:44.799
假设这次获得奖励 3

00:01:44.799 --> 00:01:47.709
使门 A 的值更新为 2

00:01:47.709 --> 00:01:49.589
在下个时间点

00:01:49.590 --> 00:01:52.409
贪婪策略表示再次选择门 A

00:01:52.409 --> 00:01:55.019
每次这么做时

00:01:55.019 --> 00:01:59.265
我们都获得正面奖励 始终是 1 或 3

00:01:59.265 --> 00:02:02.555
因此我们一直都打开相同的门

00:02:02.555 --> 00:02:04.088
这样就存在一个很大的问题

00:02:04.087 --> 00:02:08.530
因为我们始终没有机会看看第二扇门后面的值

00:02:08.530 --> 00:02:13.770
例如 假设门 A 后面的机制是你期望的结果

00:02:13.770 --> 00:02:16.515
它产生奖励 1 或 3

00:02:16.514 --> 00:02:23.484
二者的概率一样 但是门 B 后面的机制产生奖励 0 或 100

00:02:23.485 --> 00:02:26.190
这是你希望发现的信息

00:02:26.189 --> 00:02:29.849
但是贪婪策略却阻止你这么做

00:02:29.849 --> 00:02:33.030
问题是 当我们很早就遇到

00:02:33.030 --> 00:02:37.800
门 A 似乎比门 B 更有利的情形时

00:02:37.800 --> 00:02:40.945
我们非常有必要花费更多的时间确认这一点

00:02:40.944 --> 00:02:43.814
因为一开始的猜测不正确

00:02:43.814 --> 00:02:46.960
更好的策略是采用随机性策略

00:02:46.960 --> 00:02:51.210
而不是构建刚才的贪婪策略

00:02:51.210 --> 00:02:55.890
例如选择门 A 的概率是 95% 选择门 B 的概率是 5%

00:02:55.889 --> 00:02:58.804
依然很接近贪婪策略

00:02:58.805 --> 00:03:00.990
我们的方法依然很适宜

00:03:00.990 --> 00:03:03.300
但是如果我们继续以很小的概率选择门 B

00:03:03.300 --> 00:03:06.390
就会获得额外的价值

00:03:06.389 --> 00:03:10.349
在某个时刻 我们将看到回报 100

00:03:10.349 --> 00:03:15.693
这个示例解释了如何定义蒙特卡洛策略完善算法

00:03:15.693 --> 00:03:18.915
我们不再始终构建贪婪策略

00:03:18.915 --> 00:03:20.900
而是构建一个随机策略

00:03:20.900 --> 00:03:25.075
以非常高的概率选择贪婪动作

00:03:25.074 --> 00:03:30.774
但是以很小的非零概率选择某个非贪婪动作

00:03:30.775 --> 00:03:35.955
在这种情况下 你将设置一个很小的正数 ϵ

00:03:35.955 --> 00:03:39.504
设置得越大 你就越有可能选择非贪婪动作

00:03:39.503 --> 00:03:42.679
称之为 epsilon 贪婪算法

00:03:42.680 --> 00:03:46.155
ϵ 必须是 0 到 1 之间的数字

00:03:46.155 --> 00:03:49.155
智能体以 1-ϵ 的概率

00:03:49.155 --> 00:03:51.719
选择贪婪动作

00:03:51.719 --> 00:03:53.935
并以概率 ϵ

00:03:53.935 --> 00:03:56.145
随机选择任何动作

00:03:56.145 --> 00:03:59.460
只要将 ϵ 设为一个很小的数字

00:03:59.460 --> 00:04:04.920
我就能够构建一个非常接近贪婪策略的策略

00:04:04.919 --> 00:04:07.139
并且不会阻止智能体

00:04:07.139 --> 00:04:11.099
继续探索其他可能性

00:04:11.099 --> 00:04:16.139
总之 我们的蒙特卡洛控制算法将如下所示

00:04:16.139 --> 00:04:18.060
我代入了优化步骤将获得

00:04:18.060 --> 00:04:22.439
Epsilon 贪婪策略这个结果

