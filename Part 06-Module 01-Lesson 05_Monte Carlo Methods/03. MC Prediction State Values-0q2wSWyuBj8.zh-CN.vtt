WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.745
我们来看看我们面临的问题

00:00:02.745 --> 00:00:05.259
我们有一个智能体和一个环境

00:00:05.259 --> 00:00:08.009
时间被拆分为离散时间步

00:00:08.009 --> 00:00:09.285
在每个时间步

00:00:09.285 --> 00:00:12.269
智能体都从环境中获得一个奖励和状态

00:00:12.269 --> 00:00:16.835
并选择一个响应动作

00:00:16.835 --> 00:00:22.605
这样的话 互动变成一系列的状态 动作和奖励

00:00:22.605 --> 00:00:27.300
在这节课 我们将重点讨论阶段性任务

00:00:27.300 --> 00:00:32.535
当智能体在时间 T 遇到最终状态时 互动结束

00:00:32.534 --> 00:00:35.324
我们将这一过程称为一个阶段

00:00:35.325 --> 00:00:38.445
对于任何阶段 智能体的目标都是发现最优策略

00:00:38.445 --> 00:00:44.219
以便最大化预期累积奖励

00:00:44.219 --> 00:00:48.450
为此 我们将从预测问题开始

00:00:48.450 --> 00:00:54.150
给定策略后 智能体如何估算该策略的值函数？

00:00:54.149 --> 00:00:58.452
注意 智能体不了解该环境的动态特性

00:00:58.453 --> 00:01:03.638
因此它需要通过与环境互动估算值函数

00:01:03.637 --> 00:01:05.828
为了与环境互动

00:01:05.828 --> 00:01:09.250
智能体需要遵守一个策略

00:01:09.250 --> 00:01:12.329
我们可以用一个策略来进行评估

00:01:12.329 --> 00:01:16.875
并用另一个策略来与环境互动

00:01:16.875 --> 00:01:21.352
称之为针对预测问题的离线策略方法

00:01:21.352 --> 00:01:26.129
离线策略方法具有一定的复杂性 稍后我们将详细讲解

00:01:26.129 --> 00:01:29.564
我们先讲解异同策略方法

00:01:29.564 --> 00:01:33.375
即智能体通过遵循某个策略与环境互动

00:01:33.375 --> 00:01:36.531
并计算该策略的值函数

00:01:36.531 --> 00:01:39.025
在详细讲解该算法之前

00:01:39.025 --> 00:01:41.484
我们来看一个示例

00:01:41.484 --> 00:01:46.004
假设我们要处理一个阶段性任务 该 MDP 具有三个状态

00:01:46.004 --> 00:01:50.530
X Y 和 Z 其中 Z 是最终状态

00:01:50.530 --> 00:01:54.254
为了更直观地理解该 MDP

00:01:54.254 --> 00:01:56.234
你可以将问题看做一个

00:01:56.234 --> 00:01:58.314
非常非常小的冰雪环境

00:01:58.314 --> 00:02:04.679
假设状态 X 和 Y 对应于一座白雪皑皑的森林中的不同地点

00:02:04.680 --> 00:02:11.465
森林中有令人愉悦和害怕的景象 对应于正面或负面奖励

00:02:11.465 --> 00:02:17.159
状态 Z 是最终状态 对应于森林中的一个小屋

00:02:17.159 --> 00:02:19.365
如果智能体抵达该状态

00:02:19.365 --> 00:02:22.159
则进入小屋并结束这一阶段

00:02:22.159 --> 00:02:24.525
假设有两个潜在动作

00:02:24.525 --> 00:02:28.913
向上和向下 和冰冻湖泊环境相似

00:02:28.913 --> 00:02:30.855
该环境非常光滑

00:02:30.854 --> 00:02:33.659
如果智能体选择向下动作

00:02:33.659 --> 00:02:35.384
在下个时间步

00:02:35.384 --> 00:02:41.084
有一定的概率表明它会向上移动或保持不动

00:02:41.085 --> 00:02:46.680
同样 如果它决定向上移动 当它尝试朝着该方向移动时

00:02:46.680 --> 00:02:51.000
可能会向下移动或保持不动

00:02:51.000 --> 00:02:55.530
假设我们要评估以下策略

00:02:55.530 --> 00:03:00.328
智能体在状态 X 向上移动 在状态 Y 向下移动

00:03:00.328 --> 00:03:02.175
智能体可以通过遵循该策略

00:03:02.175 --> 00:03:05.535
与环境互动

00:03:05.534 --> 00:03:07.935
假设在一开始互动时

00:03:07.935 --> 00:03:10.545
智能体观察状态 X

00:03:10.544 --> 00:03:12.524
然后根据策略

00:03:12.525 --> 00:03:15.390
选择向上动作

00:03:15.389 --> 00:03:19.500
结果获得奖励 -2 并进入下个状态 Y

00:03:19.500 --> 00:03:21.900
接着根据该策略

00:03:21.900 --> 00:03:23.520
选择向下动作

00:03:23.520 --> 00:03:24.774
结果

00:03:24.774 --> 00:03:28.689
获得奖励 0 并进入下个状态 Y

00:03:28.689 --> 00:03:32.699
此刻 根据该策略选择向下动作

00:03:32.699 --> 00:03:36.889
结果获得奖励 3 并抵达最终状态 Z

00:03:36.889 --> 00:03:39.059
互动结束

00:03:39.060 --> 00:03:44.090
假设智能体在另外两个阶段与环境互动

00:03:44.090 --> 00:03:48.485
我们可以使用这些阶段估算值函数

00:03:48.485 --> 00:03:51.420
当然 这三个简短的互动阶段

00:03:51.419 --> 00:03:56.024
并不足以让智能体充分了解环境

00:03:56.025 --> 00:03:58.890
暂时先假设已足够

00:03:58.889 --> 00:04:03.309
我们将从一个状态开始 例如状态 X

00:04:03.310 --> 00:04:08.599
然后查看在所有阶段的状态 X 的所有状况

00:04:08.599 --> 00:04:13.269
接着计算在出现该状态之后的折扣回报

00:04:13.270 --> 00:04:17.699
假设折扣率是 1 即不折扣

00:04:17.699 --> 00:04:20.235
对于第一个阶段

00:04:20.235 --> 00:04:22.206
回报是 -2

00:04:22.206 --> 00:04:24.790
加上 0 加上 3 结果为 1

00:04:24.790 --> 00:04:26.955
在第三个阶段

00:04:26.954 --> 00:04:31.349
回报是 -3 加上 3 结果为 0

00:04:31.350 --> 00:04:34.770
蒙特卡罗预测算法会对这些值取平均值

00:04:34.769 --> 00:04:40.544
并代入状态 X 的值估算方程中

00:04:40.545 --> 00:04:45.824
这里 状态 X 的估值为 1/2

00:04:45.824 --> 00:04:48.629
该算法比较直观

00:04:48.629 --> 00:04:51.709
状态的值定义为

00:04:51.709 --> 00:04:54.959
该状态之后的预期回报

00:04:54.959 --> 00:05:01.114
因此智能体体验的平均回报是个很好的估值

00:05:01.115 --> 00:05:04.930
我们将按照相同的流程评估状态 Y

00:05:04.930 --> 00:05:08.025
你将发现状态 Y 在每个阶段出现了多次

00:05:08.024 --> 00:05:12.529
这种情况不太清楚该如何处理

00:05:12.529 --> 00:05:16.274
为了解决这一问题 我们需要介绍几个其他术语

00:05:16.274 --> 00:05:22.185
我们将状态在某个阶段中的每次访问定义为该状态的访问

00:05:22.185 --> 00:05:24.720
如果状态在某个阶段中访问了多次

00:05:24.720 --> 00:05:28.245
我们有两种处理方式

00:05:28.245 --> 00:05:30.030
第一种方式是

00:05:30.029 --> 00:05:33.344
在每个阶段 我们只考虑该状态的首次访问

00:05:33.345 --> 00:05:38.070
并对这些回报取平均值

00:05:38.069 --> 00:05:42.995
在这种情况下 状态 Y 的值估算为

00:05:42.995 --> 00:05:47.069
3 加 3 再加 1 的平均值 即 3/7

00:05:47.069 --> 00:05:48.490
如果采用这种方式

00:05:48.490 --> 00:05:52.875
则表示我们使用的是首次经历 MC 方法

00:05:52.875 --> 00:05:55.524
另一种方式是对所有阶段中

00:05:55.524 --> 00:06:00.154
状态 Y 的所有访问之后的回报取平均值

00:06:00.154 --> 00:06:03.299
在这种情况下 状态 Y 的值估算为

00:06:03.300 --> 00:06:06.884
所有这些数字的平均值

00:06:06.884 --> 00:06:11.295
结果为 14/7 即 2

00:06:11.295 --> 00:06:16.035
很快你将有机会自己实现该算法

00:06:16.035 --> 00:06:20.939
你可以选择首次经历或所有经历 MC 预测方法

00:06:20.939 --> 00:06:23.060
也可以同时实现这两种方式

