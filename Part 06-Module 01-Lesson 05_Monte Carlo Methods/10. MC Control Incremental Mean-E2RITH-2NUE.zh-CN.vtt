WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.850
蒙特卡洛控制算法

00:00:02.850 --> 00:00:06.325
收到广义策略迭代的启发

00:00:06.325 --> 00:00:08.984
我们从策略评估步骤开始

00:00:08.984 --> 00:00:11.939
我们已经知道如何实现这一步

00:00:11.939 --> 00:00:15.859
并且你已经在迷你项目的第二部分实现这一步

00:00:15.859 --> 00:00:20.160
在你的实现中 智能体需要猜测大概 5000 次

00:00:20.160 --> 00:00:24.350
以便很好地估算值函数

00:00:24.350 --> 00:00:27.045
对于策略迭代

00:00:27.045 --> 00:00:32.175
这样的话 在完善策略之前 似乎花费了太长的时间来评估策略

00:00:32.174 --> 00:00:34.619
或许在每次猜测之后

00:00:34.619 --> 00:00:38.274
都完善策略更合理

00:00:38.274 --> 00:00:41.504
在这种情况下 我们可以将每个状态动作对的

00:00:41.505 --> 00:00:46.300
值初始猜测设为 0 并获得起始策略

00:00:46.299 --> 00:00:49.814
然后使用该策略生成一个阶段

00:00:49.814 --> 00:00:51.409
当该阶段结束时

00:00:51.409 --> 00:00:53.915
我们可以更新值函数

00:00:53.915 --> 00:00:57.894
并使用该值函数完善策略

00:00:57.895 --> 00:01:03.190
然后使用该新策略生成下个阶段 等等

00:01:03.189 --> 00:01:07.269
为了有效地完成这一过程 我们需要更改策略评估算法

00:01:07.269 --> 00:01:11.449
我们将首先深入了解当前算法

00:01:11.450 --> 00:01:14.945
我们先执行多个阶段

00:01:14.944 --> 00:01:18.149
然后查看一些状态动作对

00:01:18.150 --> 00:01:24.100
我们计算每种情况之后的回报 然后取平均值

00:01:24.099 --> 00:01:25.544
作为更一般的情况

00:01:25.545 --> 00:01:30.554
假设该状态动作对经历了一定的次数

00:01:30.554 --> 00:01:33.763
将相应的回报表示为 x1 x2

00:01:33.763 --> 00:01:35.745
一直到 xn

00:01:35.745 --> 00:01:40.852
然后通过对这些值取平均值 计算值函数逼近结果

00:01:40.852 --> 00:01:43.745
表示为 μn

00:01:43.745 --> 00:01:47.204
n 表示经历该状态动作对 n 次

00:01:47.204 --> 00:01:52.759
我们不再在所有阶段结束之后计算该平均值

00:01:52.760 --> 00:01:58.109
而是在每次经历之后都迭代更新估值

00:01:58.109 --> 00:02:00.359
首次经历时

00:02:00.359 --> 00:02:03.340
无论回报是多少 都是我们的估值

00:02:03.340 --> 00:02:05.299
第二次经历时

00:02:05.299 --> 00:02:09.717
我们将估值更新为 x1 和 x2 的平均值

00:02:09.717 --> 00:02:12.289
在经过随机 K 次经历后

00:02:12.289 --> 00:02:17.727
我们对 x1 到 xk 的所有值取平均值

00:02:17.728 --> 00:02:19.025
最终的估值

00:02:19.025 --> 00:02:22.580
将相当于等到所有阶段结束后

00:02:22.580 --> 00:02:26.380
获得的结果

00:02:26.379 --> 00:02:28.969
我们来看看能否以更加高效的计算方式

00:02:28.969 --> 00:02:32.069
完成这一流程

00:02:32.069 --> 00:02:35.919
为此 我们需要进行一些数学变换

00:02:35.919 --> 00:02:40.334
首先需要回顾如何计算回报的估值

00:02:40.335 --> 00:02:42.409
具体而言 第 K 项估值是

00:02:42.409 --> 00:02:46.585
前 K 次经历之后的回报的平均值

00:02:46.585 --> 00:02:50.930
然后 我们可以将前 K 次回报的和

00:02:50.930 --> 00:02:56.254
重写为前 K-1 次回报的和加上第 K 次回报

00:02:56.254 --> 00:03:01.960
接着 我们将前 K-1 的和这一项

00:03:01.960 --> 00:03:05.754
重写为 K 减去第一个均值并乘以 K-1

00:03:05.754 --> 00:03:10.310
只需简单变换下就能获得最后一行

00:03:10.310 --> 00:03:14.090
这个等式表示第 K 项均值

00:03:14.090 --> 00:03:18.159
对 K 减去第一个均值和第 K 项回报

00:03:18.159 --> 00:03:20.689
我们将使用该公式设计一个算法

00:03:20.689 --> 00:03:24.719
用于不断求回报的平均值

00:03:24.719 --> 00:03:29.009
先将均值初始化为 0

00:03:29.009 --> 00:03:31.310
还需要记录平均值中已经包含的

00:03:31.310 --> 00:03:34.710
回报的数量

00:03:34.710 --> 00:03:35.990
一开始

00:03:35.990 --> 00:03:38.405
我们将该值初始化为 0

00:03:38.405 --> 00:03:42.740
然后进入 while 循环

00:03:42.740 --> 00:03:48.170
每次使 K 加一 然后使用最近的回报 xk 更新均值

00:03:48.169 --> 00:03:52.039
就这样 虽然暂时不能完全清楚

00:03:52.039 --> 00:03:56.245
如何在蒙特卡洛控制中使用该算法

00:03:56.246 --> 00:03:59.000
但是 很快该算法就会变得非常实用

