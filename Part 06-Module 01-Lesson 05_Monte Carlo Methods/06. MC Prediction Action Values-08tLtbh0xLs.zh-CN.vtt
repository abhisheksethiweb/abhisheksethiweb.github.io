WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.730
希望你成功实现了迷你项目的第一部分

00:00:02.730 --> 00:00:08.109
即使用蒙特卡洛算法解决预测问题

00:00:08.109 --> 00:00:10.244
在动态规划设置中

00:00:10.244 --> 00:00:15.484
下一步是使用状态值函数获得动作值函数

00:00:15.484 --> 00:00:21.175
我们使用该方程将状态值转换成了动作值

00:00:21.175 --> 00:00:24.789
你认为在这里也可以采用相同的流程吗？

00:00:24.789 --> 00:00:27.594
遗憾的是 不能

00:00:27.594 --> 00:00:29.599
该方程用下划线标注的部分

00:00:29.600 --> 00:00:33.755
表示环境的一步动态特性

00:00:33.755 --> 00:00:37.790
智能体在动态规划设定下知道这些信息

00:00:37.789 --> 00:00:42.094
但是在强化学习设置下并不知道这些动态特性

00:00:42.094 --> 00:00:45.350
因此我们不能像之前一样直接应用该方程

00:00:45.350 --> 00:00:47.550
为了获得动作值

00:00:47.549 --> 00:00:51.519
我们需要对预测算法进行小小的改动

00:00:51.520 --> 00:00:55.950
为了进行阐述 我们再看看上个视频中的简单示例

00:00:55.950 --> 00:00:59.400
和之前一样 假设智能体在每个时间步

00:00:59.399 --> 00:01:03.350
都根据策略选择与环境互动的动作

00:01:03.350 --> 00:01:07.980
这些是互动后产生的三个阶段

00:01:07.980 --> 00:01:11.905
但是现在 为了获得动作值

00:01:11.905 --> 00:01:15.445
我们不再查看每个状态的经历

00:01:15.444 --> 00:01:21.409
而是查看每个潜在状态动作对的经历

00:01:21.409 --> 00:01:28.024
然后计算每个状态动作对之后的回报 并像之前一样取平均值

00:01:28.025 --> 00:01:30.670
在这种情况下 状态 X 和向上动作的

00:01:30.670 --> 00:01:33.957
动作估值为 1/2

00:01:33.956 --> 00:01:39.294
然后按照同一流程评估状态 Y 和向下动作

00:01:39.295 --> 00:01:43.344
你将发现该状态动作对在每个阶段出现了多次

00:01:43.344 --> 00:01:47.620
为了解决这一问题 我们再次需要定义一些术语

00:01:47.620 --> 00:01:49.520
我们将某个阶段出现的每次状态动作对

00:01:49.519 --> 00:01:52.682
定义为该状态动作对的经历

00:01:52.682 --> 00:01:55.689
如果某个状态动作对在一个阶段中经历了多次

00:01:55.689 --> 00:01:59.109
在计算动作估值时

00:01:59.109 --> 00:02:03.250
我们可以选择只考虑首次经历或考虑所有经历

00:02:03.250 --> 00:02:07.780
你将发现两种方式得出了不同的值

00:02:07.780 --> 00:02:10.659
首次经历方法的结果是 3/7

00:02:10.659 --> 00:02:14.109
所有经历 MC 方法的结果是 2

00:02:14.110 --> 00:02:17.530
但是当智能体通过多个阶段获得更多经验后

00:02:17.530 --> 00:02:21.384
这些值将收敛为同一数字

00:02:21.384 --> 00:02:26.375
暂时先假设我们实现了首次经历 MC 方法

00:02:26.375 --> 00:02:29.155
几乎就是这么简单

00:02:29.155 --> 00:02:32.430
但是还剩下一个小小的问题要解决

00:02:32.430 --> 00:02:35.765
为此 我们来看看我们要评估的策略

00:02:35.764 --> 00:02:39.819
它是一个确定性策略 智能体在状态 X 始终选择向上动作

00:02:39.819 --> 00:02:44.379
在状态 Y 始终选择向下动作

00:02:44.379 --> 00:02:48.549
尤其是 我们将始终无法估算状态 X 和向下动作

00:02:48.550 --> 00:02:53.110
或状态 Y 和向上动作对应的动作值

00:02:53.110 --> 00:02:57.370
这是因为根据该策略 智能体在状态 X 将始终不会选择向下动作

00:02:57.370 --> 00:03:01.990
并且在状态 Y 将始终不会选择向上动作

00:03:01.990 --> 00:03:04.330
我们的算法只能估算

00:03:04.330 --> 00:03:08.905
实际经历的状态动作对对应的动作值

00:03:08.905 --> 00:03:12.590
因此无论智能体与环境互动多久

00:03:12.590 --> 00:03:16.265
动作值函数估计结果将始终不完整

00:03:16.264 --> 00:03:20.349
幸运的是 该问题有一个简单的解决方案

00:03:20.349 --> 00:03:24.799
即确保不评估确定性策略的动作值函数

00:03:24.800 --> 00:03:28.990
我们将使用随机性策略

00:03:28.990 --> 00:03:32.885
在每个状态 都有一定的非零概率经历每个动作

00:03:32.884 --> 00:03:36.033
例如 如果智能体遇到状态 X

00:03:36.033 --> 00:03:40.780
假设选择向上动作的概率是 90% 否则选择向下动作

00:03:40.780 --> 00:03:44.159
如果遇到状态 Y

00:03:44.159 --> 00:03:49.520
选择向下动作的概率是 80% 否则选择向上动作

00:03:49.520 --> 00:03:51.205
对于这种策略

00:03:51.205 --> 00:03:56.550
每个状态动作对最终都会被智能体经历

00:03:56.550 --> 00:03:59.020
此外 如果阶段数量达到无穷次

00:03:59.020 --> 00:04:03.290
每个状态动作对的经历次数也会达到无穷

00:04:03.289 --> 00:04:05.620
这样就保证我们能够

00:04:05.620 --> 00:04:08.455
为每个状态动作对

00:04:08.455 --> 00:04:10.630
计算一个完美的动作值函数

00:04:10.629 --> 00:04:14.384
只要智能体与环境互动的阶段次数足够即可

00:04:14.384 --> 00:04:17.000
你很快将有机会测试这一流程

