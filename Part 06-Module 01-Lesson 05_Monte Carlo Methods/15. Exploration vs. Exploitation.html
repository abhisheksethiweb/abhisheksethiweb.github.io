<!-- udacimak v1.2.0 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Exploration vs. Exploitation</title>
  <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../assets/css/plyr.css">
  <link rel="stylesheet" href="../assets/css/katex.min.css">
  <link rel="stylesheet" href="../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../assets/img/udacimak.png" />
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>Monte Carlo Methods</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="01. Introduction.html">01. Introduction</a>
    </li>
    <li class="">
      <a href="02. OpenAI Gym BlackjackEnv.html">02. OpenAI Gym: BlackjackEnv</a>
    </li>
    <li class="">
      <a href="03. MC Prediction State Values.html">03. MC Prediction: State Values</a>
    </li>
    <li class="">
      <a href="04. Implementation.html">04. Implementation</a>
    </li>
    <li class="">
      <a href="05. Mini Project MC (Parts 0 and 1).html">05. Mini Project: MC (Parts 0 and 1)</a>
    </li>
    <li class="">
      <a href="06. MC Prediction Action Values.html">06. MC Prediction: Action Values</a>
    </li>
    <li class="">
      <a href="07. Implementation.html">07. Implementation</a>
    </li>
    <li class="">
      <a href="08. Mini Project MC (Part 2).html">08. Mini Project: MC (Part 2)</a>
    </li>
    <li class="">
      <a href="09. Generalized Policy Iteration.html">09. Generalized Policy Iteration</a>
    </li>
    <li class="">
      <a href="10. MC Control Incremental Mean.html">10. MC Control: Incremental Mean</a>
    </li>
    <li class="">
      <a href="11. Quiz Incremental Mean.html">11. Quiz: Incremental Mean</a>
    </li>
    <li class="">
      <a href="12. MC Control Policy Evaluation.html">12. MC Control: Policy Evaluation</a>
    </li>
    <li class="">
      <a href="13. MC Control Policy Improvement.html">13. MC Control: Policy Improvement</a>
    </li>
    <li class="">
      <a href="14. Quiz Epsilon-Greedy Policies.html">14. Quiz: Epsilon-Greedy Policies</a>
    </li>
    <li class="">
      <a href="15. Exploration vs. Exploitation.html">15. Exploration vs. Exploitation</a>
    </li>
    <li class="">
      <a href="16. Implementation.html">16. Implementation</a>
    </li>
    <li class="">
      <a href="17. Mini Project MC (Part 3).html">17. Mini Project: MC (Part 3)</a>
    </li>
    <li class="">
      <a href="18. MC Control Constant-alpha, Part 1.html">18. MC Control: Constant-alpha, Part 1</a>
    </li>
    <li class="">
      <a href="19. MC Control Constant-alpha, Part 2.html">19. MC Control: Constant-alpha, Part 2</a>
    </li>
    <li class="">
      <a href="20. Implementation.html">20. Implementation</a>
    </li>
    <li class="">
      <a href="21. Mini Project MC (Part 4).html">21. Mini Project: MC (Part 4)</a>
    </li>
    <li class="">
      <a href="22. Summary.html">22. Summary</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">15. Exploration vs. Exploitation</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <h1 id="exploration-vs-exploitation">Exploration vs. Exploitation</h1>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/exploration-vs.-exploitation.png" alt="Exploration-Exploitation Dilemma ([Source]( http://slides.com/ericmoura/deck-2/embed))" class="img img-fluid">
    <figcaption class="figure-caption">
      <p>Exploration-Exploitation Dilemma (<a href="http://slides.com/ericmoura/deck-2/embed" target="_blank">Source</a>)</p>
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-solving-environments-in-openai-gym">## Solving Environments in OpenAI Gym</h2>
<p>In many cases, we would like our reinforcement learning (RL) agents to learn to maximize reward as quickly as possible.  This can be seen in many OpenAI Gym environments.  </p>
<p>For instance, the <a href="https://gym.openai.com/envs/FrozenLake-v0/" target="_blank">FrozenLake-v0</a> environment is considered solved once the agent attains an average reward of 0.78 over 100 consecutive trials.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2017-10-04-at-4.58.58-pm.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>Algorithmic solutions to the <a href="https://gym.openai.com/envs/FrozenLake-v0/" target="_blank">FrozenLake-v0</a> environment are ranked according to the number of episodes needed to find the solution.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2017-10-04-at-5.01.26-pm.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>Solutions to <a href="https://gym.openai.com/envs/Taxi-v1/" target="_blank">Taxi-v1</a>, <a href="https://gym.openai.com/envs/CartPole-v1/" target="_blank">Cartpole-v1</a>, and <a href="https://gym.openai.com/envs/MountainCar-v0/" target="_blank">MountainCar-v0</a> (along with many others) are also ranked according to the number of episodes before the solution is found.  Towards this objective, it makes sense to design an algorithm that learns the optimal policy <span class="mathquill ud-math">\pi_*</span> as quickly as possible.  </p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-exploration-exploitation-dilemma">## Exploration-Exploitation Dilemma</h2>
<p>Recall that the environment's dynamics are initially unknown to the agent.  Towards maximizing return, the agent must learn about the environment through interaction.</p>
<p>At every time step, when the agent selects an action, it bases its decision on past experience with the environment.  And, towards minimizing the number of episodes needed to solve environments in OpenAI Gym, our first instinct could be to devise a strategy where the agent always selects the action that it believes (<em>based on its past experience</em>) will maximize return.  With this in mind, the agent could follow the policy that is greedy with respect to the action-value function estimate.  We examined this approach in a previous video and saw that it can easily lead to convergence to a sub-optimal policy.</p>
<p>To see why this is the case, note that in early episodes, the agent's knowledge is quite limited (and potentially flawed).  So, it is highly likely that actions <em>estimated</em> to be non-greedy by the agent are in fact better than the <em>estimated</em> greedy action.</p>
<p>With this in mind, a successful RL agent cannot act greedily at every time step (<em>that is</em>, it cannot always <strong>exploit</strong> its knowledge); instead, in order to discover the optimal policy, it has to continue to refine the estimated return for all state-action pairs (<em>in other words</em>, it has to continue to <strong>explore</strong> the range of possibilities by visiting every state-action pair).  That said, the agent should always act <em>somewhat greedily</em>, towards its goal of maximizing return <em>as quickly as possible</em>.  This motivated the idea of an <span class="mathquill ud-math">\epsilon</span>-greedy policy.</p>
<p>We refer to the need to balance these two competing requirements as the <strong>Exploration-Exploitation Dilemma</strong>.  One potential solution to this dilemma is implemented by gradually modifying the value of <span class="mathquill ud-math">\epsilon</span> when constructing <span class="mathquill ud-math">\epsilon</span>-greedy policies.</p>
<h2 id="-setting-the-value-of-span-classmathquill-ud-mathepsilonspan-in-theory">## Setting the Value of <span class="mathquill ud-math">\epsilon</span>, in Theory</h2>
<p>It makes sense for the agent to begin its interaction with the environment by favoring <strong>exploration</strong> over <strong>exploitation</strong>.  After all, when the agent knows relatively little about the environment's dynamics, it should distrust its limited knowledge and <strong>explore</strong>, or try out various strategies for maximizing return.  With this in mind, the best starting policy is the equiprobable random policy, as it is equally likely to explore all possible actions from each state.  You discovered in the previous quiz that setting <span class="mathquill ud-math">\epsilon = 1</span> yields an <span class="mathquill ud-math">\epsilon</span>-greedy policy that is equivalent to the equiprobable random policy.</p>
<p>At later time steps, it makes sense to favor <strong>exploitation</strong> over <strong>exploration</strong>, where the policy gradually becomes more greedy with respect to the action-value function estimate.  After all, the more the agent interacts with the environment, the more it can trust its estimated action-value function.  You discovered in the previous quiz that setting <span class="mathquill ud-math">\epsilon = 0</span> yields the greedy policy (or, the policy that most favors exploitation over exploration). </p>
<p>Thankfully, this strategy (of initially favoring exploration over exploitation, and then gradually preferring exploitation over exploration) can be demonstrated to be optimal. </p>
<h2 id="-greedy-in-the-limit-with-infinite-exploration-glie">## Greedy in the Limit with Infinite Exploration (GLIE)</h2>
<p>In order to guarantee that MC control converges to the optimal policy <span class="mathquill ud-math">\pi_*</span>, we need to ensure that two conditions are met.  We refer to these conditions as <strong>Greedy in the Limit with Infinite Exploration (GLIE)</strong>.  In particular, if:</p>
<ul>
<li>every state-action pair <span class="mathquill ud-math">s, a</span> (for all <span class="mathquill ud-math">s\in\mathcal{S}</span> and <span class="mathquill ud-math">a\in\mathcal{A}(s)</span>) is visited infinitely many times, and </li>
<li>the policy converges to a policy that is greedy with respect to the action-value function estimate <span class="mathquill ud-math">Q</span>,</li>
</ul>
<p>then MC control is guaranteed to converge to the optimal policy (in the limit as the algorithm is run for infinitely many episodes).  These conditions ensure that:</p>
<ul>
<li>the agent continues to explore for all time steps, and</li>
<li>the agent gradually exploits more (and explores less).</li>
</ul>
<p>One way to satisfy these conditions  is to modify the value of <span class="mathquill ud-math">\epsilon</span> when specifying an <span class="mathquill ud-math">\epsilon</span>-greedy policy.  In particular, let <span class="mathquill ud-math">\epsilon_i</span> correspond to the <span class="mathquill ud-math">i</span>-th time step.  Then, both of these conditions are met if:</p>
<ul>
<li><span class="mathquill ud-math">\epsilon_i > 0</span> for all time steps <span class="mathquill ud-math">i</span>, and </li>
<li><span class="mathquill ud-math">\epsilon_i</span> decays to zero in the limit as the time step <span class="mathquill ud-math">i</span> approaches infinity (that is, <span class="mathquill ud-math">\lim_{i\to\infty} \epsilon_i = 0</span>).</li>
</ul>
<p>For example, to ensure convergence to the optimal policy, we could set <span class="mathquill ud-math">\epsilon_i = \frac{1}{i}</span>.  (You are encouraged to verify that <span class="mathquill ud-math">\epsilon_i > 0</span> for all <span class="mathquill ud-math">i</span>, and <span class="mathquill ud-math">\lim_{i\to\infty} \epsilon_i = 0</span>.)</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-setting-the-value-of-span-classmathquill-ud-mathepsilonspan-in-practice">## Setting the Value of <span class="mathquill ud-math">\epsilon</span>, in Practice</h2>
<p>As you read in the above section, in order to guarantee convergence, we must let <span class="mathquill ud-math">\epsilon_i</span> decay in accordance with the GLIE conditions.  But sometimes "guaranteed convergence" <em>isn't good enough</em> in practice, since this really doesn't tell you how long you have to wait!  It is possible that you could need trillions of episodes to recover the optimal policy, for instance, and the "guaranteed convergence" would still be accurate! </p>
<blockquote>
  <p>Even though convergence is <strong>not</strong> guaranteed by the mathematics, you can often get better results by either:</p>
  <ul>
  <li>using fixed <span class="mathquill ud-math">\epsilon</span>, or</li>
  <li>letting <span class="mathquill ud-math">\epsilon_i</span> decay to a small positive number, like 0.1.  </li>
  </ul>
</blockquote>
<p>This is because one has to be very careful with setting the decay rate for <span class="mathquill ud-math">\epsilon</span>; letting it get too small too fast can be disastrous.  If you get late in training and <span class="mathquill ud-math">\epsilon</span> is really small, you pretty much want the agent to have already converged to the optimal policy, as it will take way too long otherwise for it to test out new actions!</p>
<p>As a famous example in practice, you can read more about how the value of <span class="mathquill ud-math">\epsilon</span> was set in the famous DQN algorithm by reading the Methods section of <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" target="_blank">the research paper</a>: </p>
<blockquote>
  <p><em>The behavior policy during training was epsilon-greedy with epsilon annealed linearly from 1.0 to 0.1 over the first million frames, and fixed at 0.1 thereafter.</em></p>
</blockquote>
<p>When you implement your own algorithm for MC control later in this lesson, you are strongly encouraged to experiment with setting the value of <span class="mathquill ud-math">\epsilon</span> to build your intuition.  </p>
</div>

</div>
<div class="divider"></div>
          </div>
        </div>
      </main>

      <footer class="footer">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p class="text-center">
                <a href="https://github.com/udacimak/udacimak#readme" target="_blank">udacimak v1.2.0</a>
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </div>


  <script src="../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../assets/js/bootstrap.min.js"></script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });
    });
  </script>
</body>

</html>
