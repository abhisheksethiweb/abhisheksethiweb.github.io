WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.640
我们已经了解了预测问题

00:00:02.640 --> 00:00:05.849
现在可以讨论控制问题了

00:00:05.849 --> 00:00:11.539
智能体如何通过与环境互动得出最优策略？

00:00:11.539 --> 00:00:13.529
终于能在这门课程中回答这个问题了

00:00:13.529 --> 00:00:16.265
太让人兴奋了

00:00:16.265 --> 00:00:20.300
要了解蒙特卡洛控制算法

00:00:20.300 --> 00:00:24.620
有必要先回顾下动态规划设置流程

00:00:24.620 --> 00:00:29.839
在该设置中 我们首先了解的控制算法是策略迭代

00:00:29.839 --> 00:00:33.539
即执行一系列的评估和完善步骤

00:00:33.539 --> 00:00:38.369
使评估步骤不断接近收敛效果

00:00:38.369 --> 00:00:41.664
截断策略迭代有所不同

00:00:41.664 --> 00:00:45.254
我们不再强制评估步骤达到收敛效果

00:00:45.255 --> 00:00:50.295
而是只对状态空间执行一定数量的循环过程

00:00:50.295 --> 00:00:53.490
值迭代是指仅完成一次评估步骤

00:00:53.490 --> 00:00:57.530
然后进入完善步骤

00:00:57.530 --> 00:01:00.420
虽然这些算法有所不同

00:01:00.420 --> 00:01:04.155
但是对侧重于共同之处有帮助

00:01:04.155 --> 00:01:07.769
我们将这种一般流程

00:01:07.769 --> 00:01:10.739
称为广义策略迭代

00:01:10.739 --> 00:01:13.259
即不对策略评估周期次数进行限制

00:01:13.260 --> 00:01:18.195
并且不对收敛接近程度进行限制

00:01:18.194 --> 00:01:22.484
虽然我们在动态规划设定下得出了这些算法

00:01:22.484 --> 00:01:24.894
但是我们将在这门课程中研究的

00:01:24.894 --> 00:01:27.729
所有强化学习方法都符合这一格式

00:01:27.730 --> 00:01:32.480
在下个视频中 我们将运用这些概念为蒙特卡洛控制指定一个算法

