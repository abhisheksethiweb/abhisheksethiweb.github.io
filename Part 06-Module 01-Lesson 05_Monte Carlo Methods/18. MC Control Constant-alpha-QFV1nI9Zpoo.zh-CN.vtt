WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.580
目前 策略评估的更新步骤看起来差不多是这样

00:00:04.580 --> 00:00:06.105
你生成一个阶段

00:00:06.105 --> 00:00:09.220
然后对于经历的每个状态动作对

00:00:09.220 --> 00:00:12.554
你都计算接下来的相应回报

00:00:12.554 --> 00:00:16.004
然后使用该回报获得更新的估算

00:00:16.004 --> 00:00:20.594
我们将进一步查看这个更新步骤 以便完善它

00:00:20.594 --> 00:00:22.679
你可以将其看做

00:00:22.679 --> 00:00:25.739
首先计算最近取样的回报

00:00:25.739 --> 00:00:28.799
和对应的状态动作对之间的差别

00:00:28.800 --> 00:00:30.914
我们用 δt 表示

00:00:30.914 --> 00:00:33.140
可以看做误差项

00:00:33.140 --> 00:00:37.215
因为它是我们期望的回报

00:00:37.215 --> 00:00:39.553
与实际回报之间的差

00:00:39.552 --> 00:00:41.969
如果该误差为正

00:00:41.969 --> 00:00:43.929
则表示我们收到的回报

00:00:43.929 --> 00:00:47.640
比值函数预期的要大

00:00:47.640 --> 00:00:50.155
在这种情况下 值函数太低

00:00:50.155 --> 00:00:53.829
我们使用这个更新步骤增大估值

00:00:53.829 --> 00:00:55.004
另一方面

00:00:55.005 --> 00:00:57.090
如果误差为负

00:00:57.090 --> 00:01:02.760
表示误差项比动作值函数预期的要高

00:01:02.759 --> 00:01:05.655
因此有必要考虑这个新的证据

00:01:05.655 --> 00:01:08.805
并降低预期的动作值函数

00:01:08.805 --> 00:01:12.465
到底要增大或降低该函数多少幅度呢？

00:01:12.465 --> 00:01:16.950
目前该算法降低的量

00:01:16.950 --> 00:01:21.790
与我们已经经历状态动作对的次数成反比

00:01:21.790 --> 00:01:24.085
因此前几次经历该对时

00:01:24.084 --> 00:01:26.259
变化可能很大

00:01:26.260 --> 00:01:27.840
但是在后面的时间点

00:01:27.840 --> 00:01:30.734
当这个分数的分母变得很大时

00:01:30.734 --> 00:01:33.344
变化会越来越小

00:01:33.344 --> 00:01:35.965
要理解为何是这种情况

00:01:35.965 --> 00:01:38.370
你需要记住 这个方程式计算的是

00:01:38.370 --> 00:01:41.665
所有取样回报的平均值

00:01:41.665 --> 00:01:45.795
如果已经拥有 999 次回报的平均值

00:01:45.795 --> 00:01:49.575
当你考虑第 1000 次回报时

00:01:49.575 --> 00:01:52.605
将不会对平均值有多大的改变

00:01:52.605 --> 00:01:53.905
记住这些信息后

00:01:53.905 --> 00:01:56.400
我们将更改该算法

00:01:56.400 --> 00:02:00.765
并改用常量步长 表示为 α

00:02:00.765 --> 00:02:03.030
这样可以确保稍后获得的回报

00:02:03.030 --> 00:02:05.835
比之前获得的回报更受重视

00:02:05.834 --> 00:02:09.599
这样的话 智能体将最信任最新的回报

00:02:09.599 --> 00:02:13.129
并逐渐忘记先前获得的回报

00:02:13.129 --> 00:02:17.734
这一点非常重要 因为该策略不断变化

00:02:17.734 --> 00:02:21.159
每一步都越来越优化

00:02:21.159 --> 00:02:27.030
因此实际上 后面的时间步对估算动作值来说很重要

00:02:27.030 --> 00:02:29.370
强烈建议你对你的蒙特卡洛策略评估算法

00:02:29.370 --> 00:02:33.000
进行这一修改

