WEBVTT
Kind: captions
Language: zh-CN

00:00:00.370 --> 00:00:04.269
为了解答这个问题 我们来仔细查看数据

00:00:04.269 --> 00:00:09.039
红色的点和绿色的点似乎可以被一条线很好地分开

00:00:10.070 --> 00:00:14.140
我们可以看出在这条线上方的大部分点是绿色的

00:00:14.140 --> 00:00:17.859
而下方的大部分点是红色的 但有些数据点例外

00:00:18.920 --> 00:00:23.760
我们以条线为模型 每当接到新的学生申请

00:00:23.760 --> 00:00:26.670
我们把他们的成绩画在坐标图上

00:00:27.910 --> 00:00:30.899
如果数据点是在这条线的上方 那么预测他们会被录取

00:00:31.989 --> 00:00:35.530
如果数据点是在下方 则预测他们会被拒绝

00:00:35.530 --> 00:00:40.260
学生 3 的数据坐标为 (7, 6)

00:00:40.259 --> 00:00:44.689
位于直线上方 因此我们判断这个学生会被录取

00:00:46.500 --> 00:00:48.849
如果你的回答是肯定的 那么它是正确答案

00:00:50.100 --> 00:00:53.420
这种方法叫做逻辑回归 (Logistic Regression)

00:00:53.420 --> 00:00:57.960
现在的问题是 我们如何找到这条能最好地分割数据点的线？

00:00:59.390 --> 00:01:02.460
来看一个简单的例子

00:01:02.460 --> 00:01:06.670
如何画出一条线以最好地区分绿色数据点和红色数据点？

00:01:06.670 --> 00:01:08.780
计算机无法依靠视觉来画出这条线

00:01:08.780 --> 00:01:11.379
所以我们从画一条像这样的随机的线开始

00:01:12.590 --> 00:01:15.900
有了这条线 我们再随机地规定

00:01:15.900 --> 00:01:19.530
位于线的上方的点为绿色 下方的点为红色

00:01:20.831 --> 00:01:22.729
然后就像线性回归那样

00:01:22.730 --> 00:01:26.170
我们先计算这条线的效果

00:01:26.170 --> 00:01:29.859
一个简单的测量误差的标准是出错的数目

00:01:29.859 --> 00:01:33.250
即被错误归类的数据点的数量

00:01:33.250 --> 00:01:38.319
这条线错判了两个点 一个红色的点和一个绿色的点

00:01:38.319 --> 00:01:40.059
因此我们说它有两个错误

00:01:41.439 --> 00:01:43.109
仍然与线性回归类似

00:01:43.109 --> 00:01:47.209
我们移动这条线 通过梯度下降算法

00:01:47.209 --> 00:01:48.419
最小化错误数量

00:01:49.640 --> 00:01:51.650
如果沿着这个方向稍微移动这条线

00:01:51.650 --> 00:01:55.530
我们可以看到它开始能正确地归类其中一个数据点

00:01:55.530 --> 00:01:57.620
把错误数目降低到了一个

00:01:58.790 --> 00:02:01.380
如果继续移动这条线 它正确归类了另一个点

00:02:01.379 --> 00:02:04.569
错误数目被降低到了零

00:02:06.150 --> 00:02:09.289
实际使用中 为了正确地使用梯度下降算法

00:02:09.288 --> 00:02:12.759
我们需要最小化的并不是错误数目

00:02:12.759 --> 00:02:13.840
取而代之的是

00:02:13.840 --> 00:02:18.420
能代表错误数目的对数损失函数 (log loss function)

00:02:18.419 --> 00:02:20.699
我们进行更仔细的学习

00:02:20.699 --> 00:02:23.939
这里有六个数据点 其中四个被正确归类

00:02:23.939 --> 00:02:25.669
它们是两个红色和两个绿色

00:02:25.669 --> 00:02:28.609
两个被错误归类 它们是一个红色和一个绿色

00:02:29.860 --> 00:02:33.410
误差函数会对这两个被错误归类的数据点施加很大的惩罚

00:02:33.409 --> 00:02:39.039
而对四个被正确归类的点施加很小的惩罚

00:02:39.039 --> 00:02:41.799
在这门课程中我们将会正式地学习误差函数公式

00:02:43.099 --> 00:02:46.030
我们现在使用所有数据点的错误之和作为误差函数

00:02:46.030 --> 00:02:49.140
我们得到一个很大的误差值

00:02:49.139 --> 00:02:52.799
因为两个被错判的点带来很大的误差

00:02:53.939 --> 00:02:57.800
现在四处移动这条线以将误差降到最小

00:02:59.099 --> 00:03:00.849
如果我们沿着这个方向移动这条线

00:03:00.849 --> 00:03:05.509
可以看到误差有些误差减小了 有些则微微增加了

00:03:05.509 --> 00:03:09.159
但总体上 误差之和变小了

00:03:09.159 --> 00:03:12.799
因为我们正确地归类了之前被错判的两个点

00:03:13.939 --> 00:03:15.969
这个过程的意图是

00:03:15.969 --> 00:03:19.759
找出能最小化误差函数的最佳拟合线

00:03:21.139 --> 00:03:23.399
我们如何最小化误差函数？

00:03:23.400 --> 00:03:24.870
依旧是使用梯度下降算法

00:03:26.129 --> 00:03:28.944
我们再次回到珠穆朗玛峰顶

00:03:28.944 --> 00:03:33.120
我们所在的位置很高 因为此时有很大的误差

00:03:33.120 --> 00:03:36.819
这可以从绿色和红色区域之和的大小中看出

00:03:36.819 --> 00:03:40.000
我们探索四周寻找下降最大的方向

00:03:40.000 --> 00:03:41.030
或者等价地

00:03:41.030 --> 00:03:46.219
寻找能通过移动直线最大程度减小误差的方向

00:03:46.219 --> 00:03:48.310
我们决定沿着这个方向前进一步

00:03:48.310 --> 00:03:51.460
现在只有一个点被错误地归类

00:03:51.460 --> 00:03:55.270
我们可以看到这种方法如何减小误差函数 把我们从山顶上带下来

00:03:55.270 --> 00:03:56.180
我们继续这样做

00:03:56.180 --> 00:04:00.300
沿着最大程度减小误差的方向前进一步

00:04:00.300 --> 00:04:01.820
就到达了山底

00:04:01.819 --> 00:04:04.729
因为我们已经将误差减少到了最小值

