WEBVTT
Kind: captions
Language: en

00:00:00.370 --> 00:00:04.270
To answer this question let's
look closely at the data.

00:00:04.270 --> 00:00:09.040
It seems that the red and green dots
are nicely separated by a line.

00:00:10.070 --> 00:00:14.140
Here is the line and we can see
that most points over it are green,

00:00:14.140 --> 00:00:17.860
whereas most points under it are red,
with some exceptions.

00:00:18.920 --> 00:00:23.760
So that line is going to be our model
and from now on, every time we get

00:00:23.760 --> 00:00:26.670
a new student, we check their scores and
plot them in this graph.

00:00:27.910 --> 00:00:30.900
If they end up over the line,
we predict that they'll get accepted.

00:00:31.990 --> 00:00:35.530
And if they end up below the line,
we predict that they'll get rejected.

00:00:35.530 --> 00:00:40.260
So, since student three ends up here and
the point with coordinate seven and

00:00:40.260 --> 00:00:44.690
six which is over the line, we conclude
that the string gets accepted.

00:00:46.500 --> 00:00:48.849
So, if you said yes,
that is the correct answer.

00:00:50.100 --> 00:00:53.420
This method is known as,
Logistic Regression.

00:00:53.420 --> 00:00:57.960
And now the question is, how do we find
this line that best cuts the data?

00:00:59.390 --> 00:01:02.460
So let's look at a simple example,
how do we draw the line that

00:01:02.460 --> 00:01:06.670
best separates the green
points from the red points?

00:01:06.670 --> 00:01:08.780
Well again the computer can't
really eyeball the line.

00:01:08.780 --> 00:01:11.380
And so, we can start by drawing
a random line like this one.

00:01:12.590 --> 00:01:15.900
And now given this line, let's
randomly say that we label the region

00:01:15.900 --> 00:01:19.530
over the line as green and
the region under the line as red.

00:01:20.832 --> 00:01:22.730
So just with linear regression,

00:01:22.730 --> 00:01:26.170
we'll first try to see how
bad this first line is.

00:01:26.170 --> 00:01:29.860
A simple measure of how bad the line is,
is just the number of errors.

00:01:29.860 --> 00:01:33.250
That is,
the number of misclassified points.

00:01:33.250 --> 00:01:38.320
This line misclassified two points,
one red and one green.

00:01:38.320 --> 00:01:40.060
So we'll say, it has two errors.

00:01:41.440 --> 00:01:43.110
And again with linear regression.

00:01:43.110 --> 00:01:47.210
What we'll do is, move the line around
trying to minimize the number of errors,

00:01:47.210 --> 00:01:48.420
using gradient descent.

00:01:49.640 --> 00:01:51.650
So if we move the line
a bit in this direction,

00:01:51.650 --> 00:01:55.530
we can see that we start correctly
classifying one of the points,

00:01:55.530 --> 00:01:57.620
bringing down the number
of errors to one.

00:01:58.790 --> 00:02:01.380
And if we move it even more,
we correctly classify the other one

00:02:01.380 --> 00:02:04.570
of the points, bringing down
the number of errors to zero.

00:02:06.150 --> 00:02:09.289
In reality, in order to properly
use the gradient descent method,

00:02:09.289 --> 00:02:12.760
it turns out the number of errors
is not what we need to minimize.

00:02:12.760 --> 00:02:13.840
But instead,

00:02:13.840 --> 00:02:18.420
something that captures the number
of errors called log loss function.

00:02:18.420 --> 00:02:20.700
Let's actually study
that in more detail.

00:02:20.700 --> 00:02:23.940
So here are our six points again with
four of them correctly classified,

00:02:23.940 --> 00:02:25.670
two red and two green and

00:02:25.670 --> 00:02:28.610
two of them incorrectly classified,
one red and one green.

00:02:29.860 --> 00:02:33.410
The error function will assign
a large penalty to the two incorrectly

00:02:33.410 --> 00:02:39.040
classified points and small penalty to
the four correctly classified points.

00:02:39.040 --> 00:02:41.800
We'll actually learn the formula for
the error function in the class.

00:02:43.100 --> 00:02:46.030
Now we obtain the error function
by adding all the errors

00:02:46.030 --> 00:02:49.140
from the corresponding points,
here we obtain a large

00:02:49.140 --> 00:02:52.800
number since the two misclassified
points add a lot to the error function.

00:02:53.940 --> 00:02:57.800
Now the idea is to, jiggle the line
around to minimize this error.

00:02:59.100 --> 00:03:00.850
So if we move the line
in this direction,

00:03:00.850 --> 00:03:05.510
we can see that some errors decrease and
some slightly increase.

00:03:05.510 --> 00:03:09.160
But in general, when we consider the
sum, the sum gets smaller since we've

00:03:09.160 --> 00:03:12.800
correctly classified the two points
that were misclassified before.

00:03:13.940 --> 00:03:15.970
So the point of this process is,

00:03:15.970 --> 00:03:19.760
to find the best line fit by
minimizing the error function.

00:03:21.140 --> 00:03:23.400
And how do we minimize
the error function?

00:03:23.400 --> 00:03:24.870
Again, with gradient descent.

00:03:26.130 --> 00:03:28.945
So once more, here we are at
the summit of Mount Errorest,

00:03:28.945 --> 00:03:33.120
we're quite high up, because our
error is large, we're you can see,

00:03:33.120 --> 00:03:36.820
how the height is the sum
of the green and red areas.

00:03:36.820 --> 00:03:40.000
So, we explore around to see what
direction brings us down the most or

00:03:40.000 --> 00:03:41.030
equivalently.

00:03:41.030 --> 00:03:46.220
What direction can we move the line in
order to reduce the error the most?

00:03:46.220 --> 00:03:48.310
And we decide to take
a step in that direction.

00:03:48.310 --> 00:03:51.460
So now, we're misclassifying
only one of the points.

00:03:51.460 --> 00:03:55.270
We can see how that decreases our
function, bringing us down the mountain.

00:03:55.270 --> 00:03:56.180
And we do it again,

00:03:56.180 --> 00:04:00.300
we take a step in the direction
that reduces the arrow the most.

00:04:00.300 --> 00:04:01.820
Now, we're at the bottom
of the mountains,

00:04:01.820 --> 00:04:04.730
since we were reduced the error
twist minimum, possible value.

