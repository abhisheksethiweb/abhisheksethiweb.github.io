WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.759
为了理解如何寻找最佳策略

00:00:03.759 --> 00:00:06.344
我们需要通过一个示例来讲解

00:00:06.344 --> 00:00:11.445
假设有一个非常非常小的世界 里面住着一个智能体

00:00:11.445 --> 00:00:15.769
这个小世界主要由 9 块草坪组成

00:00:15.769 --> 00:00:20.094
但是其中的两个位置有大山

00:00:20.094 --> 00:00:21.599
将这个世界的所有 9 个可能的位置

00:00:21.600 --> 00:00:25.760
看做状态和环境

00:00:25.760 --> 00:00:27.660
在每个时间点

00:00:27.660 --> 00:00:30.240
假设智能体只能上下左右移动

00:00:30.239 --> 00:00:35.600
并且只能采取不离开网格的动作

00:00:35.600 --> 00:00:39.899
图中的箭头展示了可能的动作

00:00:39.899 --> 00:00:42.420
假设智能体的目标是

00:00:42.420 --> 00:00:46.664
尽快抵达右下角

00:00:46.664 --> 00:00:49.034
我们将此任务看做阶段性任务

00:00:49.034 --> 00:00:52.369
每当智能体抵达目的地时 该阶段结束

00:00:52.369 --> 00:00:56.530
因此不用担心离开该目标状态

00:00:56.530 --> 00:01:02.590
此外 假设对于大多数变动 智能体都收到奖励 -1

00:01:02.590 --> 00:01:06.075
但是如果遇到大山

00:01:06.075 --> 00:01:08.460
则奖励为 -3

00:01:08.459 --> 00:01:10.214
如果遇到目标状态

00:01:10.215 --> 00:01:12.125
则奖励为 5

00:01:12.125 --> 00:01:14.670
我们将奖励信号看做

00:01:14.670 --> 00:01:18.570
每一个时间步智能体没有抵达目标就会受到惩罚

00:01:18.569 --> 00:01:20.159
可以将大山看做

00:01:20.159 --> 00:01:22.319
具有非常大的惩罚力度

00:01:22.319 --> 00:01:25.494
因为跨过大山需要的时间更长

00:01:25.495 --> 00:01:30.300
奖励机制鼓励智能体尽快抵达目的地

00:01:30.299 --> 00:01:31.769
抵达目的地后

00:01:31.769 --> 00:01:33.089
就会获得奖励 5

00:01:33.090 --> 00:01:34.665
这一阶段结束

00:01:34.665 --> 00:01:38.060
我们将通过该示例讲解如何寻找最佳策略

