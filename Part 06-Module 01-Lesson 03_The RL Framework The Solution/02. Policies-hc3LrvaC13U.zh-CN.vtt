WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.875
我们刚刚介绍了我们可以使用

00:00:03.875 --> 00:00:06.150
马尔可夫决策流程 (MDP)

00:00:06.150 --> 00:00:09.144
正式地定义想要通过强化学习解决的问题

00:00:09.144 --> 00:00:14.394
在本视频中 我们将指定该问题解决方案的正式定义

00:00:14.394 --> 00:00:17.730
我们可以将解决方案看作

00:00:17.730 --> 00:00:21.725
智能体为了实现目标必须学会的一系列动作

00:00:21.725 --> 00:00:23.550
例如 为了行走

00:00:23.550 --> 00:00:28.675
类人机器人需要学习向接合处应用力的合适方式

00:00:28.675 --> 00:00:32.924
但是我们知道 正确的动作会随着状况而改变

00:00:32.924 --> 00:00:34.949
如果机器人遇到墙壁

00:00:34.950 --> 00:00:40.505
一系列最佳动作与道路畅通无阻时会不一样

00:00:40.505 --> 00:00:43.859
奖励始终由作出决策时遇到的状态

00:00:43.859 --> 00:00:47.879
及其之后的状态决定

00:00:47.880 --> 00:00:50.310
因此 只要智能体学会根据

00:00:50.310 --> 00:00:55.109
它能够观察的任何环境状态作出合适的动作响应

00:00:55.109 --> 00:00:58.045
我们就拥有了问题的解决方案

00:00:58.045 --> 00:01:00.825
这就要提到策略这一概念

00:01:00.825 --> 00:01:04.125
最简单的策略是从一组环境状态

00:01:04.125 --> 00:01:07.885
到一组潜在动作的映射

00:01:07.885 --> 00:01:10.335
如果你不熟悉映射概念

00:01:10.334 --> 00:01:14.599
可以将策略看做一个工厂

00:01:14.599 --> 00:01:19.259
工厂将环境状态作为输入 并将智能体将采取的相应动作作为输出

00:01:19.260 --> 00:01:22.065
如果智能体想要跟踪策略

00:01:22.064 --> 00:01:27.034
它只需构建该工厂或指定该映射

00:01:27.034 --> 00:01:30.259
我们将这种策略称之为决定性策略

00:01:30.260 --> 00:01:32.085
输入是状态

00:01:32.084 --> 00:01:34.500
输出是要采取的动作

00:01:34.500 --> 00:01:36.135
可以看出

00:01:36.135 --> 00:01:40.435
通常将策略表示为希腊字母 π

00:01:40.435 --> 00:01:44.644
我们将了解的另一种策略是随机性策略

00:01:44.644 --> 00:01:49.414
随机性策略使智能体能够随机地选择动作

00:01:49.415 --> 00:01:55.094
我们将随机性策略定义为以下映射

00:01:55.094 --> 00:01:58.950
接收环境状态 s 和动作

00:01:58.950 --> 00:02:03.000
并返回智能体在状态 s 时采取动作 a 的概率

00:02:03.000 --> 00:02:07.370
为了进行阐述 我们再看看上一节课的回收机器人示例

00:02:07.370 --> 00:02:10.289
决策性策略将指定为

00:02:10.289 --> 00:02:14.054
每当电量很低时 则充电

00:02:14.055 --> 00:02:17.800
每当电量很高时 则搜索易拉罐

00:02:17.800 --> 00:02:23.140
而随机性策略则是

00:02:23.139 --> 00:02:25.439
每当电量很低时 充电概率是 50%

00:02:25.439 --> 00:02:28.650
等待概率是 40%

00:02:28.650 --> 00:02:30.210
搜索易拉罐的概率是 10%

00:02:30.210 --> 00:02:32.310
每当电量很高时

00:02:32.310 --> 00:02:35.219
搜索易拉罐的概率是 90%

00:02:35.219 --> 00:02:37.634
否则等待有人扔进易拉罐

00:02:37.634 --> 00:02:41.804
需要注意的是 任何决策性策略

00:02:41.805 --> 00:02:46.319
都可以采用随机性策略通常用到的同一记法

00:02:46.319 --> 00:02:50.204
例如 这个策略可以表示为

00:02:50.205 --> 00:02:52.415
每当电量很低时

00:02:52.414 --> 00:02:55.469
充电概率是 100%

00:02:55.469 --> 00:02:58.129
每当电量很高时

00:02:58.129 --> 00:03:02.460
搜索易拉罐的概率是 100%

00:03:02.460 --> 00:03:07.945
我们将在这门课程中了解随机性等级各不相同的策略

00:03:07.944 --> 00:03:10.889
现在你可能会疑问

00:03:10.889 --> 00:03:14.084
我们已经知道如何指定策略

00:03:14.085 --> 00:03:19.189
应该采取什么样的措施来保证智能体的策略是最佳策略？

00:03:19.189 --> 00:03:24.000
我们将在接下来的几个部分回答这一问题

