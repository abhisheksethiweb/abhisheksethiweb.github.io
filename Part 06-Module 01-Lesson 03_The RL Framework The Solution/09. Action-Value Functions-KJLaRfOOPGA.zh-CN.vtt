WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.799
到目前为止 我们介绍的都是策略的状态值函数

00:00:04.799 --> 00:00:06.689
对于每个状态 s

00:00:06.690 --> 00:00:11.160
它都会生成智能体从状态 s 开始时的期望折扣回报

00:00:11.160 --> 00:00:16.065
然后使用该策略选择所有时间步的动作

00:00:16.065 --> 00:00:18.540
你已经看过几个示例 知道如何计算

00:00:18.539 --> 00:00:22.030
对应某个策略的状态值函数

00:00:22.030 --> 00:00:24.100
在这部分 我们将定义一种新的值函数

00:00:24.100 --> 00:00:27.810
称之为动作值函数

00:00:27.809 --> 00:00:31.550
这种值函数表示为小写的 q 而不是 v

00:00:31.550 --> 00:00:35.317
状态值是环境状态的函数

00:00:35.317 --> 00:00:40.695
而动作值是环境状态和智能体动作的函数

00:00:40.695 --> 00:00:43.149
对于每个状态 s 和动作 a

00:00:43.149 --> 00:00:45.600
动作值函数都会生成

00:00:45.600 --> 00:00:49.240
智能体从状态 s 开始并选择动作 a 时

00:00:49.240 --> 00:00:53.190
获得的期望折扣回报

00:00:53.189 --> 00:00:58.000
然后使用该策略选择所有未来时间步的动作

00:00:58.000 --> 00:00:59.479
和状态值函数一样

00:00:59.479 --> 00:01:04.149
自己计算该公式可以加深理解

00:01:04.150 --> 00:01:06.180
对于状态值函数

00:01:06.180 --> 00:01:10.650
我们用网格中的数字记录每个状态的值

00:01:10.650 --> 00:01:13.725
对于动作值函数 我们将采取相似的方法

00:01:13.724 --> 00:01:17.153
现在对于每个状态 我们最多需要四个值

00:01:17.153 --> 00:01:20.159
每个对应不同的动作

00:01:20.159 --> 00:01:21.840
这四个数字对应相同的状态

00:01:21.840 --> 00:01:26.130
但是顶部的数字对应向上动作

00:01:26.129 --> 00:01:31.829
右侧数字对应在遵守策略向右移动前的动作 等等

00:01:31.829 --> 00:01:33.359
很快你就会明白

00:01:33.359 --> 00:01:36.194
将最终状态除外

00:01:36.194 --> 00:01:39.479
我拆分了该图表 以便留出空间

00:01:39.480 --> 00:01:43.795
记录对应于每个潜在状态和动作的值

00:01:43.795 --> 00:01:47.212
看看我们能否计算某些动作值

00:01:47.212 --> 00:01:49.575
我们将从这个状态开始

00:01:49.575 --> 00:01:54.034
计算对应于该状态和动作的值

00:01:54.034 --> 00:01:56.295
智能体从该状态开始

00:01:56.295 --> 00:02:00.615
向下采取动作并获得奖励 -1

00:02:00.614 --> 00:02:03.045
然后对于未来的每个时间步

00:02:03.045 --> 00:02:07.320
它都遵守该策略 直到抵达最终状态

00:02:07.319 --> 00:02:10.004
我们将一路上遇到的所有奖励相加

00:02:10.004 --> 00:02:13.319
结果是 0

00:02:13.319 --> 00:02:16.769
0 对应智能体从该状态开始

00:02:16.770 --> 00:02:21.110
并采取向下动作时的动作值

00:02:21.110 --> 00:02:23.985
我们再计算另一个动作值

00:02:23.985 --> 00:02:28.020
这次对应这个状态和向上动作

00:02:28.020 --> 00:02:31.890
智能体从该状态开始并采取向上动作

00:02:31.889 --> 00:02:35.454
获得奖励 -1

00:02:35.455 --> 00:02:39.830
然后在所有未来时间步都遵守策略

00:02:39.830 --> 00:02:42.210
我们将一路上收集的奖励相加

00:02:42.210 --> 00:02:46.055
结果是累积奖励为 1

00:02:46.055 --> 00:02:51.115
这个值对应该状态和向上动作的动作值

00:02:51.115 --> 00:02:56.439
你可以继续下去并为所有合理的状态动作对完成这一计算流程

00:02:56.439 --> 00:02:59.775
当你这么计算时 就会获得这个动作值函数

00:02:59.775 --> 00:03:04.204
强烈建议你自己计算并检查这些值

00:03:04.204 --> 00:03:09.780
在继续学习新内容之前 有必要复习下之前学到的某些知识

00:03:09.780 --> 00:03:16.862
注意 我们用 v* 表示最优状态值函数

00:03:16.862 --> 00:03:22.080
同样 我们用 q* 表示最优动作值函数

