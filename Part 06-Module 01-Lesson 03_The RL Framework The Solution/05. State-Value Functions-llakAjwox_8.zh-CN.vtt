WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.209
我们介绍了这个网格世界示例

00:00:03.209 --> 00:00:08.164
并讲解了尽快实现目标状态的最佳策略

00:00:08.164 --> 00:00:09.689
我们先从一个非常糟糕的策略开始

00:00:09.689 --> 00:00:13.064
看看它为何很糟糕

00:00:13.064 --> 00:00:14.824
然后逐渐改善它

00:00:14.824 --> 00:00:17.925
具体而言 在此策略中

00:00:17.925 --> 00:00:21.449
智能体将以这种非常迂回的方式抵达每个状态

00:00:21.449 --> 00:00:26.219
我们将忽略智能体始终无法在此策略下采取的变动

00:00:26.219 --> 00:00:30.390
为了理解为何此策略很糟糕

00:00:30.390 --> 00:00:33.750
我们计算下最终的累积奖励

00:00:33.750 --> 00:00:36.750
如果智能体从该网格的左上角开始

00:00:36.750 --> 00:00:40.215
并按照该策略抵达目标状态

00:00:40.215 --> 00:00:43.470
它只是一路收集奖励

00:00:43.469 --> 00:00:46.185
即 -1 加 -1

00:00:46.185 --> 00:00:47.655
再加 -1 等等

00:00:47.655 --> 00:00:51.509
将所有奖励相加

00:00:51.509 --> 00:00:53.189
结果是 -6

00:00:53.189 --> 00:00:56.963
假设我们没有进行折扣 即折扣率是 1

00:00:56.963 --> 00:01:00.570
我们记下这个 -6

00:01:00.570 --> 00:01:05.394
它表示从左上角开始

00:01:05.394 --> 00:01:08.474
然后在所有时间步都遵守该策略

00:01:08.474 --> 00:01:11.459
结果是 -6

00:01:11.459 --> 00:01:17.009
现在假设智能体在向右一个网格的位置开始

00:01:17.010 --> 00:01:20.910
那么对于同一策略 回报可能是多少？

00:01:20.909 --> 00:01:25.706
我们将智能体在一路上获得的奖励相加

00:01:25.706 --> 00:01:26.880
相加之后

00:01:26.879 --> 00:01:28.793
回报是 -5

00:01:28.793 --> 00:01:31.534
同样记下该数据

00:01:31.534 --> 00:01:35.819
继续下去并对每个状态执行这一流程

00:01:35.819 --> 00:01:41.219
目标状态的回报应该是 0

00:01:41.219 --> 00:01:43.590
毕竟 如果智能体从目标开始

00:01:43.590 --> 00:01:47.609
这一阶段立即结束 没有获得奖励

00:01:47.609 --> 00:01:51.170
这样的话 无论智能体从哪里开始

00:01:51.171 --> 00:01:54.465
我们都可以记录获得的回报

00:01:54.465 --> 00:01:58.680
以这种方式分析这个糟糕的策略可以帮助我们完善它

00:01:58.680 --> 00:02:01.590
但是在完善之前

00:02:01.590 --> 00:02:07.064
我们来介绍下这一流程涉及的记法和术语

00:02:07.064 --> 00:02:11.775
你可以将这个数字网格看做环境状态函数

00:02:11.775 --> 00:02:15.060
对于每个状态 它都有一个对应的数字

00:02:15.060 --> 00:02:19.004
我们将此函数称为状态值函数

00:02:19.004 --> 00:02:23.340
对于每个状态 它都生成当智能体从该状态开始

00:02:23.340 --> 00:02:28.509
然后在所有时间步都遵循该策略时可能会获得的回报

00:02:28.509 --> 00:02:33.969
但是更常见的情况是表示为公式记法

00:02:33.969 --> 00:02:35.939
在介绍该公式记法之前

00:02:35.939 --> 00:02:38.620
我要提醒下 它看起来会比较复杂

00:02:38.620 --> 00:02:44.539
但是对应的正是我们已经讨论的知识 这个公式是这样的

00:02:44.539 --> 00:02:49.949
策略 π 的状态值函数是环境状态的函数

00:02:49.949 --> 00:02:55.049
对于每个状态 s 它都告诉我们

00:02:55.050 --> 00:02:57.465
如果智能体从该状态 s 开始

00:02:57.465 --> 00:03:01.694
然后在所有时间步根据该策略选择动作 预期的折扣回报是多少

00:03:01.694 --> 00:03:06.344
状态值函数将始终对应特定的策略

00:03:06.344 --> 00:03:08.219
如果我们更改策略

00:03:08.219 --> 00:03:10.574
就会更改状态值函数

00:03:10.574 --> 00:03:13.394
我们通常用小写的 v 表示该函数

00:03:13.395 --> 00:03:17.000
并用下标表示对应的策略

