<!-- udacimak v1.2.0 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Quiz: Optimal Policies</title>
  <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../assets/css/plyr.css">
  <link rel="stylesheet" href="../assets/css/katex.min.css">
  <link rel="stylesheet" href="../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../assets/img/udacimak.png" />
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>The RL Framework: The Solution</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="01. Introduction.html">01. Introduction</a>
    </li>
    <li class="">
      <a href="02. Policies.html">02. Policies</a>
    </li>
    <li class="">
      <a href="03. Quiz Interpret the Policy.html">03. Quiz: Interpret the Policy</a>
    </li>
    <li class="">
      <a href="04. Gridworld Example.html">04. Gridworld Example</a>
    </li>
    <li class="">
      <a href="05. State-Value Functions.html">05. State-Value Functions</a>
    </li>
    <li class="">
      <a href="06. Bellman Equations.html">06. Bellman Equations</a>
    </li>
    <li class="">
      <a href="07. Quiz State-Value Functions.html">07. Quiz: State-Value Functions</a>
    </li>
    <li class="">
      <a href="08. Optimality.html">08. Optimality</a>
    </li>
    <li class="">
      <a href="09. Action-Value Functions.html">09. Action-Value Functions</a>
    </li>
    <li class="">
      <a href="10. Quiz Action-Value Functions.html">10. Quiz: Action-Value Functions</a>
    </li>
    <li class="">
      <a href="11. Optimal Policies.html">11. Optimal Policies</a>
    </li>
    <li class="">
      <a href="12. Quiz Optimal Policies.html">12. Quiz: Optimal Policies</a>
    </li>
    <li class="">
      <a href="13. Summary.html">13. Summary</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">12. Quiz: Optimal Policies</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <h1 id="quiz-optimal-policies">Quiz: Optimal Policies</h1>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>If the state space  <span class="mathquill ud-math">\mathcal{S}</span> and action space <span class="mathquill ud-math">\mathcal{A}</span> are finite, we can represent the optimal action-value function <span class="mathquill ud-math">q_*</span> in a table, where we have one entry for each possible environment state <span class="mathquill ud-math">s \in \mathcal{S}</span> and action <span class="mathquill ud-math">a\in\mathcal{A}</span>. </p>
<p>The value for a particular state-action pair <span class="mathquill ud-math">s,a</span> is the expected return if the agent starts in state <span class="mathquill ud-math">s</span>, takes action <span class="mathquill ud-math">a</span>, and then henceforth follows the optimal policy <span class="mathquill ud-math">\pi_*</span>.  </p>
<p>We have populated some values for a hypothetical Markov decision process (MDP) (where <span class="mathquill ud-math">\mathcal{S}={ s_1, s_2, s_3 }</span> and <span class="mathquill ud-math">\mathcal{A}={a_1, a_2, a_3}</span>) below.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2017-09-25-at-5.51.40-pm.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>You learned in the previous concept that once the agent has determined the optimal action-value function <span class="mathquill ud-math">q_<em></span>, it can quickly obtain an optimal policy <span class="mathquill ud-math">\pi_</em></span> by setting  <span class="mathquill ud-math">\pi_<em>(s) = \arg\max_{a\in\mathcal{A}(s)} q_</em>(s,a)</span> for all <span class="mathquill ud-math">s\in\mathcal{S}</span>.</p>
<p>To see <em>why</em> this should be the case, note that it must hold that <span class="mathquill ud-math">v_<em>(s) = \max_{a\in\mathcal{A}(s)} q_</em>(s,a)</span>.</p>
<p>In the event that there is some state <span class="mathquill ud-math">s\in\mathcal{S}</span> for which multiple actions <span class="mathquill ud-math">a\in\mathcal{A}(s)</span> maximize the optimal action-value function, you can construct an optimal policy by placing any amount of probability on any of the (maximizing) actions.  You need only ensure that the actions that do not maximize the action-value function (for a particular state) are given 0% probability under the policy.</p>
<p>Towards constructing the optimal policy, we can begin by selecting the entries that maximize the action-value function, for each row (or state).</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2017-09-25-at-6.02.37-pm.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>Thus, the optimal policy <span class="mathquill ud-math">\pi_*</span> for the corresponding MDP must satisfy:</p>
<ul>
<li><span class="mathquill ud-math">\pi_<em>(s_1) = a_2</span> (or, equivalently, <span class="mathquill ud-math">\pi_</em>(a_2| s_1) = 1</span>), and</li>
<li><span class="mathquill ud-math">\pi_<em>(s_2) = a_3</span> (or, equivalently, <span class="mathquill ud-math">\pi_</em>(a_3| s_2) = 1</span>).</li>
</ul>
<p>This is because <span class="mathquill ud-math">a_2 = \arg\max_{a\in\mathcal{A}(s_1)}q_<em>(s_1,a)</span>, and <span class="mathquill ud-math">a_3 = \arg\max_{a\in\mathcal{A}(s_2)}q_</em>(s_2,a)</span>.</p>
<p>In other words, under the optimal policy, the agent must choose action <span class="mathquill ud-math">a_2</span> when in state <span class="mathquill ud-math">s_1</span>, and it will choose action <span class="mathquill ud-math">a_3</span> when in state <span class="mathquill ud-math">s_2</span>.  </p>
<p>As for state <span class="mathquill ud-math">s_3</span>, note that <span class="mathquill ud-math">a_1, a_2 \in \arg\max_{a\in\mathcal{A}(s_3)}q_<em>(s_3,a)</span>.  Thus, the agent can choose either action <span class="mathquill ud-math">a_1</span> or <span class="mathquill ud-math">a_2</span> under the optimal policy, but it can never choose action <span class="mathquill ud-math">a_3</span>.  That is, the optimal policy <span class="mathquill ud-math">\pi_</em></span> must satisfy:</p>
<ul>
<li><span class="mathquill ud-math">\pi_*(a_1| s_3) = p</span>,</li>
<li><span class="mathquill ud-math">\pi_*(a_2| s_3) = q</span>, and</li>
<li><span class="mathquill ud-math">\pi_*(a_3| s_3) = 0</span>,</li>
</ul>
<p>where <span class="mathquill ud-math">p,q\geq 0</span>, and <span class="mathquill ud-math">p + q = 1</span>.  </p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="question">Question</h2>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>Consider a different MDP, with a different corresponding optimal action-value function.  Please use this action-value function to answer the following question.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2017-09-25-at-9.18.00-pm.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <form>
    <fieldset>
      <legend><p>Which of the following describes a potential optimal policy that corresponds to the optimal action-value function?</p></legend>
    </fieldset>

    <div>
      <input type="checkbox" value="a1506391418690" name="409981" id="a1506391418690">
      <label for="a1506391418690"><p>The agent always selects action a_1 in state s_1.</p></label>
    </div>
    <div>
      <input type="checkbox" value="a1506392447372" name="409981" id="a1506392447372">
      <label for="a1506392447372"><p>The agent always selects action a_3 in state s_1.</p></label>
    </div>
    <div>
      <input type="checkbox" value="a1506392447990" name="409981" id="a1506392447990">
      <label for="a1506392447990"><p>The agent is free to select either action a_1 or action a_2 in state s_2.</p></label>
    </div>
    <div>
      <input type="checkbox" value="a1506392488342" name="409981" id="a1506392488342">
      <label for="a1506392488342"><p>The agent must select action a_3 in state s_2.</p></label>
    </div>
    <div>
      <input type="checkbox" value="a1506392502012" name="409981" id="a1506392502012">
      <label for="a1506392502012"><p>The agent must select action a_1 in state s_3.</p></label>
    </div>
    <div>
      <input type="checkbox" value="a1506392565169" name="409981" id="a1506392565169">
      <label for="a1506392565169"><p>The agent is free to select either action a_2 or a_3 in state s_3.</p></label>
    </div>
  </form>

  <details>
    <summary><strong>SOLUTION:</strong></summary>
    <ul>
      <li>The agent always selects action a_3 in state s_1.</li>
      <li>The agent is free to select either action a_1 or action a_2 in state s_2.</li>
      <li>The agent must select action a_1 in state s_3.</li>
  </ul>
  </details>
</div>

</div>
<div class="divider"></div>
          </div>
        </div>
      </main>

      <footer class="footer">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p class="text-center">
                <a href="https://github.com/udacimak/udacimak#readme" target="_blank">udacimak v1.2.0</a>
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </div>


  <script src="../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../assets/js/bootstrap.min.js"></script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });
    });
  </script>
</body>

</html>
