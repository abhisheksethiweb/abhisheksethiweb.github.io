WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.349
如果你花时间自己计算了该策略的值函数

00:00:04.349 --> 00:00:09.379
可能会发现 你不用每次都从头开始计算

00:00:09.380 --> 00:00:11.400
尤其是 你不需要查看第一个状态

00:00:11.400 --> 00:00:15.275
将之后的所有奖励相加

00:00:15.275 --> 00:00:16.615
然后查看第二个状态

00:00:16.614 --> 00:00:17.979
并将所有奖励相加

00:00:17.980 --> 00:00:21.105
然后查看第三个状态 将所有奖励相加 等等

00:00:21.105 --> 00:00:23.685
这么做属于重复工作

00:00:23.684 --> 00:00:26.989
实际上有快了很多的方法

00:00:26.989 --> 00:00:32.614
我们删掉大多数的值 只留下最底下的值

00:00:32.615 --> 00:00:36.920
然后看看如何往回重新计算这些值

00:00:36.920 --> 00:00:42.030
在此过程中 我们将发现值函数有一个递归特性

00:00:42.030 --> 00:00:44.730
为此 假设我们尝试计算

00:00:44.729 --> 00:00:48.015
我突出显示的状态的值

00:00:48.015 --> 00:00:53.320
即抵达最终状态之前的奖励之和

00:00:53.320 --> 00:00:55.844
在此示例中 智能体从该状态开始

00:00:55.844 --> 00:00:57.369
遵守该策略

00:00:57.369 --> 00:00:59.909
获得奖励 -1

00:00:59.909 --> 00:01:02.724
然后进入这个值为 2 的下个状态

00:01:02.725 --> 00:01:06.629
这里要注意的是 这个 2 已经对应了

00:01:06.629 --> 00:01:11.509
一直到结束时的奖励之和

00:01:11.510 --> 00:01:13.650
我们不用计算这个和

00:01:13.650 --> 00:01:16.890
而是使用值 2 得出

00:01:16.890 --> 00:01:20.834
该状态是 -1 加上 2 即 1

00:01:20.834 --> 00:01:22.364
同样

00:01:22.364 --> 00:01:27.134
下个状态的值是 -1 加上 1 即 0

00:01:27.135 --> 00:01:33.540
然后是 -3 加上 0 即 -3 等等

00:01:33.540 --> 00:01:38.370
这样 我们可以看出任何状态的值为

00:01:38.370 --> 00:01:44.734
即时奖励加上下个状态的值

00:01:44.734 --> 00:01:45.974
需要注意的是

00:01:45.974 --> 00:01:47.234
为了简便

00:01:47.234 --> 00:01:50.670
我将此示例中的折扣率设成了 1

00:01:50.670 --> 00:01:54.629
但是通常都需要考虑折扣

00:01:54.629 --> 00:01:59.109
因此我们需要使用下个状态的折扣值

00:01:59.109 --> 00:02:02.250
我们可以将此概念表示为

00:02:02.250 --> 00:02:05.900
贝尔曼预期方程

00:02:05.900 --> 00:02:11.175
对于一般的 MDP 我们需要计算和的预期值

00:02:11.175 --> 00:02:12.855
因为通常

00:02:12.854 --> 00:02:14.534
对于更加复杂的环境

00:02:14.534 --> 00:02:19.094
我们无法确定即时奖励和下个状态

00:02:19.094 --> 00:02:23.805
这个方程式非常重要 我们将在后续课程中经常用到它

00:02:23.805 --> 00:02:28.568
暂时只需记住大概含义

00:02:28.568 --> 00:02:30.260
也就是

00:02:30.259 --> 00:02:34.199
我们可以将 MDP 中任何状态的值表示为

00:02:34.199 --> 00:02:38.299
即时奖励和下个状态的折扣值

