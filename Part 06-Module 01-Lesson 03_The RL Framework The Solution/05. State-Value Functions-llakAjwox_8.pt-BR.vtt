WEBVTT
Kind: captions
Language: pt-BR

00:00:00.297 --> 00:00:02.489
Estamos trabalhando
com este exemplo em gridworld

00:00:02.519 --> 00:00:06.090
e procurando pela melhor política
que nos levará ao estado-objetivo

00:00:06.120 --> 00:00:07.642
o mais rápido possível.

00:00:08.113 --> 00:00:10.905
Vamos começar com uma política
muito, muito ruim

00:00:10.935 --> 00:00:13.033
para podermos entender
o motivo dela ser ruim,

00:00:13.063 --> 00:00:15.009
e, depois, trabalhar
para melhorá-la.

00:00:15.270 --> 00:00:17.137
Especificamente,
vamos examinar uma política

00:00:17.167 --> 00:00:19.321
em que o agente
visita todos os estados

00:00:19.351 --> 00:00:21.249
dessa forma circular.

00:00:21.706 --> 00:00:24.809
Podemos ignorar a transição
que o agente nunca vai fazer

00:00:24.839 --> 00:00:26.448
sob essa política.

00:00:26.478 --> 00:00:30.385
Certo, agora para entender
o motivo dessa política ser ruim,

00:00:30.415 --> 00:00:33.473
vamos calcular a recompensa
cumulativa que resultará.

00:00:34.017 --> 00:00:37.418
Se o agente começar no canto
superior esquerdo do mundo,

00:00:37.448 --> 00:00:40.113
e seguir esta política
para chegar ao estado-objetivo,

00:00:40.143 --> 00:00:43.360
ele apenas coleta todas
as recompensas ao longo do caminho.

00:00:43.937 --> 00:00:48.577
Isto é, -1 mais -1 mais -1 de novo
e assim por diante,

00:00:48.607 --> 00:00:51.593
de modo que, se eu somar
todas as recompensas do caminho,

00:00:51.623 --> 00:00:53.256
vou obter -6.

00:00:53.286 --> 00:00:55.064
Digamos que a gente
não desconte,

00:00:55.094 --> 00:00:57.278
ou que a taxa de desconto
seja 1.

00:00:57.308 --> 00:01:01.769
Vamos guardar esse -6
e lembrar que ele representa o fato

00:01:01.799 --> 00:01:05.361
de que se começarmos com o estado
no canto superior esquerdo

00:01:05.391 --> 00:01:08.473
e depois só seguir a política
em todos os instantes de tempo,

00:01:08.503 --> 00:01:11.424
isso vai resultar
em um retorno de -6.

00:01:11.744 --> 00:01:14.265
Mas, agora, digamos que o agente
tenha começado

00:01:14.295 --> 00:01:16.640
em um local mais à direita.

00:01:17.001 --> 00:01:19.321
Então, qual retorno
devemos obter

00:01:19.351 --> 00:01:21.017
sob a mesma política?

00:01:21.047 --> 00:01:24.552
De novo, somamos todas
as recompensas que o agente recebe

00:01:24.582 --> 00:01:26.809
ao longo do caminho
e, ao fazer isso,

00:01:26.839 --> 00:01:28.993
obtemos um retorno
de -5.

00:01:29.023 --> 00:01:31.377
Vamos guardar isso também.

00:01:31.407 --> 00:01:35.137
Podemos continuar fazendo isso
para cada estado do mundo.

00:01:35.632 --> 00:01:37.848
Faz sentido pensar
que o estado-objetivo

00:01:37.878 --> 00:01:40.489
vai resultar
em um retorno de 0,

00:01:40.992 --> 00:01:43.553
afinal, se o agente começar
no objetivo,

00:01:43.583 --> 00:01:45.409
o episódio é finalizado
imediatamente

00:01:45.439 --> 00:01:47.505
e nenhuma recompensa
é recebida.

00:01:47.929 --> 00:01:51.047
Assim, não importa em qual local
o agente comece no mundo,

00:01:51.077 --> 00:01:54.201
temos uma forma de monitorar
o retorno resultante.

00:01:54.609 --> 00:01:56.961
Essa maneira de analisar
essa péssima política

00:01:56.991 --> 00:01:58.657
vai nos ajudar a melhorá-la.

00:01:58.687 --> 00:02:01.321
Mas, antes de falarmos
como faremos isso,

00:02:01.897 --> 00:02:05.528
vamos adicionar fórmulas
e terminologias a esse processo

00:02:05.558 --> 00:02:06.912
que acabamos de realizar.

00:02:06.942 --> 00:02:09.024
Você pode encarar
essa grade de números

00:02:09.054 --> 00:02:11.864
como uma função
do estado do ambiente.

00:02:11.894 --> 00:02:14.712
Para cada estado,
ela tem um número correspondente.

00:02:15.504 --> 00:02:18.753
Nós nos referimos a essa função
como "função de valor de estado".

00:02:19.369 --> 00:02:22.736
Para cada estado, ela produz
o provável valor de retorno

00:02:23.152 --> 00:02:24.992
caso o agente comece
nesse estado

00:02:25.022 --> 00:02:28.136
e, depois, siga a política
para todos os instantes de tempo.

00:02:28.832 --> 00:02:31.721
Mas é mais comum vê-la
expressa equivalentemente,

00:02:31.751 --> 00:02:33.952
só que com um pouco
mais de fórmulas.

00:02:33.982 --> 00:02:35.880
Antes de mostrar
essa fórmula,

00:02:35.910 --> 00:02:38.463
eu devo avisá-los
que ela parece um pouco complicada,

00:02:38.944 --> 00:02:42.136
mas é equivalente
ao que já discutimos.

00:02:42.864 --> 00:02:44.198
E aqui está ela.

00:02:44.608 --> 00:02:47.177
A função de valor de estado
para uma política pi

00:02:47.207 --> 00:02:49.639
é uma função
do ambiente de estado.

00:02:50.215 --> 00:02:54.841
Para cada estado S, ela nos diz
o desconto de retorno esperado

00:02:54.871 --> 00:02:57.464
caso o agente comece
nesse estado S,

00:02:57.494 --> 00:03:00.152
e, depois, usa a política
para escolher suas ações

00:03:00.182 --> 00:03:01.721
para todos os instantes
de tempo.

00:03:01.751 --> 00:03:04.512
A função de valor de estado
vai sempre corresponder

00:03:04.542 --> 00:03:06.704
a uma política específica.

00:03:06.734 --> 00:03:10.288
Então, se mudarmos a política,
mudamos a função de valor de estado.

00:03:11.000 --> 00:03:13.840
Costumamos representar a função
com um v minúsculo,

00:03:13.870 --> 00:03:16.617
com uma política
correspondente subscrita.

